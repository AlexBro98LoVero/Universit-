\documentclass{rapport}
\usepackage{lipsum}
\usepackage{gensymb}
\usepackage{float}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsthm} 
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{polynom}

\usepackage{enumitem}
\usepackage[most, theorems]{tcolorbox}
\usepackage{xcolor}
\usepackage[italian]{babel} % lingua italiana

\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\usepackage{tikz}
\newcommand*\circled[2][]{%
  \tikz[baseline=(char.base)]{
    \node[shape=circle,draw=#1,inner sep=1pt] (char) {$\displaystyle #2$};
  }%
}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{matrix, positioning}

\newtcbtheorem{teorema}{Teorema}{
  colback=blue!5!white,
  colframe=blue!75!black,
  enhanced,
  fonttitle=\bfseries,
}{theo}

\newtcbtheorem{definizione}{Definizione}{
    colback=green!5!white, 
    colframe=green!75!black, 
    enhanced,
    fonttitle=\bfseries,
}{def}

\newtcbtheorem{esercizio}{Esercizio}{
    colback=red!5!white, 
    colframe=red!75!black, 
   enhanced,
  fonttitle=\bfseries,
}{es}

\newtcbtheorem{corollario}{Corollario}{
    colback=pink!5!white, 
    colframe=pink!75!black, 
    enhanced,
  fonttitle=\bfseries,
}{corr}

\newtcbtheorem{esempio}{Esempio}{
    colback=purple!5!white, 
    colframe=purple!75!black, 
    enhanced,
  fonttitle=\bfseries,
}{esem}

% \newtheorem{theorem}{Teorema}
% \newtheorem{proposition}[theorem]{Proposizione}
% \newtheorem{corollary}[theorem]{Corollario}
% \newtheorem{lemma}[theorem]{Lemma}

% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definizione}
% \newtheorem{axiom}[theorem]{Assioma}
% \newtheorem{example}[theorem]{Esempio}

% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}
% \newtheorem{note}[theorem]{Note}
\def\mathunderline#1#2{\color{#1}\underline{{\color{black}#2}}\color{black}}
\newcommand{\mathrect}[2]{%
  \fcolorbox{#1}{white}{\strut$\displaystyle #2$}%
}

\title{DataBase} %title of the file

\begin{document}

%----------- Report information ---------

\logo{logos/logo.jpg}
\uni{\textbf{Università degli Studi di Padova}}
\ttitle{Analisi 1} %title of the file
\subject{Analisi 1} % Subject name
\topic{Analisi 1} % Topic name

\students{Alex Gasparini} % information related to the students

%----------- Init -------------------
        
\buildmargins % display margins
\buildcover % create the front cover of the document
\toc % creates the table of contents

%------------ Report body ----------------



\section{Principio d'Induzione}
\addcontentsline{toc}{subsection}{Teorema del binomio di Newton}
\begin{teorema}{del binomio di Newton}{}
\begin{equation*}
    (a+b)^n = \sum_{k=0}^{n}\binom{n}{k}a^{n-k}b^{k} \quad \forall n \in \mathbb{N}_0
\end{equation*}

\end{teorema} 

\begin{proof} 
Facciamo una dimostrazione per induzione. Partiamo con la base induttiva, con $n_0 = 0$:

\begin{align*}
    (a+b)^0 &= \sum_{k=0}^{0}\binom{0}{k}a^{0-k}b^{k} \\
    1 &= \binom{0}{0}a^{0-0}b^{0} \\
    1 &= 1 \cdot 1 \cdot 1 \\
    1 &= 1
\end{align*}



La base induttiva è stata verificata. Ora passiamo al passo, quindi supponiamo che $P(n)$ sia vero, e proviamo a vedere se è vero $P(n+1)$

\begin{equation*}
    (a+b)^{n+1} = \sum_{k=0}^{n+1}\binom{n}{k}a^{n+1-k}b^{k}    
\end{equation*}

Per proseguire con la dimostrazione lasceremo in alterato il termine di destra e andremo a modificare quello di sinistra.

\begin{equation*}
    (a+b)^{n+1} =(a+b)^{n} \cdot(a+b) 
\end{equation*}

Ora possiamo sostituire $(a+b)^{n}$ con $P(n)$ visto che è $P(n)$ è vera (dato che è una nostra ipotesi)

\begin{align}
    (a+b)^{n+1} &= \left( \sum_{k=0}^{n}\binom{n}{k}a^{n-k}b^{k}\right) \cdot(a+b)   \\
     &= \mathunderline{blue}{a \cdot \left(\sum_{k=0}^{n}\binom{n}{k}a^{n-k}b^{k} \right)}+\mathunderline{red}{b\cdot \left(\sum_{k=0}^{n}\binom{n}{k}a^{n-k}b^{k}\right)} \label{eq:bin1}
\end{align}


Per comodità andiamo a d analizzare singolarmente le due sommatorie, prima quella in blu e poi quella in rosso.

\begin{align*}
\mathunderline{blue} {a \cdot \left( \sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^{k} \right)} &= \sum_{k=0}^{n} a \cdot \binom{n}{k} a^{n-k} b^{k} \\
 &= \sum_{k=0}^{n}\binom{n}{k} a^{n+1-k} b^{k}
\end{align*}



\newpage
Ora analiziamo la parte rossa


\begin{align*}
\mathunderline{red} {b \cdot \left( \sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^{k} \right)} &= \sum_{k=0}^{n} b \cdot \binom{n}{k} a^{n-k} b^{k} \\
 &= \sum_{k=0}^{n}\binom{n}{k} a^{n-k} b^{k+1}
\end{align*}

ora facciamo una sostituzione $h = k+1$

\begin{align*}
\mathunderline{red} {\sum_{k=0}^{n}\binom{n}{k} a^{n-k} b^{k+1}} &= \sum_{h=1}^{n+1}\binom{n}{h-1} a^{n-(h-1)} b^{h} \\
 &= \sum_{h=1}^{n+1}\binom{n}{h-1} a^{n-h+1} b^{h}
\end{align*}

Visto che gli indici nelle sommatorie sono muti, sostituiamo $h$ con $k$, in modo che tutte le sommatorie sono rispetto a $k$

\begin{align*}
\mathunderline{red} {\sum_{h=1}^{n+1}\binom{n}{h-1} a^{n-h+1} b^{h}} = \sum_{k=1}^{n+1}\binom{n}{k-1} a^{n+1-k} b^{k}
\end{align*}

Ricomposiamo le due sommatorie ritornando al punto \eqref{eq:bin1}


\begin{align}
    (a+b)^{n+1} &= \mathunderline{blue}{\sum_{k=0}^{n}\binom{n}{k} a^{n+1-k} b^{k}}+\mathunderline{red}{\sum_{k=1}^{n+1}\binom{n}{k-1} a^{n+1-k}b^{k}} \label{eq:bin2}
\end{align}
 Per unire le due sommatorie devono avere gli stessi indici, quindi rimoviamo gli indici "in più", nella prima togliamo il termine con indice $k=0$, in modo che entrambe partano con $k=1$, e nella seconda rimuoviamo il termine $k=n+1$ in modo che entrambe finiscano con il termine $k=n$

 \begin{align*}
    \mathunderline{blue}{\sum_{k=0}^{n}\binom{n}{k} a^{n+1-k} b^{k}} &= \binom{n}{0} a^{n+1-0} b^{0} + \sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k} \\
    &= 1\cdot a^{n+1} \cdot 1 + \sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k}\\
    &= a^{n+1} + \sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k}
\end{align*}

\newpage
 \begin{align*}
    \mathunderline{red}{\sum_{k=1}^{n+1}\binom{n}{k-1} a^{n+1-k}b^{k}} &= \binom{n}{(n+1)-1} a^{n+1 -(n+1)}b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}  \\
    &= \binom{n}{n} a^{0}b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k} \\
    &= 1 \cdot 1 \cdot b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}\\
    &= b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}
\end{align*}

 Riscriviamo il termine \eqref{eq:bin2}


\begin{align*}
    (a+b)^{n+1} &= \mathunderline{blue}{a^{n+1} + \sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k}}+\mathunderline{red}{b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}} \\
    &= \mathunderline{blue}{a^{n+1}} + \mathunderline{red}{b^{n+1}} + \mathunderline{blue}{\sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k}} + \mathunderline{red}{\sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}}  \\
    &=a^{n+1} + b^{n+1} + \sum_{k=1}^{n} \left(\binom{n}{k} a^{n+1-k} b^{k} +\binom{n}{k-1} a^{n+1-k}b^{k}\right)\\
    &=a^{n+1} + b^{n+1} + \sum_{k=1}^{n} a^{n+1-k} b^{k} \cdot\left( \binom{n}{k} +\binom{n}{k-1} \right)\\
    &=a^{n+1} + b^{n+1} + \sum_{k=1}^{n} a^{n+1-k} b^{k} \cdot\binom{n+1}{k} \\
    &=a^{n+1} + b^{n+1} + \sum_{k=1}^{n} \binom{n+1}{k}a^{n+1-k} b^{k} 
\end{align*}
 Siamo quasi alla fine ma notiamo che rispetto a $P(n+1)$ gli indici sono sbagliati, infatti $P(n+1)$ parte con indice $k=0$ e termina con $k=n+1$, mentre la sommatoria che abbiamo appena trovato parte da $k=1$ e termina con $k=n$. Proviamo a vedere cosa sarebbero i termini $k=0$ e $k=n+1$ (quelli che a noi mancano)


\begin{align*}
    \binom{n+1}{0}a^{n+1-0} b^{0}  &= 1 \cdot a^{n+1} \cdot 1 = \mathunderline{blue}{a^{n+1}} \qquad (k=0)  \\ 
    \binom{n+1}{n+1}a^{n+1-(n+1)} b^{n+1}  &= 1 \cdot 1 \cdot b^{n+1} = \mathunderline{red}{b^{n+1}} \qquad (k=n+1)  
\end{align*}
 \newpage
Vediamo che i termini che ci mancano ($a^{n+1}$ e $b^{n+1}$) in realtà ce li abbiamo fuori dalla sommatoria, quindi possiamo "portarli dentro" alla sommatoria sistemando gli indici

\begin{align*}
    (a+b)^{n+1} &= a^{n+1} + b^{n+1} + \sum_{k=1}^{n} \binom{n+1}{k}a^{n+1-k} b^{k}  \\
    &= \sum_{k=0}^{n+1} \binom{n+1}{k}a^{n+1-k} b^{k}
\end{align*}

In questa maniera siamo riusciti a dimostrare il passo indutttivo, visto che siamo partiti da $P(n)$ e siamo riusciti a dimostrare che $P(n+1)$. Pertanto, visto che sia il passo induttivo che la base induttiva sono verificati, allora il teorema è dimostrato. 
\end{proof}


\addcontentsline{toc}{subsection}{Teorema dell'Irrazionalità di $\sqrt{2}$}
\begin{teorema}{Irrazionalità di $\sqrt{2}$}{}
    

    \begin{equation*}
        \sqrt{2} \notin \mathbb{Q}
    \end{equation*}
\end{teorema}
    

\begin{proof}
    La dimostrazione sarà fatta per assurdo, quindi partiamo supponendo che $\sqrt{2} \in \mathbb{Q}$, pertanto $\sqrt{2}$ lo possiamo scrivere come:

    \begin{equation*}
        \sqrt{2} = \frac{p}{q} \qquad p,q \in \mathbb{Z} \setminus\{0\}
    \end{equation*}

    Supponiamo anche che il $mcd(p, q) = 1$ (Massimo Comun Divisore), altrimenti $p$ e $q$ sarebbero semplificabili ulteriormente. Attenzione perchè questo punto sarà fondamentale per la dimostrazione.

    Per semplicità, eleviamo tutto al quadrato

    \begin{align}
        2 &= \frac{p^2}{q^2} \\
        2q^2 &= p^2 \label{eq:rad2_1}
    \end{align}

    Ora notiamo che $p^2$ è un multiplo di $2$, perciò $p^2$ è un numero pari. di conseguenza anche $p$ è un numero pari dato che solamente un il prodotto di due numeri pari da un numero pari. Allora possiamo scrivere $p$ come 

    \begin{equation*}
        p = 2k \qquad k \in \mathbb{Z}         
    \end{equation*}

    Quindi andiamo a sostituirlo nell'equazione \eqref{eq:rad2_1}

    \begin{align*}
        2q^2 &= (2k)^2  \\
        2q^2 &= 4k^2 \\
        q^2 &= 2k^2
    \end{align*}

    Dopo le varie semplificazioni notiamo che anche $q^2$ è divisivile per $2$, e come abbiamo dedotto per $p$, allora anche $q$ è divisibile per 2. Però sia $p$ che $q$ sono divisibili per $2$, questo implica che $mcd(p,q) \ge 2$. Cosa assurda, visto che avevamo imposto che $mcd(p,q) = 1$, pertanto sono sbagliate le tesi: ovvero che $\sqrt{2} \in \mathbb{Q}$, e se questa affermazione è sbagliata allora per forza $\sqrt{2} \notin \mathbb{Q}$. 
\end{proof}


\newpage

\section{Insiemistica}

\begin{definizione}{Insieme Limitato}{}
\addcontentsline{toc}{subsection}{Definizione di Insieme Limitato}
Dato $\varnothing \ne A \subseteq \mathbb{R} $ è detto
\begin{itemize}
    \item \textbf{Superiorimente limitato} se  $\exists M \in \mathbb{R} : M \geq a \;\; \forall a \in A $
    \item \textbf{Inferiormente limitato} se  $\exists m \in \mathbb{R} : m \leq a \;\; \forall a \in A $
\end{itemize}

L'insieme $\{M: M \geq a \;\; \forall a \in A \}\label{eq:mag} $ è detto \textbf{Insieme dei maggioranti di $A$}.


L'insieme $\{m: m \leq a \;\; \forall a \in A \}$ è detto \textbf{Insieme dei minoranti di $A$}

\end{definizione}




\begin{definizione} {Massimo e Minimo}{}

\addcontentsline{toc}{subsection}{Definizione di Massimo e Minimo}
\noindent

\begin{itemize}
    \item Un maggiorante $M$ di $ A \subseteq \mathbb{R} $ è detto \textbf{massimo} se $M \in A$
    \begin{equation}\label{eq:max}
       M=  max(A) = (M \geq a \;\; \forall a \in A)  \wedge M \in A
    \end{equation}
    \item Un minorante $m$ di $ A \subseteq \mathbb{R} $ è detto \textbf{minimo} se $m \in A$
    \begin{equation} \label{eq:min1}
       m=  min(A) = (m \leq a \;\; \forall a \in A)  \wedge m \in A
    \end{equation}
\end{itemize}



    
\end{definizione}



\begin{teorema}{Unicità dei Massimi e Minimi}{}
\addcontentsline{toc}{subsection}{Teorema dell'unicità dei Massimi e Minimi}
    Dato $\varnothing \ne A \subseteq \mathbb{R}$ se ammette un massimo o un minimo, essi sono \textbf{unici} 
\end{teorema}

\begin{proof}
    Supponiamo che ci siano due massimi $M_1, M_2$ con $M_1 \neq M_2$. Visto che $M_1$ è un massimo allora, per definizione di massimo (\ref{eq:max}), deve essere $M_1 \in A$. Poi visto che $M_2$ è un massimo, di conseguenza è anche un maggiorante allora, per definizione di maggiorante,  deve valere la seguente affermazione

    \begin{equation*}
         M_2 \geq a \;\; \forall a \in A
    \end{equation*}

    Dato che $M_1 \in A$ possiamo dire che 
    
    \begin{equation} \label{eq:max1}
         M_2 \geq M_1
    \end{equation}


    Ora rifacciamo il seguente ragionamento ma al contrario. Dato che $M_2$ è massimo allora deve essere $M_2 \in A$. Visto che $M_1$ è un massimo deve anche essere un maggiorante, e per tanto vale

    \begin{equation*}
         M_1 \geq a \;\; \forall a \in A
    \end{equation*}

    Dato che $M_2 \in A$ possiamo dire che 
    
    \begin{equation} \label{eq:max2}
         M_1 \geq M_2
    \end{equation}


    Ora combinando le informazioni (\ref{eq:max1}) e (\ref{eq:max2}) deve valere 

    \begin{equation*}
        (M_2 \geq M_1) \wedge(M_1 \geq M_2) \Rightarrow M_1 = M_2
    \end{equation*}

    Dato che una delle ipotesi era che $M_1 \neq M_2$ abbiamo raggiunto un assurdo, per tanto il massimo deve essere unico. La dimostrazione per il minimo è analoga. 
\end{proof}



\begin{definizione}{Estremi Superiore e Inferiore}{}
 \addcontentsline{toc}{subsection}{Definizione Estremi Superiori e Inferiore}
    Dato $\varnothing \ne A \subseteq \mathbb{R} $ definiamo 
    \begin{itemize}
        \item \textbf{estremo superiore} di $A$ come il minore dei maggioranti.  
        \begin{equation}
            sup(A) = min\{M: M \geq a \;\; \forall a \in A \}
        \end{equation}

        \item \textbf{estremo inferiore} di $A$ come il maggiore dei minoranti.  
        \begin{equation*}
            inf(A) = max\{m: m \leq a \;\; \forall a \in A \}
        \end{equation*}
    \end{itemize}
\end{definizione}


\begin{teorema}{Relazione Massimi/Minimi e Estremi}{}
    Dato $\varnothing \ne A \subseteq \mathbb{R}$
\addcontentsline{toc}{subsection}{Relazione Massimi/Minimi e Estremi}
    \begin{itemize}
        \item Se esiste $max(A)$, allora coincide con $sup(A)$

        \begin{equation*}
            M = max(A) \Rightarrow M = sup(A)
        \end{equation*}

         \item Se esiste $min(A)$, allora coincide con $inf(A)$

        \begin{equation*}
            m = min(A) \Rightarrow m = inf(A)
        \end{equation*}
        
    \end{itemize}
\end{teorema}

\begin{proof}
    Supponiamo che esista $M_1 = max(A)$ allora sappiamo che è un maggiorante, e di conseguenza appartiene all'insieme dei maggioranti ($N$)
    
    \begin{equation} \label{eq:min3}
        M_1\in N = \{M:M\geq a \;\; \forall a \in A\}
    \end{equation}
    
    e sappiamo anche che $M_1 \in A$, visto che è il massimo. 
    
    Se prendiamo un numero $u \in N$ abbiamo  che 

    \begin{equation*}
        u\geq a \;\; \forall a \in A \;\; \forall u\in N
    \end{equation*}

    Visto che $M_1 \in A$ allora deve valere 
    
    \begin{equation*}
        u\geq M_1 \;\;  \forall u\in N
    \end{equation*}

    Per questo deduciamo che $M_1$ è minorante di $N$, ma nel punto (\ref{eq:min3}) avevamo detto che $M_1 \in N$, di conseguenza 

    \begin{equation*}
        M_1 = min(N)
    \end{equation*}

    Che per definizione è anche l'estremo superiore, quindi

    \begin{equation*}
        M_1 = sup(A)
    \end{equation*}

    La dimostrazione del minimo è analoga.
\end{proof}

\textbf{N.B.} che $max(A) \Rightarrow sup(A)$ e che $sup(A) \not\Rightarrow max(A) $. Per vederlo basta farsi degli esempi, come $A = \{x \in \mathbb{R} ^+ : x^2 < 2\}$ si vede che esiste un estremo superiore ($sup(A) = \sqrt{2}$) mentre non esiste il massimo.



\begin{teorema}{Caratterizzazione degli Estremi}{}

\addcontentsline{toc}{subsection}{Caratterizzazione degli Estremi}
Dato $\varnothing \ne A \subseteq \mathbb{R} $ se esiste $sup(A)$ oppure $inf(A)$ allora possiamo definirli anche come:


\begin{equation*}
    S= \sup(A)= \begin{cases}
S \in \{M:M\geq a \;\; \forall a \in A \} \\
\forall\varepsilon > 0  \;\; \exists a \in A : a > S-\varepsilon 
\end{cases}
\end{equation*}


  
\begin{equation*}
    s= \inf(A)= \begin{cases}
s \in \{m:m\leq a \;\; \forall a \in A \} \\
\forall\varepsilon > 0  \;\; \exists a \in A : a < s+\varepsilon 
\end{cases}
\end{equation*}


\end{teorema}
    

\addcontentsline{toc}{subsection}{Completezza di $\mathbb{R}$ \textit{I}° forma}

\begin{teorema}{Completezza di $\mathbb{R}$ \textit{I°} forma}{}

Dati $A, B \subseteq \mathbb{R}$ tali che $a \leq b \;\;\forall a\in A \;\;\forall b\in B$

allora $\exists c \in \mathbb{R}$, detto \textbf{elemento separatore} tale che 

\begin{equation*}
    a \leq c \leq  b \;\;\forall a\in A \;\;\forall b\in B
\end{equation*}
    
\end{teorema}

\addcontentsline{toc}{subsection}{Completezza di $\mathbb{R}$ \textit{II}° forma}
\begin{teorema}{Completezza di $\mathbb{R}$ \textit{II°} forma}{}
Dato $\varnothing \ne A \subseteq \mathbb{R}$ se è superiormente/inferiormente limitato allora ammette un estremo superiore/inferiore.

\end{teorema}

\begin{proof}
    Dato che $A$ è superiormente limitato allora esiste l'insieme dei maggiornati $N = \{M: M \in \mathbb{R} : M \geq a \;\; \forall a \in A\}$. Visto che tutti gli elementi dell'inisieme dei maggioranti è maggiore di tutti gli elementi di $A$ ($n \geq a \;\;\forall a\in A \;\;\forall n\in N$) possiamo applicare il Teorema di completezza di $\mathbb{R}$ \textit{I°} forma


    \begin{equation*}
        \exists c \in \mathbb{R} : \;\; \mathunderline{blue}{a \leq }c \mathunderline{red}{\leq  n }\;\;\forall a\in A \;\;\forall n\in N
    \end{equation*}

    Visto che $\mathunderline{blue}{a \leq c} \;\;\forall a\in A$ vuol dire che $c$ è un maggiorante di $A$ e di conseguenza $c\in N$ (l'insieme dei maggioranti). 

    Dato che $\mathunderline{red}{c \leq  n } \;\;\forall n\in N$ allora $c$ è un minorante di $N$. Quindi visto che $c \in N$, per definizione di minimo (\ref{eq:min1}) possiamo dire che 

    \begin{equation*}
        c = min(N)
    \end{equation*}

    Questo coincide con la definizione di \textbf{estremo superiore}, quindi

    \begin{equation*}
        c = sup(A)
    \end{equation*}

    Quindi abbiamo dimostrato che esiste un estremo superiore. 

    La dimostrazione don l'estremo inferiore è analoga.
\end{proof}

\newpage


\addcontentsline{toc}{subsection}{Definizione di Intorno}
\begin{definizione}{Intorno}
    Sia $r \in \mathbb{R} \;\cup \; \{\pm\infty\}$ allora

\vspace{-0.35cm}
    \begin{itemize}
        \item Se $r \in \mathbb{R}$ diciamo \textbf{intorno} un qualsiasi intervallo aperto della forma 
        \vspace{-0.35cm}
        \begin{equation*}
            (r-\varepsilon, r+\varepsilon) \;\; \epsilon >0
        \end{equation*}

        \item Se $r = +\infty$ diciamo \textbf{intorno} un qualsiasi intervallo aperto della forma 
        \vspace{-0.35cm}
        \begin{equation*}
            (M, +\infty) \;\; M \in \mathbb{R} \;\cup \; \{-\infty\}
        \end{equation*}

        \item Se $r = -\infty$ diciamo \textbf{intorno} un qualsiasi intervallo aperto della forma 
        \vspace{-0.35cm}
        \begin{equation*}
            (-\infty, M) \;\; M \in \mathbb{R} \;\cup \; \{+\infty\}
        \end{equation*}
    \end{itemize}
\end{definizione}



\begin{teorema}{Intersezione degli introni}{}
\addcontentsline{toc}{subsection}{Teorema di Intersezione degli Intorni}
\noindent

    Sia $I_1, I_2 \subseteq \mathbb{R}$ intorni di $x_0$, allora anche $I = I_1 \;\cap\; I_2$ è intorno di $x_0$.
\end{teorema}

\begin{teorema}{Separazione degli introni}{}
\addcontentsline{toc}{subsection}{Teorema di Separazione degli introni}
\noindent

    Sia $r_1, r_2 \in \mathbb{R} \;\cup \; \{\pm\infty\}$ allora $\exists I_1$ intorno di $r_1$ e $\exists I_2$ intorno di $r_2$ tali che $I_1 \;\cap\; I_2 = \varnothing$ 
\end{teorema}




    \addcontentsline{toc}{subsection}{Definizione di punto di Accumulazione}
\begin{definizione}{Definizione di punto di Accumulazione}
Sia $r\in \mathbb{R} \cup  \{\pm\infty\}$ è detto \textbf{punto di accumulazione} di $\varnothing \ne A \subseteq \mathbb{R}$ se
\vspace{-0.25cm}
    \begin{itemize}
        \item caso $r \in \mathbb{R}$: 
        \vspace{-0.35cm}
        \begin{center}
        
            $\forall I\subseteq \mathbb{R}$ intorno di $r\;\;\;A \cap(I \setminus \{r\}) \neq \varnothing$  
        \end{center}
    \vspace{-0.35cm}
        \item caso $r \in \{\pm\infty\}$: 
        \vspace{-0.35cm}
        \begin{center}
        
            $\forall I\subseteq \mathbb{R}$ intorno di $r\;\;\;A \cap I \neq \varnothing$  
        \end{center}
    \end{itemize}
\end{definizione}



\addcontentsline{toc}{subsection}{Definizione di punto di Accumulazione Destro/Sinistro}
\begin{definizione}{Accumulazione Destro/Sinistro}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $r\in \mathbb{R}$ è detto 

    \begin{itemize}
        \item \textbf{Punto di accumulazione destro} se
    \vspace{-0.20cm}
        \[
        \forall\varepsilon>0\;\;\; (r,r+\varepsilon) \cap A\neq \varnothing
        \]


        \item \textbf{Punto di accumulazione sinistro} se
\vspace{-0.20cm}
        \[
        \forall\varepsilon>0\;\;\; (r-\varepsilon,r) \cap A \neq \varnothing
        \]
    \end{itemize}

    \textbf{N.B.} i simboli $\pm \infty$ non ha senso definirli punti di accumulazione destro/sinistro
\end{definizione}

\newpage
\begin{esercizio}{}{}
    Sia $r\in \mathbb{R}$ e $\varnothing \ne A \subseteq \mathbb{R}$, $r$ è punto di accumulazione destro o sinistro di $A$ se e solo se $r$ è punto di accumulazione di $A$
\end{esercizio}

\begin{proof}
    Visto che questa è una doppia implicazione dobbiamo controllare che l'implicazione sia vera da entrambi i lati. 
    
    ($\implies$)Partiamo dimostrando che se $r$ è punto di accumulazione destro (o sinistro) di $A$ allora $r$ è punto di accumulazione di $A$. Partiamo con la definizione di punto di accumulazione destro.

    \[
    \forall\varepsilon>0\;\;\; (r,r+\varepsilon) \cap A \neq \varnothing
    \]

    Ora dobbiamo controllare se è vera la definizione di punto di accumulazione, per farla riscriviamola 

    \begin{center}
        $\forall I\subseteq \mathbb{R}$ intorno di $r\;\;\;A \cap(I \setminus \{r\}) \neq \varnothing$  
    \end{center}
    \[
        \forall \varepsilon>0 \;\;\; ((r-\varepsilon, r+\varepsilon) \setminus \{r\} ) \cap A \neq \varnothing
    \]


    Con questa riscrittura si nota che 


    \[
    (r,r+\varepsilon) \subseteq (r-\varepsilon, r+\varepsilon) \setminus \{r\}
    \]

    Di conseguenza per ogni insieme che troviamo per il punto di accumulazione destro, posso trovare un intervallo sul punto di accumulazione, di conseguenza se $r$ è un punto di accumulazione destro deve anche essere un punto di accumulazione in $A$. Ragionamento analogo per il punto di accumulazione sinistro


    ($\impliedby$)Per dimostrare che se $r$ è punto di accumulazione allora deve essere punto di accumulazione destro o sinistro, ragioniamo per assurdo: quindi supponiamo che $r$ non è ne punto di accumulaione destro ne sinistro. Allora sappiamo

    \[
        \exists\varepsilon_1>0,\exists\varepsilon_2>0 :(r-\varepsilon_1, r) \cap A = \varnothing \wedge (r, r + \varepsilon_2) \cap A = \varnothing 
    \]


    Ma se prendiamo $\varepsilon = min(\varepsilon_1,\varepsilon_2)$ allora 

    \[
        \exists\varepsilon>0 :  (r-\varepsilon_,  r + \varepsilon_) \cap A = \varnothing
    \]


    Questo non è altro che la definizione di punto isolato, ovvero la negazione di punto di accumulazione. Quindi abbiamo scoperto che 

    \begin{center}
        $r$ non è punto acc. dx e $r$ non è punto acc. sx $\Rightarrow$ $r$ non è punto di acc.
    \end{center}

    Usando le proprietà dell'implicazione $(P \Rightarrow Q ) \Leftrightarrow (\overline{Q}
 \Rightarrow \overline{P})$

    \begin{center}
        $r$ è punto di acc.$\Rightarrow$ $r$ è punto acc. dx opure $r$ è punto acc. sx 
    \end{center}

    
\end{proof}






\newpage





\section{Limiti}

\begin{definizione}{Definizione di Limite}{}

\addcontentsline{toc}{subsection}{Definizione di Limite}
    Dato $\varnothing \neq A \subseteq \mathbb{R}$, $x_0\in \mathbb{R} \cup  \{\pm\infty\}$ punto di accumulazione per $A$, $f:A\rightarrow\mathbb{R}$. Diciamo che $l  \in \mathbb{R} \;\cup \; \{\pm\infty\}$ è \textbf{limite di $f$ per $x\rightarrow x_0$} se

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I \subseteq \mathbb{R}$ intorno di $x_0$ tale che \\ $f(x) \in U\;\;\; \forall x \in A \cap (I \;\setminus \{x_0\})$
    \end{center}
\end{definizione}



\begin{teorema}{Unicità del limite}{}

\addcontentsline{toc}{subsection}{Teorema di Unicità del Limite}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $x_0\in \mathbb{R}\cup \{\pm\infty\}$ punto di accumulazione per $A$, $f:A\rightarrow\mathbb{R}$. Se esiste il limite $f$ per $x\rightarrow x_0$ allora è unico. E lo si indica con:

    \begin{equation}
        \lim_{x\to x_0} f(x) = l
    \end{equation}
\end{teorema}

\begin{proof}
    Supponiamo che il limite esista ed abbiamo due valori: $l_1, l_2 \in \mathbb{R}\cup \{\pm\infty\}$ con $l_1 \neq l_2$. Visto che per ipotesi $l_1 \neq l_2$ allora per il teorema di separazione abbiamo che $\exists U_1 \subseteq \mathbb{R}$ intorno di $l_1$ e $\exists U_2 \subseteq \mathbb{R}$ intorno di $l_2$ tali che:

    \begin{equation}\label{eq:unicita1}
         U_1 \cap U_2 = \varnothing
    \end{equation}

     Applicando la definizione di limite, $\exists I_1, I_2$ intorni di $x_0$ tali che:

    \begin{center}
        $\forall U_1$ intorno di $l_1 \;\;\;\;\; f(x) \in U_1 \;\;\; \forall x \in A \cap (I_1 \setminus \{x_0\})$
    \end{center}

    
    \begin{center}
        $\forall U_2$ intorno di $l_2 \;\;\;\;\; f(x) \in U_2 \;\;\; \forall x \in A \cap (I_2 \setminus \{x_0\})$
    \end{center}


     Usando il teorema di intersezione degli intorni, $\exists I_3 =I_1 \cap I_2$ intorno di $x_0$, e visto che $I_3 \subseteq I_1$, anche in esso varrà la proprietà del limite $l_1$. Contemporaneamente varrà anche la proprietà del limite $l_2$ visto che $I_3 \subseteq I_2$, per tanto

     \begin{center}
        $\forall U_1 \;\forall U_2  \;\;\;\;\; f(x) \in U_1  \wedge f(x) \in U_2 \;\;\; \forall x \in A \cap (I_3 \setminus \{x_0\})$
    \end{center}


    la congiungiole logica la possiamo riscrivere come congiunzione insiemistica

    \begin{center}
        $\forall U_1 \;\forall U_2  \;\;\;\;\; f(x) \in U_1  \cap U_2 \;\;\; \forall x \in A \cap (I_3 \setminus \{x_0\})$
    \end{center}

    Però questo necessita che $\forall U_1 \;\forall U_2 \;\;\; U_1  \cap U_2 \neq \varnothing$, perchè altrimenti il limite non esisterebbe. Ma all'inizio con l'equazione (\ref{eq:unicita1}) sappiamo che esistono almeno un $U_1$ e un $U_2$ che rendono l'intersezione vuoto. Per tanto è un assurdo e le ipotesi erano sbagliate. Di conseguenza il limite deve essere unico e non può assumere più di un valore. 

\end{proof}


\newpage

\begin{esercizio}{}{}
\addcontentsline{toc}{subsection}{Esercizi Dimostrazione Limite}
    \begin{equation*}
        \lim_{x\to x_0} 2x+3 = 2x_0 +3
    \end{equation*}    
\end{esercizio}

\begin{proof}
    Proviamo a dimostrarlo con la definizione. $f(x)=2x+3$ e, dato che è un polinomio, il suo dominio sarà $A = \mathbb{R}$. l'intorno di $l = 2x_0 +3$ sarà $U = (2x_0 +3-\varepsilon, 2x_0 +3 + \varepsilon)$, e un intorno di $x_0$ sarà $I =(x_0-\delta, x_0+ \delta)$. Di conseguenza con la definizione di limite sarà


    \begin{equation*}
        \forall U \;\;\;\; f(x) \in U \;\;\; \forall x \in  I \setminus\{x_0\}
    \end{equation*}

    Con i dati dell'esercizio

    \begin{equation*}
        \forall \varepsilon>0 \;\;\;\; 2x+3 \in (2x_0 +3-\varepsilon, 2x_0 +3 + \varepsilon) \;\;\; \forall x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\}
    \end{equation*}

    Riscriviamo il termine $2x+3 \in (2x_0 +3-\varepsilon, 2x_0 +3 + \varepsilon)$

    \begin{center}
         $2x_0 +3-\varepsilon <2x+3 < 2x_0 +3 + \varepsilon $\\
         $2x_0-\varepsilon <2x < 2x_0 + \varepsilon$ \\
         $x_0-\frac{\varepsilon }{2} <x < x_0 + \frac{\varepsilon }{2}$\\
    \end{center}


    Vediamo come partendo dalla prima porzione abbiamo trovato un intorno su cui deve stare $x$, pertanto affinchè sia vero la definizione di limite possiamo scegliere 

    \begin{equation*}
        \delta \leq \frac{\varepsilon}{2}
    \end{equation*}
\end{proof}


\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} c = c \;\;\; \forall c \in \mathbb{R}
    \end{equation*} 
\end{esercizio}


\begin{proof}
    Usiamo la definizione di limite 

\[
\forall \varepsilon>0 \;\;\;\; c \in (c-\varepsilon, c + \varepsilon) \;\;\; \forall x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\}
\]
    Scrivendola in un'altra maniera 
\[
\forall \varepsilon>0 \;\;\;\; x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\} \Rightarrow c \in (c-\varepsilon, c + \varepsilon)
\]

Però il conseguente della nostra implicazione è sempre vero, perchè $c$ sarà sempre nell'intevallo $(c-\varepsilon, c + \varepsilon)$ per qualsiasi valore di $\varepsilon$ positivo. Quindi volendo riscrivere la proposizione


    \[
\forall \varepsilon>0 \;\;\;\; x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\} \Rightarrow Vero
\]

Una implicazione è sempre vera quando implica vero (vedi tabella di verità dell'implicazione). Quindi la nostra proposizione è sempre vera, e di conseguenza il limite è verificato.
\end{proof}

\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} x^2 = x_0^2
    \end{equation*}
\end{esercizio}

\begin{proof}
    Con la definizione di limite abbiamo

    \begin{equation*}
        \forall \varepsilon>0 \;\;\;\; x^2 \in (x_0^2 -\varepsilon, x_0^2 + \varepsilon) \;\;\; \forall x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\}
    \end{equation*}

    Utilizzando le proprietà della funzione modulo possiamo scriverlo anche come 

    \begin{equation*}
        \forall \varepsilon>0 \;\;\;\; |x^2 -x_0^2| < \varepsilon \; \Leftarrow \; |x - x_0| < \delta
    \end{equation*}

    Riscriviamo il primo termine

    
        \[|x^2 -x_0^2| < \varepsilon\] 
        \[|(x-x_0)(x+x_0)| < \varepsilon\]
        \[|x-x_0||x+x_0| < \varepsilon\] 
    


    Ora dobbiamo capire quanto vale $|x+x_0| $in modo da non avere più la variabile $x$. Per farlo scegliamo $\delta<1$

        
    
        \[|x+x_0| < \delta <1\]
        \[|x+x_0|<1\]
    

    con questo scopriamo che 
    
        \[|x+x_0| = |x-x_0+2x_0| \leq \mathunderline{red}{|x-x_0|} + 2|x_0| < \mathunderline{red}{1} + 2|x_0|\]
        \[|x+x_0| < 1+2|x_0|\]
    
    
    Moltiplicando per $|x-x_0|$, ricordandoci anche che $|x-x_0|<\delta$
    
    \begin{center}
        \[|x-x_0||x+x_0| < |x-x_0|(1 + 2|x_0|)\]
        \[|x-x_0||x+x_0| < \mathunderline{red}{|x-x_0|}(1 + 2|x_0|) < \mathunderline{red}{\delta}(1 + 2|x_0|)\]
        \[|x-x_0||x+x_0| < \delta(1 + 2|x_0|)\]
    \end{center}

    Partendo da $|x-x_0|<\delta$ siamo riusciti a capire che $|x^2-x_0^2|<\delta(1 + 2|x_0|)$, quindi se per verificare il limite bisogna che $|x^2-x_0^2|<\varepsilon$ è necessario imporre 

    
        \[\delta(1 + 2|x_0|) < \varepsilon  \]
        \[\delta < \frac{\varepsilon}{1 + 2|x_0|}\]
    
    Visto che abbiamo imposto $\delta<1$, aggiustiamo la definizione

    \[\delta < min \left( 1,\frac{\varepsilon}{1 + 2|x_0|}\right) \]
    
\end{proof}


\newpage

\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} \sqrt{x} = \sqrt{x_0}  \;\;\; \forall x_0\geq 0
    \end{equation*}
\end{esercizio}

\begin{proof}
    Usiamo la definizione di limite

    \[
    \forall \varepsilon > 0 \;\;\; |\sqrt{x} - \sqrt{x_0}|<\epsilon \;\;\; |x-x_0| < \delta 
    \]


    Partiamo analizzando il termine $|x-x_0| < \delta$


    \[
    |x-x_0| < \delta
    \]

    \[
    |\sqrt{x}-\sqrt{x_0}|\cdot|\sqrt{x}+\sqrt{x_0}| < \delta
    \]

    \[
    |\sqrt{x}-\sqrt{x_0}| < \frac{\delta}{|\sqrt{x}+\sqrt{x_0}|}
    \]

    Ora possiamo sfruttare la seguente espressione

    \[
    |\sqrt{x}+\sqrt{x_0}| \geq |\sqrt{x_0}|
    \]

    \[
    \frac{1}{|\sqrt{x}+\sqrt{x_0}|} \leq \frac{1}{|\sqrt{x_0}|}
    \]

    Di conseguenza

    \[
    |\sqrt{x}-\sqrt{x_0}| < \frac{\delta}{|\sqrt{x}+\sqrt{x_0}|} \leq \frac{\delta}{\sqrt{x_0}}
    \]

    \[
    |\sqrt{x}-\sqrt{x_0}| <  \frac{\delta}{\sqrt{x_0}}
    \]


    Affinchè il limite sia verificato è necessiario che 
    
    \[
    \frac{\delta}{\sqrt{x_0}} < \varepsilon
    \]

    \[
    \delta < \sqrt{x_0}\varepsilon
    \]


    \textbf{N.B.} per tutto l'esercio abbiamo potuto scrivere $\sqrt{x_0}$ non controllando se $x_0$ fosse non negativo perchè il dominio di $f(x) = \sqrt{x}$ è $\mathbb{R}^+_0$ e di conseguenza qualsiasi punto $x<0$ non è punto di accumulazione, dato che esiste almeno un intorno di un numero negativo che intersecato con il dominio ($\mathbb{R}^+_0$) dà insieme vuoto. E per questo il limite lo possiamo fare solo con valori di $x\geq0$ e possiamo scrivere $\sqrt{x_0}$ senza alcun problema.

\end{proof}
\newpage
\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} x^n = x_0^n \;\;\;\; \forall n\in\mathbb{N}
    \end{equation*}
\end{esercizio}

\begin{proof}
    Con la definizione di limite abbiamo
    
        \[|x^n -x_0^n| < \varepsilon\] 
        \[\left|(x-x_0)\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right| < \varepsilon\]
        \[|x-x_0|\left|\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right| < \varepsilon\] 
    


    Ora dobbiamo capire quanto vale il secondo termine in modo da non avere più la variabile $x$. 

        
    
        \[|x+x_0| < \delta <1\]
        \[|x+x_0|<1\]
    
    con questo scopriamo che 
    
        \[|x| = |x-x_0+x_0| \leq \mathunderline{red}{|x-x_0|} + |x_0| < \mathunderline{red}{1} + |x_0|\]
        \[|x| < 1+|x_0|\]
    
    
    Di conseguenza

    \[\left|\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right| \leq \sum_{k=0}^{n-1} |x^{n-1-k}x_0^k| = \sum_{k=0}^{n-1} |\mathunderline{red}{x^{n-1-k}}||x_0^k| \leq \sum_{k=0}^{n-1} |(\mathunderline{red}{1+|x_0|})^{n-1-k}||x_0^k|\] 


    Semplificando un po troviamo che 

    \[\left|\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right| \leq (1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k\]


    
    Moltiplicando per $|x-x_0|$, ricordandoci anche che $|x-x_0|<\delta$

        \[\mathunderline{blue}{|x-x_0|\left|\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right|}  < |x-x_0|(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k\]
        \[\mathunderline{blue}{|x^n-x_0^n|}< \mathunderline{red}{|x-x_0|}(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k < \mathunderline{red}{\delta}(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k\]
        \[|x^n-x_0^n| < \delta(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k\]
        


    Per trovare quanto vale $\varepsilon$ basta fare 

    
        \[\delta(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k < \varepsilon  \]
        \[\delta < \frac{\varepsilon}{(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k}\]
    
    Visto che abbiamo imposto $\delta<1$, aggiustiamo la definizione

    \[\delta < min \left( 1,\frac{\varepsilon}{(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k}\right) \]
    
\end{proof}

\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} sin(x) = sin(x_0)
    \end{equation*}
\end{esercizio}


\begin{proof}
    Dalla definizione di limite

    \[
    |sin(x)-sin(x_0)| < \varepsilon
    \]

    Usando le formule di Prostaferesi ($sin(\alpha) - sin(\beta) = 2cos\left(\frac{\alpha+\beta}{2}\right)sin\left(\frac{\alpha-\beta}{2}\right)$)


    \[
        |sin(x)-sin(x_0)| =     \left|2cos\left(\frac{x+x_0}{2}\right)sin\left(\frac{x-x_0}{2}\right)\right|
    \]

    Ora ricordiamo che per definizione delle funzioni trigonometriche, vale sempre la seguente proposizione

    \[
    |cos(x)| \leq 1  \;\;\;\;\;\;\;\;\;\;\;\; |sin(x)|\leq 1
    \]

    Quindi usiamo questa proprietà del coseno per diventare

     \[
        \left|2\mathunderline{red}{cos\left(\frac{x+x_0}{2}\right)}sin\left(\frac{x-x_0}{2}\right)\right| \leq 2\left|\mathunderline{red}{1}\cdot sin\left(\frac{x-x_0}{2}\right) \right|
    \]

    Poi ricordiamo anche che per la funzione seno vale la seguente relazione

    \[
    |sin(x)| \leq |x|
    \]

    E che quindi nella nostra dimostrazione possiamo usarla 

    \[
    2\left|sin\left(\frac{x-x_0}{2}\right) \right| \leq 2\left|\frac{x-x_0}{2}\right| = |x-x_0|
    \]


    Riscrivendo le informazioni trovate finora sappiamo che

    \[
        |sin(x)-sin(x_0)| \leq |x-x_0|
    \]

    Per la definizione di limite sappiamo che $|x-x_0| < \delta$

    \[
        |sin(x)-sin(x_0)| \leq |x-x_0| <\delta
    \]


    Ora se dobbiamo trovare il $\varepsilon$ basta impore

    \[
        |sin(x)-sin(x_0)| \leq |x-x_0| <\delta < \varepsilon
    \]

    \[\delta < \varepsilon\]


    Per il coseno la dimostrazione è analoga.
\end{proof}
    
\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} a^x = 1 \;\;\;\;\;a \in [1,+\infty)
    \]
\end{esercizio}

\begin{proof}
    Per dimostrare questo limite dobbiamo trovare un un qualche $\delta>0$ tale che

    \[
    \forall \varepsilon>0 \;\;\; |x-0| < \delta \Rightarrow |a^x-1| < \epsilon
    \]

    \[
        \forall \varepsilon>0 \;\;\; -\delta <x < \delta \Rightarrow 1-\epsilon < a^x < 1+\epsilon
    \]

    Quindi iniziamo partendo dalla disuguaglianza di Bernulli

    \begin{equation}
        (1+\varepsilon)^n \geq 1+n\varepsilon \;\;\; \forall n \in \mathbb{N}_0 \; \forall \varepsilon \geq 0
    \end{equation}

    Ora decidiamo che $a < 1+n\varepsilon$ e che quindi $\forall n>\frac{a-1}{\varepsilon}$ vale

    \[
    (1+\varepsilon)^n \geq 1+n\varepsilon > a
    \]

    \[
    (1+\varepsilon)^n  > a
    \]

    \[
        1+\varepsilon  > a^{\frac{1}{n}}
    \]


    Ora quindi sappiamo che per qualsiasi valore di $n>\frac{a-1}{\varepsilon}$ vale la relazione $a^{\frac{1}{n}} < 1+\varepsilon$, quindi se trovo per quali valori di $x$ vale la relazione $a^x<a^{\frac{1}{n}}$ posso imporre $a^x < a^{\frac{1}{n}} < 1+\varepsilon $ che vuol dire che abbiamo dimostrato la prima parte. 


    \[
    a^x < a^{\frac{1}{n}}
    \]

    Per monotonia della funzione $f(x) = a^x$ allora  vale

    \[
    x < \frac{1}{n}
    \]

    Quindi per dimostrare il limite basta scegliere un $\delta <\frac{1}{n}$ in modo tale che l'espressione $a^x <1+\varepsilon$ sia valida.
\newpage
    Per dimostrare la porzione $1-\varepsilon < a^x$ dobbiamo ricorrere alla formula 

    \begin{equation} \label{eq:bern}
        1-\varepsilon < \frac{1}{1+\varepsilon}
    \end{equation}
    


    Dai ragionamenti di prima sappiamo che $(1+\varepsilon)^n > a$, quindi vale anche

    \[
    \left(\frac{1}{1+\varepsilon}\right)^n < \frac{1}{a^n}
    \]

    Quindi se eleviamo tutto alla $n$ l'equazione (\ref{eq:bern}) ricaviamo

     \[
    (1-\varepsilon )^n<\left(\frac{1}{1+\varepsilon}\right)^n < \frac{1}{a}
    \]
    
    \textbf{N.B.} per non avere problemi di segno dobbiamo imporre $1-\varepsilon >0 \Rightarrow \varepsilon < 1$.
    
    \[
    (1-\varepsilon )^n< \frac{1}{a}
    \]

     \[
    1-\varepsilon < a^{-\frac{1}{n}}
    \]

    Quindi come per la prima parte della dimostrazione ora basta scegliere delle $x$ per cui $a^{-\frac{1}{n} }< a^x$, che per monotonia come prima rimane

    \[
    -\frac{1}{n} < x
    \]
    
    Per confermare la dimostrazione possiamo scegliere un $\delta$ tale che 

    \[
    -\frac{1}{n} < -\delta
    \]

    
    \[
    \delta < \frac{1}{n} 
    \]

    Quindi sce scegliamo un $\delta < \frac{1}{n}$ anche l'espressione $1-\varepsilon < a^x$ sarà verificata. Pertanto per verificare il limite basta scegliere 

    \[
    \delta = min\left(1, \frac{\varepsilon}{a-1}\right)
    \]
    
\end{proof}


\begin{esercizio}{}{}
    

    \[
        \lim_{x\to 0} a^x = 1 \;\;\;\;\;a \in(0,1)
    \]
\end{esercizio}
\begin{proof}
    Per dimostrare questo limite possiamo usare uno stratagemma per evitare di fare tutta la dimostrazione classica. Perchè con l'esercizio precendente abbiamo dimostrato con la base $a\geq 1$, quindi cerchiamo di ricondurli a quel limite. Per farlo usiamo le regole delle potenze infatti 

    \[
        0<a<1 \Rightarrow \frac{1}{a} > 1
    \]

    Quindi il limite lo possiamo riscrivere come


    \vspace{-0.30cm}
    \[
        \lim_{x\to 0} a^x = \lim_{x\to 0} \left( \frac{1}{a}\right)^{-x}
    \]

    In questa maniera la base è maggiore di 1, di conseguenza è uguale al limite dell'esercizio precedente da quel punto di vista. Quello che cambia è che all'esponente abbiamo $-x$ e non più $x$, però non è troppo un problema, infatti se $x\to 0$ allora anche $-x\to 0$, quindi l'esponente si avvicina lo stesso allo $0$, di conseguenza il limite sarà lo stesso, di prima e possiamo usare quello (che abbiamo già dimostrato) per dimostrare questo senza la dimostrazione rigorosa.

    \vspace{-0.35cm}
    \[
        \lim_{x\to 0} \left( \frac{1}{a}\right)^{-x} = \lim_{x\to 0} \left( \frac{1}{a}\right)^{x} = 1
    \]

    \textbf{N.B.} il passaggio dove diciamo che se $x\to 0$ allora $-x\to 0$ non è dimostrato in maniera rigorosa, infatti per questo passaggio serve il teorema del cambio di variabile che vedremo più avanti, ma intuitivamente ha senso che se $x\to 0$ allora $-x\to 0$.
\end{proof}



    Ora vediamo un caso particolare, che come vedremo non ha soluzione per come abbiamo definito il limite. 
  \begin{esercizio}{}{}  

    \begin{equation*}
        \lim_{x\to 0} \frac{1}{x} \neq +\infty
    \end{equation*}
\end{esercizio}
   \begin{proof}
       
    Proviamo usando la definizione di limite.
\vspace{-0.10cm}
    \[\forall M > 0 \; \exists \delta > 0 : f(x) \in (M, +\infty) \; \forall x \in (x_0-\delta, x_0+\delta)\setminus\{x_0\}
    \]
    \[\forall M > 0 \; \exists \delta > 0 : \frac{1}{x} \in (M, +\infty) \; \forall x \in (0-\delta, 0+\delta)\setminus\{0\}
    \]

    Partiamo analizzando $\frac{1}{x} \in (M, +\infty)$

    \[
    \frac{1}{x} \in (M, +\infty) \Leftrightarrow \frac{1}{x} > M 
    \]

    Ricordiamo che $M>0$, quindi anche $\frac{1}{x} >0 \Leftrightarrow x>0$. Ora possiamo fare il reciproco di entrambi i  membri (visto che sono entrampi positivi)

    \[
    \frac{1}{x} > M > 0 \Leftrightarrow  0 < x < \frac{1}{M}
    \]

    Quindi fino ad ora abbiamo capito che $f(x) \in (M, +\infty)$ è uguale a dire $0 < x < \frac{1}{M}$, però nella definizione di limite abbiamo che $\forall x \in (-\delta, \delta)\setminus\{0\}$ però questo è impossibile, perchè per qualsiasi valore di $\delta$ l'intevallo comprenderà anche numeri negativi (dato che l'intervallo è $(-\delta, \delta)$ ma ciò va in contraddizione con quanto abbiamo trovato prima (ovvero che $0<x<\frac{1}{M}$. Infatti non c'è nessun valore di $\delta>0$ che valida la seguente affermazione.

    \[
    (-\delta, \delta)\setminus\{0\} \nsubseteq (0, \frac{1}{M})
    \]

    Pertanto il limite è sbagliato. Il ragionamento con $-\infty$ è analogo.

    Per poter calcolare questo limite ci serve la nozione di limite destro e limite sinistro.
    
    \end{proof}



\begin{definizione}{Limite Destro e Sinistro}{}
    
\addcontentsline{toc}{subsection}{Definizione di Limite Destro e Sinistro}

    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f:A\to \mathbb{R}$ 

    \begin{itemize}
        \item Sia $x_0$ punto di accumulazione destro, allora definiamo limite destro di $f(x)$ come 

        \[
        \lim_{x\to x_0^+}f(x) = l
        \]

        E la sua caratterizzazione sarà

        \begin{center}
            $\lim_{x\to x_0^+}f(x) = l \Leftrightarrow $ $\forall U \subseteq \mathbb{R}$ intorno di $l$ $\exists I \subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \cap (x_0,+\infty) \;\;\; f(x) \in U$
        \end{center}


         \item Sia $x_0$ punto di accumulazione sinistro, allora definiamo limite sinistro di $f(x)$ come 

        \[
        \lim_{x\to x_0^-}f(x) = l
        \]

        E la sua caratterizzazione sarà

        \begin{center}
            $\lim_{x\to x_0^-}f(x) = l \Leftrightarrow $ $\forall U \subseteq \mathbb{R}$ intorno di $l$ $\exists I \subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \cap (-\infty,x_0) \;\;\; f(x) \in U$
        \end{center}
    \end{itemize}
\end{definizione}

    Ora proviamo a risolvere il limite di prima con il limite destro.

\begin{esercizio}{}{}

    \begin{equation*}
        \lim_{x\to 0^+} \frac{1}{x} = +\infty
    \end{equation*}

\end{esercizio}

\begin{proof}
    Usiamo la definizione di limite destro

    \[
    \forall M>0 \;\;\exists \delta >0 : \; \frac{1}{x} \in (M, +\infty ) \;\; \forall x \in (0-\delta, 0+\delta) \setminus \{0\} \cap (0, +\infty)  
    \]

    Riscriviamo meglio l'ultimo termine

    \[
    (-\delta, +\delta) \setminus \{0\} \cap (0, +\infty) = (0, \delta)
    \]

    Ora possiamo fare gli stessi ragionamenti di prima 

    \[
    \frac{1}{x} \in (M, +\infty ) \Leftrightarrow \frac{1}{x} > M 
    \]

    Visto che $M>0 $ allora anche $x>0$, per lo stesso ragionamento di prima

    \[
    \frac{1}{x} > M  \Leftrightarrow 0<x <\frac{1}{M}
    \]

    Ora però il risultato è diverso da prima infatti, dopo le semplificazione, la nostra condizione del limite sarà $\forall x \in (0, \delta) \Rightarrow x \in (0, \frac{1}{M})$. Ora affinchè questa proposizione sia vera basta prendere

\vspace{-0.50cm}
    \[
        \delta \leq \frac{1}{M}
    \]

    E quindi ora il limite è verificato. Per il limite $\lim_{x\to 0^-} \frac{1}{x} = -\infty$ il ragionamento è analogo.
\end{proof}


\begin{esercizio}{}{}
    \[
    \lim_{x\to 0^-} \frac{1}{x^2} = +\infty
    \]
\end{esercizio}

\begin{proof}
    Usiamo la definizione 

    \[
        \forall M>0 \;\;\exists \delta >0 : \; \frac{1}{x^2} \in (M, +\infty ) \;\; \forall x \in (0-\delta, 0+\delta) \setminus \{0\} \cap (-\infty, 0)  
    \]

    Riscrivendo meglio la definizione

    \begin{equation}\label{eq:sx}
       \forall \delta >0 \;\;\; x \in (-\delta, 0) \Rightarrow \frac{1}{x^2}  \in (M, +\infty)        
    \end{equation}
     


    Riscriviamo il secondo termine

    \[ 
    \frac{1}{x^2}  \in (M, +\infty) \Leftrightarrow \frac{1}{x^2} > M
    \]

    Ora non abbiamo nessun problema riguardante il segno visto che $x^2>0\;\;\forall x \neq 0$, quindi possiamo invertire la disequazione
    \vspace{-0.30cm}
    \[
    x^2 < \frac{1}{M}
    \]

    Visto che, sia $x^2$ che $\frac{1}{M}$ sono positivi possiamo fare la radice quadrata ambo i membri 

    \[
    \sqrt{x^2} < \sqrt{\frac{1}{M}}
    \]
    
    \[
    |x| < \frac{1}{\sqrt{M}}
    \]

    \[
   - \frac{1}{\sqrt{M}}< x < \frac{1}{\sqrt{M}}
    \]

    Riscrivendo l'espressione (\ref{eq:sx}) 

    \[
    \forall \delta >0 \;\;\; x \in (-\delta, 0) \Rightarrow x \in \left(- \frac{1}{\sqrt{M}},  \frac{1}{\sqrt{M}}\right) 
    \]

    Questa implicazione è vera quando vale

    \[
    -\delta\geq - \frac{1}{\sqrt{M}}
    \]

    \[
    \delta \leq \frac{1}{\sqrt{M}}
    \]
    
\end{proof}

\begin{teorema}{Relazione Limite con limite Destro e Sinistro}{}
\addcontentsline{toc}{subsection}{Relazione Limite con limite Destro e Sinistro}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 \in \mathbb{R}$ punto di accumulazione di $f(x)$ e $l \in \mathbb{R} \cup \{\pm \infty\}$ allora


    \begin{equation*}
        \lim_{x\to x_0} f(x) = l \Leftrightarrow \lim_{x\to x_0^-} f(x) = \lim_{x\to x_0^+} f(x) = l
    \end{equation*}
\end{teorema}


\begin{proof}
    Visto che è una doppia implicazione dovremmo controllare entrambe le direzione

    ($\implies$) quindi, usando la definizione di limite, sappiamo che 

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \setminus \{x_0\} \; \Rightarrow \; f(x) \in U$
    \end{center}


    Se quindi sappiamo che l'affermazione è vera (per ipotesi) $\forall x \in A \cap I \setminus \{x_0\}$ allora varrà anche per un qualunque sottoinsieme, quindi la proposizione sarà vera anche per l'insieme 
    $A \cap I \cap (x_0, +\infty)$ e anche  per $A \cap I \cap (-\infty, x_0)$, che sono gli insiemi compresi nella definizione di limite destro e limite sinistro. Pertanto saranno valide anche le definizioni di limite destro e sinistro e quindi è verificata l'implicazione.


    ($\impliedby$) Se quindi esiste il limite destro e sinistro sappiamo che

    \begin{itemize}
        \item per limite destro

        \begin{center}
            $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I_1\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I_1 \cap (x_0, +\infty) \; \Rightarrow \; f(x) \in U$
        \end{center}

        \item per limite sinistro

        \begin{center}
            $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I_2\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I_2 \cap (-\infty,x_0 ) \; \Rightarrow \; f(x) \in U$
        \end{center}
    \end{itemize}

    Se noi ora, per il teorema di intersezione degli intorni, possiamo trovare un $I = I_1 \cap I_2$ intorno di $x_0$. tale che vale
\begin{center}
    $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \cap ((-\infty,x_0 ) \cup (x_0, +\infty)) \; \Rightarrow \; f(x) \in U$

\end{center}
    
    che scrivendo meglio

\begin{center}
    $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \setminus \{x_0 \} \; \Rightarrow \; f(x) \in U$

\end{center}

    Che valida la definizione di $\displaystyle\lim_{x\to x_0} f(x) = l$.
\end{proof}
\newpage
\begin{esercizio}{}{}

    Sia $f(x)=\begin{cases}
        1 & \text{se } x<0 \\ 
        200 & \text{se } x=0 \\
         1 & \text{se } x>0 \\
    \end{cases}$ Allora 

    \begin{equation*}
        \lim_{x\to 0} f(x) = 1
    \end{equation*}
    
\end{esercizio}


\begin{proof}
    Partiamo analizzando il limite sinistro 

    \[
    \lim_{x\to 0^-} f(x)
    \]


    Ora nell'intervallo $(-\infty, 0)$ la funzione assume sempre il valore $1$, quindi possiamo sostiture la funzione nel limite nel suo valore

    \[
    \lim_{x\to 0^-} f(x)=\lim_{x\to 0^-} 1 = 1
    \]

    Ora facciamo lo stesso ragionamendo con il limite destro, e visto che anche nell'intervallo $(0, +\infty)$ assume sempre il valore $1$ possiamo calcolare il limite destro

    \[
    \lim_{x\to 0^+} f(x)=\lim_{x\to 0^+} 1 = 1
    \]

    Ora, dato che il limite destro e sinistro esistono e sono uguali, per il teorema visto prima sappiamo che esiste anche il limite

     \[
    \lim_{x\to 0} f(x)= 1
    \]

    \textbf{N.B.} questo è un esempio lampante per capire che il limite studia "ciò che è attorno" ad un punto di una funzione, e al limite "non tiene conto" di cosa fa la funzione nel punto effettivo, come in questo esempio anche se $f(0) = 200$ non influenza il valore del limite. 
\end{proof}

\newpage

\begin{teorema}{Limite del Valore Assoluto di una Funzione}{}

\addcontentsline{toc}{subsection}{Limite del Valore Assoluto di una Funzione}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 \in \mathbb{R}$ punto di accumulazione di $f(x)$ e $l \in \mathbb{R}$ allora

    \begin{equation*}
        \lim_{x\to x_0} f(x) = l \implies \lim_{x\to x_0} |f(x)| = |l|
    \end{equation*}
\end{teorema}


\begin{proof}
    Inizialmente iniziamo a studiare per $l>0$, quindi per ipotesi sappiamo che 

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $l$ $\exists I\subseteq \mathbb{R}$ intorno di $x_0 :
        x \in I \implies f(x) \in U$
    \end{center}

    E dobbiamo trovare che

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $|l|$ $\exists I\subseteq \mathbb{R}$ intorno di $x_0 :
        x \in I \implies |f(x)| \in U$
    \end{center}

    Partiamo dalla prima proposizione, e riscriviamo meglio la prima parte

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $l$ 
    \end{center}

    \[
     \forall \varepsilon > 0 \;\;\; |f(x)-l| < \varepsilon
    \]

    \[
        l-\varepsilon < f(x) < l+\varepsilon
    \]

    Notiamo che se scegliamo $\varepsilon<l$ allora $l-\varepsilon > 0$ e quindi

    \[
       0 < l-\varepsilon < f(x) < l+\varepsilon
    \]

    Quindi ora tutti i termini sono positivi allora, per la seguente proprietà del valore assoluto $a >0 \implies a = |a|$ possiamo sostituire il termine $f(x)$ con $|f(x)|$

    \[
       0 < l-\varepsilon < |f(x)| < l+\varepsilon
    \]

    Ora riprendiamo il la condizione che abbiamo imposto $\varepsilon < l$, noi sappiamo, per definizione di limite, che $\varepsilon > 0$ quindi $0<\varepsilon < l$ di conseguenza $l>0$, quindi abbiamo scoperto che che anche $l$ è positivo e che quindi possiamo usare la stessa proprietà di che abbiamo usato per $|f(x)|$ per mettere il modulo

    \[
       |l|-\varepsilon < |f(x)| < |l|+\varepsilon
    \]

    Con questo siamo riusciti a verificare il limite perchè $|f(x)|$ è in un introno di $|l|$. Ora però ci manca da controllare i casi con $l<0$, e per evitare di usare la dimostrazione classica di limite usiamo uno stratagemma

    \[
    \lim_{x\to x_0} f(x) = l < 0 \iff \lim_{x\to x_0} -f(x) = -l > 0 
    \]

    Ora visto che $l<0$ allora $-l > 0$ e di conseguenza possiamo usare il teorema che abbiamo appena verificato (e possiamo applicarlo proprio perchè $-l > 0$)
    
    \[
    \lim_{x\to x_0} -f(x) = -l  \implies \lim_{x\to x_0} |-f(x)|  = |-l| \implies \lim_{x\to x_0} |f(x)|  = |l|
    \]
    
    E quindi anche per $l<0$ il risultato rimane lo stesso e quindi il limite è verificato $\forall l \neq 0$. 
\end{proof}

\addcontentsline{toc}{subsection}{Limite del Valore Assoluto di una Funzione (caso $l=0$)}

\begin{teorema}{Limite del Valore Assoluto di una Funzione (caso $l=0$)}{}

    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 \in \mathbb{R}$ punto di accumulazione di $f(x)$ e $l \in \mathbb{R}$ allora

    \begin{equation*}
        \lim_{x\to x_0} f(x) = 0 \iff \lim_{x\to x_0} |f(x)| = 0
    \end{equation*}
\end{teorema}

\begin{proof}
    Visto che c'è una dobbia implicazione controlliamo entrambi i sensi

    ($\implies$) Sappiamo che $\forall \varepsilon >0$ $\exists\delta>0$ tale che

    \[
    f(x) \in (-\varepsilon, \varepsilon) \;\;\;\; \forall x \in (x_0-\delta,x_0+\delta)
    \]

    Riscrivendo meglio il primo termine

    \[
    -\varepsilon < f(x) <  \varepsilon
    \]

    Usiamo le proprietà dei valori assoluti

    \[
    -\varepsilon < f(x) <  \varepsilon \iff 0\leq|f(x)| < \varepsilon
    \]

    Quindi ora sappiamo che il limite è verificato per $0\leq |f(x)| < \varepsilon$ ma quindi possiamo "allargare" l'intervallo e varrà comunque la proprietà e quindi

    \[
    0\leq |f(x)| < \varepsilon \implies -\varepsilon < |f(x)| < \varepsilon
    \]

    E di conseguenza è verificato il limite $\lim_{x\to x_0} |f(x)| = 0$.

    ($\impliedby$) Sappiamo che $\forall \varepsilon >0$ $\exists\delta>0$ tale che

    \[
    |f(x)| \in (-\varepsilon, \varepsilon) \;\;\;\; \forall x \in (x_0-\delta,x_0+\delta)
    \]

    Riscrivendo meglio il primo termine

    \[
    -\varepsilon < |f(x)| <  \varepsilon
    \]

    Possiamo togliere la parte $-\varepsilon$ perchè il valore assoluto è sempre positivo

    \[
    |f(x)| <  \varepsilon \iff -\varepsilon < f(x) <  \varepsilon
    \]

     e quindi è verificato il limite $\displaystyle\lim_{x\to x_0} f(x) = 0$.
\end{proof}

\textbf{N.B.} faccendo un riassunto dei due teoremi appena fatti sappiamo che 

\[
\lim_{x\to x_0} f(x) = l \implies \lim_{x\to x_0} |f(x)| = |l|
\]\[
\lim_{x\to x_0} f(x) = l \iff \lim_{x\to x_0} |f(x)| = 0
\]

E vedendo bene notiamo che se $\displaystyle\lim_{x\to x_0} |f(x)| = |l|$ allora non possiamo dire nulla su $\displaystyle\lim_{x\to x_0} f(x) = l $. Vediamo un esempio.

\newpage
\begin{esercizio}{}{}
    Sia $f(x)=\begin{cases}
        1 & \text{se }x\leq 0 \\
        -1 & \text{se }x> 0
    \end{cases}$  

\end{esercizio}
possiamo notare che 
    \[
    |f(x)|=\begin{cases}
        |1| & \text{se }x\leq 0 \\
        |-1| & \text{se }x> 0
    \end{cases} = \begin{cases}
        1 & \text{se }x\leq 0 \\
        1& \text{se }x> 0
    \end{cases} \iff  |f(x)| = 1
    \]

    E che quindi 

    \[
    \lim_{x\to 0} |f(x)| = \lim_{x\to 0} 1 = 1
    \]

    Ora proviamo a vedere $\lim_{x\to 0} f(x)$, e visto che è una funzione definita a tratti facciamo il limite destro e sinistro. Partiamo con quello sinistro e vediamo che la funzione nell'intervallo $(-\infty, 0) $ assume il valore $1$ quindi

    \[
     \lim_{x\to 0^-} f(x) = \lim_{x\to 0^-} 1 = 1
    \]

    Con il limite destro e la nostra funzione nell'intervallo $(0, +\infty)$ assume il valore $-1$ e quindi

    \[
     \lim_{x\to 0^+} f(x) = \lim_{x\to 0^-} -1 = -1
    \]

    Notiamo che 
    \[
     \lim_{x\to 0^-} f(x) \ne \lim_{x\to 0^+} f(x) \implies\nexists\lim_{x\to 0} f(x)
    \]
    


\begin{teorema}{ Permanenza del Segno}{}

\addcontentsline{toc}{subsection}{Teorema della Permanenza del Segno}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f:A\to\mathbb{R}$, $x_0$ punto di accumulazione di $f(x)$ e $l=\lim_{x\to x_0} f(x)$ allora 

    \begin{itemize}
        \item Se $l>0$ allora $f(x) >0$ definitivamente per $x\to x_0$
        \item Se $l<0$ allora $f(x) <0$ definitivamente per $x\to x_0$
    \end{itemize}
\end{teorema}

\begin{proof}
    Facciamo la dimostrazione per $l>0$, gli altri casi sono analoghi.

    Per ipotesi sappiamo che il limite esiste, e pertanto 

    \begin{center}
        $\forall \varepsilon>0$ $\exists I$ intorno di $x_0$ tale che $f(x) \in (l-\varepsilon, l+\varepsilon) \;\;\; \forall x \in A\cap (I \setminus \{x_0\}) $
    \end{center}

    Se scelgo $\varepsilon < l$ avrò che 

    \[
    \varepsilon < l \implies l-\varepsilon >0 \implies 0 < l-\varepsilon < f(x)
    \]

    di conseguenza

    \[
    f(x) > 0 \;\;\; \forall x \in A\cap (I \setminus \{x_0\})
    \]

    
\end{proof}
\newpage

\addcontentsline{toc}{subsection}{Limiti e Relazioni d'Ordine \textit{I}}
\begin{teorema}{limiti e relazioni d'ordine \textit{I}}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $l_1=\lim_{x\to x_0} f(x)$ e   $l_2=\lim_{x\to x_0} g(x)$

  
   \begin{center}
       $l_1 < l_2 \implies f(x) < g(x)$ definitivamente per $x\to x_0$
   \end{center}
    

\end{teorema}

\begin{proof}
    Dato che $l_1 < l_2$ sappiamo che $l_1 \ne l_2$ e quindi per il teorema di separazione degli intorni $\exists U_1$ intorno di $l_1$ e $\exists U_2$ intorno di $l_2$ tali che 

    \begin{equation}\label{eq:not}
        U_1 \cap U_2 = \varnothing
    \end{equation}

    Ora per definizione di limite sappiamo 

    \begin{itemize}
        \item $\forall U_1$ intorno di $l_1$ $\exists I_1$ intorno di $x_0$ tale che $f(x) \in U_1 \;\;\; \forall x \in A\cap (I_1 \setminus \{x_0\}) $

        \item $\forall U_2$ intorno di $l_2$ $\exists I_2$ intorno di $x_0$ tale che $g(x) \in U_2 \;\;\; \forall x \in A\cap (I_2 \setminus \{x_0\}) $
    \end{itemize}


    Ora se dato che abbiamo $I_1$ e $I_2$ intorni di $x_0$, per il teorema di intersezione sappiamo 

   \begin{center}
       $\exists I = I_1 \cap I_2$ intorno di $x_0$
   \end{center}

   E quindi nell'intorno $I$ varrà
    
    \begin{equation}\label{eq:fg}
        f(x) \in U_1 \land g(x) \in U_2 \;\;\; \forall x \in A\cap (I \setminus \{x_0\}) 
    \end{equation}

    riscriviamo l'equazione (\ref{eq:not})

    \[
        (l_1 - \varepsilon_1,l_1 + \varepsilon_1) \cap (l_2 - \varepsilon_2,l_2 + \varepsilon_2)  = \varnothing 
    \] 

    Dato che per ipotesia sappiamo $l_1 < l_2$, cioò può accedere soltanto se 

    \[
    \mathunderline{red}{l_1 + \varepsilon_1 < l_2 - \varepsilon_2}
    \]

\vspace{-0.35cm}
    Ora usiamo questa informazione e combiniamola con la formula (\ref{eq:fg})
    
    \[
    f(x) \in (l_1 - \varepsilon_1,l_1 + \varepsilon_1)
    \land g(x) \in (l_2 - \varepsilon_2,l_2 + \varepsilon_2)
    \]
    \[
    \mathunderline{blue}{l_1 - \varepsilon_1 <f(x) < l_1 + \varepsilon_1} \land \mathunderline{blue}{l_2 - \varepsilon_2 <g(x)< l_2 + \varepsilon_2}
    \]
    \vspace{-0.35cm}
    \[
    \mathunderline{blue}{f(x) < l_1} +\mathunderline{red}{ \varepsilon_1 < l_2} - \mathunderline{blue}{\varepsilon_2 <g(x)}
    \]
    \vspace{-0.35cm}
    \[
    f(x) < g(x) \;\;\;\forall x \in A\cap (I \setminus \{x_0\}) 
    \]
\end{proof}

\newpage

\begin{teorema}{limiti e relazioni d'ordine \textit{II}}{}

\addcontentsline{toc}{subsection}{Limiti e Relazioni d'Ordine \textit{II}}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $l_1=\lim_{x\to x_0} f(x)$ e   $l_2=\lim_{x\to x_0} g(x)$

  
   \begin{center}
       $f(x) \leq g(x)$ definitivamente per $x\to x_0 \implies l_1 \leq l_2 $
   \end{center}
    

\end{teorema}

\begin{proof}
    La dimostrazione segue per assurdo, quindi supponiamo che $l_1 > l_2$, allora per il teorema della relazione d'ordine I sappiamo che 

    \begin{center}
       $l_1 > l_2 \implies f(x) > g(x)$ definitivamente per $x\to x_0$
   \end{center}

   Ma ciò va in contraddizione con le ipotesi iniziali $f(x) < g(x)$ pertanto è impossibile che $l_1> l_2$ e di conseguenza è vero che $l_1 \leq l_2$. 
\end{proof}

\begin{esercizio}{}{}
    \textbf{N.B.} se $f(x) < g(x)$ non possiamo dire con certezza nulla su $l_1 < l_2$. Vediamo un esempio. Sia $f(x) = 0$ e $g(x) = x^2$. Noi sappiamo che 

    \[
    f(x) < g(x) \;\;\; \forall x \in \mathbb{R} \setminus \{0\}
    \]

    Ma i limiti per $x\to 0$ fanno $\displaystyle\lim_{x\to 0} f(x) =\lim_{x\to 0} g(x) = 0$.
\end{esercizio}


\begin{teorema}{Due Carabinieri}{}

\addcontentsline{toc}{subsection}{Teorema dei due Carabinieri}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g,h:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $\lim_{x\to x_0} f(x)=\lim_{x\to x_0} h(x) = l$.

    
    \[
    f(x) \leq g(x) \leq h(x) \implies \lim_{x\to x_0} g(x) = l
    \]
\end{teorema}

\begin{proof}
    Visto che $f(x)$ e $h(x)$ hanno limite, sappiamo che

    \begin{itemize} 
    \centering
        \item $\forall U$ intorno di $l$ $\exists I_1$ intorno di $x_0$ tale che $f(x) \in U \;\;\; \forall x \in A\cap (I_1 \setminus \{x_0\}) $

        \item $\forall U$ intorno di $l$ $\exists I_2$ intorno di $x_0$ tale che $h(x) \in U \;\;\; \forall x \in A\cap (I_2 \setminus \{x_0\}) $
    \end{itemize}

    Ora per il teorema di intersezione degli intorni sappiamo che

    \begin{center}
        $\exists I = I_1 \cap I_2$ intorno di $x_0$
    \end{center}

    In $I$ vale 

    \[
    f(x) \in U \land h(x) \in U \;\;\; \forall x \in A\cap (I \setminus \{x_0\})
    \]

    Pertanto sappiamo che 

    \[
    \mathunderline{red}{l - \varepsilon <f(x)} < l + \varepsilon \land  - \varepsilon <\mathunderline{red}{h(x) < l + \varepsilon}
    \]
\newpage
    Combinando questa informazione con le ipotesi ($\mathunderline{blue}{f(x) \leq g(x) \leq h(x)}$ definitivamente per $x\to x_0$)

    \[
    \mathunderline{red}{l - \varepsilon <f}(\mathunderline{blue}{x)\leq g(x) \leq h }(\mathunderline{red}{x) < l + \varepsilon}
    \]

    Di conseguenza 

    \[
    l - \varepsilon < g(x) < l + \varepsilon\;\;\; \forall x \in A\cap (I \setminus \{x_0\})
    \]

    E questà è la definizione di limite, quindi questo implica che 

    \[
    \exists \lim_{x\to x_0} g(x) =  l
    \]
    
\end{proof}


\begin{corollario}{Teorema dei carabinieri II}{}
Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $f(x) \leq g(x)$ definitivamente per $x\to x_0$. Allora


    \begin{itemize}
    \centering
        \item se $\displaystyle  \lim_{x\to x_0} f(x) = +\infty \implies \lim_{x\to x_0} g(x) = +\infty$
        \item se $\displaystyle  \lim_{x\to x_0} g(x) = -\infty \implies \lim_{x\to x_0} f(x) = -\infty$
    \end{itemize}

\end{corollario}

\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{\sin(x)}{x} = 1
    \]
\end{esercizio}

\begin{proof}
Iniziamo disegnando una circonferenza unitaria e notiamo che

\begin{center}
    

\begin{tikzpicture}
  \begin{axis}[
  xmin=0, xmax=1.3,
ymin=0, ymax=1.3,
axis equal,
axis lines=middle,
enlargelimits=false,
clip=false,
axis line style={-stealth, thick},
 width=8cm
]


    % --- Circonferenza unitaria ---
    \addplot[
      domain=0:90,
      samples=200,
      black,
       thick
    ] ({cos(x)}, {sin(x)});

    % --- Raggio ---
    \addplot[gray, ultra thick] coordinates {(0,0) (1, {tan(40)})};


    % --- Arco dell’angolo (sulla circonferenza) ---
    \addplot[blue, ultra thick, domain=0:40, samples=100]
      ({cos(x)}, {sin(x)});

    % --- Seno (rosso, verticale) ---
    \addplot[red, ultra thick] coordinates {({cos(40)}, 0) ({cos(40)}, {sin(40)})};



    \addplot[green, ultra thick] coordinates {(1, 0) (1, {tan(40)})};

    % --- Etichette ---
    \node[blue] at (axis cs:{cos(40*0.7)+0.01}, {sin(40*0.7)+0.1}) {$x$};
    \node[red] at (axis cs:{cos(40)-0.15},{sin(40)/2}) {$\sin(x)$};
    \node[green!50!black, right] at (axis cs:1,0.84) {$\tan(x)$};

  \end{axis}
\end{tikzpicture}
\end{center}

Dal grafico possiamo notare che in un intorno di 0 abbiamo che

\[
\sin(x) \leq x \le \tan(x)
\]
 \newpage
Ora possiamo  dividere tutto per $\sin(x)$

\[
\frac{\sin(x)}{\sin(x)} \leq \frac{x}{\sin(x)} \le \frac{\tan(x)}{\sin(x)}
\]

\[
1 \leq \frac{x}{\sin(x)} \le \frac{1}{\cos(x)}
\]

inveriamo tutti i membri (e anche i segni delle disequazioni)

\[
1 \geq \frac{\sin(x)}{x} \ge \cos(x)
\]


Ora vediamo che $\displaystyle\lim_{x\to 0} 1 = 1$,$\displaystyle\lim_{x\to 0} \cos(x) = \cos(0)=  1$, e visto che le due funzioni estreme tendono entrambe a $1$ e la funzione $\frac{\sin(x)}{x} $ è compresa tra le altre due funzioni definitivamente per $x\to x_0$ allora per il teorema dei carabinieri abbiamoche 

\[
\lim_{x\to 0} \frac{\sin(x)}{x} = 1
\]

\end{proof}
\vspace{-0.50cm}
\begin{teorema}{Algebra dei Limiti Finiti}{}
\addcontentsline{toc}{subsection}{Algebra dei Limiti Finiti}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $\displaystyle\lim_{x\to x_0} f(x)=l_1\in\mathbb{R}$ e $ \displaystyle\lim_{x\to x_0} g(x) = l_2 \in \mathbb{R}$. Allora

\begin{enumerate}[label=(\roman*)]

    \centering
    \item $\displaystyle\lim_{x\to x_0} [f(x)+g(x)] = l_1+l_2$
    \item $\displaystyle \lim_{x\to x_0} [f(x)\cdot g(x)] = l_1\cdot l_2$
    \item $\displaystyle\lim_{x\to x_0} \frac{f(x)}{g(x)} = \frac{l_1}{l_2}$ (se $l_2 \ne 0$)
\end{enumerate}

\end{teorema}


\begin{proof}
    $(i)$ Partiamo scrivendo le definizioni di limite come sappiamo 
    \begin{itemize} 
    \centering
        \item $\forall \varepsilon>0$ $\exists I_1$ intorno di $x_0$ tale che $f(x) \in (l_1-\varepsilon, l_1+\varepsilon) \;\;\; \forall x \in A\cap (I_1 \setminus \{x_0\}) $

        \item $\forall \varepsilon>0$ $\exists I_2$ intorno di $x_0$ tale che $g(x) \in (l_2-\varepsilon, l_2+\varepsilon) \;\;\; \forall x \in A\cap (I_2 \setminus \{x_0\}) $
    \end{itemize}

    Quindi per il teorema di intersezione trovo un $I= I_1 \cap I_2$ intorno di $x_0$ tale che

    \[
    \forall \varepsilon>0\;\; f(x) \in (l_1-\varepsilon, l_1+\varepsilon) \land g(x) \in (l_2-\varepsilon, l_2+\varepsilon)
    \]
    \[
    \mathunderline{red}{l_1-\varepsilon <f(x)< l_1+\varepsilon} \;\;\;\;\; \mathunderline{blue}{l_2-\varepsilon<g(x)< l_2+\varepsilon}
    \]
    \[
    \mathunderline{red}{l_1-\varepsilon} +\mathunderline{blue}{l_2-\varepsilon}<\mathunderline{red}{f(x)}+\mathunderline{blue}{g(x)}< \mathunderline{red}{l_1+\varepsilon}+\mathunderline{blue}{l_2+\varepsilon}
    \]
    \[
    (l_1 +l_2)-2\varepsilon<f(x)+g(x)< (l_1+l_2)+2\varepsilon
    \]

E notiamo che $f(x)+g(x)$ è in un intorno di $l_1+l_2$ e che quindi il limite è verificato.

\textbf{N.B.} anche se c'è scritto $2\varepsilon$ e non solamente $\varepsilon$ va bene lo stesso, anche perchè l'espressione all'interno della definizione di limite è $\forall \varepsilon>0$ e quindi anche se moltiplico $\varepsilon$ per una qualsiasi costante, potrò rappresentare qualunque intorno.

\newpage

$(ii)$ Partiamo analizzando il seguente modulo, e compensando il termine $l_1\cdot g(x)$


\[
    |f(x)\cdot g(x) - l_1\cdot l_2| = |f(x)\cdot g(x)-\mathunderline{red}{l_1\cdot g(x)} + \mathunderline{red}{l_1\cdot g(x)} - l_1\cdot l_2| \]

    Ora raccogliamo alcuni termini
\[
|f(x)\cdot \mathunderline{red}{g(x)}-l_1\cdot \mathunderline{red}{g(x)} + \mathunderline{blue}{l_1}\cdot g(x) - \mathunderline{blue}{l_1}\cdot l_2| = |(f(x)-l_1)\cdot \mathunderline{red}{g(x)} + \mathunderline{blue}{l_1}\cdot (g(x)-l_2)|
\]

Applichiamo la disuguaglianza triangolare


\begin{align*}
    |(f(x)-l_1)\cdot g(x) + l_1\cdot (g(x)-l_2)| &\leq |(f(x)-l_1)\cdot g(x)| + |l_1\cdot (g(x)-l_2)| \\
    &=|(f(x)-l_1)|\cdot |g(x)| + |l_1|\cdot |(g(x)-l_2)| 
\end{align*}

Ora dalle definizioni di limite sappiamo che $|f(x)-l_1| < \varepsilon$ e  $|g(x)-l_2| < \varepsilon$

\[
\mathunderline{red}{|(f(x)-l_1)|}\cdot |g(x)| + |l_1|\cdot \mathunderline{blue}{|(g(x)-l_2)|} < \mathunderline{red}{\varepsilon}\cdot |g(x)| + |l_1|\cdot\mathunderline{blue}{\varepsilon} = \varepsilon\cdot(|g(x)| + |l_1|)
\]

Per valutare la quanto vale $|g(x)|$ facciamo qualche sistemazione algebrica

\begin{align*}
    |g(x)| &= |g(x) - l_2 + l_2|\\
    &\leq | g(x) - l_2| + |l_2|\\
    &< \varepsilon + |l_2|
\end{align*}

Con questo possiamo semplificare

\[
\varepsilon\cdot (|g(x)| + |l_1|)< \varepsilon\cdot ((\varepsilon+|l_2|) + |l_1|)
\]

Per semplificare possiamo scegliere $\varepsilon<1$

\[
\varepsilon\cdot ((1+|l_2|) + |l_1|) = \varepsilon\cdot (1+|l_2| + |l_1|)
\]

Facendo un po' di ordine vediamo che 

\[
|f(x)\cdot g(x) - l_1\cdot l_2| < \varepsilon\cdot (1+|l_2| + |l_1|)
\]


Di Conseguenza abbiamo trovato che $|f(x)\cdot g(x) - l_1\cdot l_2|$ è sempre minore di $\varepsilon$, appatto di qualche costante proporzionale. Infatti $1+|l_2| + |l_1|$ è sempre maggiore di $1$ e quindi il limite è verificato.

\newpage

\textit{($iii$)} Per verificare questo limite è necessario verificare che 

\begin{equation}\label{eq:divLim1}
\lim_{x\to x_0} \frac{1}{g(x)} = \frac{1}{l_2}
\end{equation}

Perchè se fosse vero potremmo usare il teorema del prodotto perchè

\begin{equation}\label{eq:divLim2}
    \lim_{x\to x_0} \frac{f(x)}{g(x)} = \lim_{x\to x_0}\left[  f(x)\cdot \frac{1}{g(x)}\right] = l_1 \cdot \frac{1}{l_2} = \frac{l_1}{l_2}
\end{equation}


Quindi proviamo a verificare (\ref{eq:divLim1}) con la definizione di limite

\[
    \left| \frac{1}{g(x)} - \frac{1}{l_2} \right| = \left| \frac{l_2 - g(x)}{g(x)\cdot l_2}\right| = \frac{|g(x) - l_2|}{|g(x)||l_2|}
\]


Visto che il limite $\displaystyle\lim_{x\to x_0} g(x) = l_2$ è verificato per ipotesi, allora sappiamo $\exists I_1$ intorno di $x_0$ tale che $|g(x) - l_2|< \varepsilon$ $\forall x \in I$,  e quindi 

\[
\forall x \in I_1 \;\;\; \frac{|g(x) - l_2|}{|g(x)||l_2|} < \frac{\varepsilon}{|g(x)||l_2|}
\]

Ora per capire quanto vale $|g(x)|$ dobbiamo dividere i casi con $l_2 >0$ e $l_2 <0$, noi ora vedremo la dimostrazione per $l_2 >0$, l'altro caso è analogo.

Quindi sfruttando il teorema della permanenza del segno noi sappiamo che $exists I_2$ intorno di $x_0$ tale che $g(x)>0 $ $\forall x \in I_2$, di conseguenza sarà vero anche che $\forall x \in I_2$  $g(x) > \frac{l_2}{2}$, e quindi anche $\frac{1}{g(x)} < \frac{2}{|l_2|}$.

Possiamo usare il teorema di intersezione degli intorno per trovare $I_3 = I_1 \cap I_2$ intorno di $x_0$ tale che 


\[
\forall x \in I_3 \;\;\; \frac{|g(x) - l_2|}{|g(x)||l_2|} < \frac{\varepsilon}{|g(x)||l_2|} = \frac{\varepsilon}{|l_2|} \cdot \frac{1}{|g(x)|} < \frac{\varepsilon}{|l_2|} \cdot \frac{2}{|l_2|} = \frac{2\varepsilon}{|l_2|^2}
\]

E che quindi 
\[
\left| \frac{1}{g(x)} - \frac{1}{l_2} \right|  < \frac{2\varepsilon}{|l_2|^2}
\]

Per lo stesso ragionamento fatto prima nel prodotto, abbiamo trovato che il limite è minore di $\varepsilon$ appatto di una costante moltiplicativa.

Quindi abbiamo trovato un intevallo $I_3$ che verifica il limite $\displaystyle\lim_{x\to x_0} \frac{1}{g(x)} = \frac{1}{l_2}$ e che quindi usando anche il teorema del prodotto, si verifica il teorema della divisione, come visto nel punto (\ref{eq:divLim2}). Chiaramente visto che $g(x)$ è a denominatore è necessario che $g(x) \ne 0$ definitivamente per $x\to x_0$.
\end{proof}

\newpage 


\addcontentsline{toc}{subsection}{Algebra dei Limiti Infiniti (Forme Determinate)}
\begin{teorema}{Algebra dei Limiti Infiniti (Forme Determinate)}{}
        Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$. Allora


    \begin{enumerate}[label=(\roman*)]

        \centering
        \item 
        Se $\displaystyle\lim_{x\to x_0} f(x) = \color{red}\pm \color{black}\infty$ e $g(x)$ è definitivamente limitata per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} [f(x)\pm g(x)] = \color{red}\pm \color{black}\infty\]

    
        \item Se $\displaystyle\lim_{x\to x_0} f(x) = 0$ e $g(x)$ è definitivamente limitata per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} [f(x)\cdot g(x)] = 0]\]

        \item Se $\displaystyle\lim_{x\to x_0} f(x) = \color{red}\pm \color{black}\infty$ e $\exists c > 0 : g(x) \leq c$ definitivamente per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} [f(x)\cdot g(x)] = \color{red}\pm \color{black}\infty\]
    
        \item Se $\displaystyle\lim_{x\to x_0} f(x) = \color{red}\pm \color{black}\infty$ e $\exists c > 0 : 0 < g(x) \leq c$ definitivamente per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} \frac{f(x)}{g(x)} = \color{red}\pm \color{black}\infty\]
    
        \item Se $\displaystyle\lim_{x\to x_0} f(x) = \pm \infty$ e $g(x)$ è definitivamente limitata per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} \frac{g(x)}{f(x)} = 0\]

        \item Se $\displaystyle\lim_{x\to x_0} f(x) = 0$, $f(x)$ è positiva definitivamente per $x\to x_0$ e $\exists c > 0 : g(x) > c$ definitivamente per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} \frac{g(x)}{f(x)} = +\infty\]
        
        \item Se $\displaystyle\lim_{x\to x_0} f(x) = 0$, $f(x)$ è negativa definitivamente per $x\to x_0$ e $\exists c > 0 : g(x) > c$ definitivamente per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} \frac{g(x)}{f(x)} = -\infty\]
    \end{enumerate}


    
\end{teorema}

\newpage

\addcontentsline{toc}{subsection}{Esercizi sull'Algebra dei Limiti Infiniti}
\begin{esercizio}{}{}
    \[
        \lim_{x \to +\infty} x^2 + \sin(x) = +\infty
   \]
\end{esercizio}

\begin{proof}
    Vediamo che 
    \[
    \lim_{x \to +\infty} x^2 = +\infty
    \]

    In più $|\sin(x)| \leq 1$ $\forall x \in \mathbb{R}$ e quindi lo è anche definitivamente per $x\to +\infty$, Quindi come nella casistica $(i)$ dell'algebra dei limiti finiti abbiamo che 
\[\lim_{x \to +\infty} x^2 + \sin(x) = +\infty\]

\end{proof}


\begin{esercizio}{}{}
    \[
        \lim_{x \to +\infty} x(\sin(x)+2) = +\infty
   \]
\end{esercizio}

\begin{proof}
    Notiamo che 
    \[
    \lim_{x \to +\infty} x = +\infty
    \]

    In più $0< \sin(x) +2 \leq 3$ $\forall x \in \mathbb{R}$ e quindi lo è anche definitivamente per $x\to +\infty$, Quindi come nella casistica $(iv)$ dell'algebra dei limiti finiti abbiamo che 
\[\lim_{x \to +\infty} x(\sin(x)+2) = +\infty\]

\end{proof}


\begin{esercizio}{}{}
    \[
        \lim_{x \to -2} \frac{x}{(x+2)^2} = +\infty
   \]
\end{esercizio} 



\begin{proof}
    Se proviamo a calcolare il limite notiamo che il denominatore tende a 0, mentre il numeratore tende a -2 quindi sembra di essere nella casistica ($vi$), controlliamo se le ipotesi sono verificate. 

    In primis il teorema richiede che $f(x)$ sia positivo definitivamente per $x\to -2$, è questo è verificato sempre, infatti $(x+2)^2 > 0 \implies \forall x \ne -2 $. In più il numeratore ($x$) è limitato definitivamente per $x\to -2$, pertanto il teorema è applicabile e quindi 
    \[
        \lim_{x \to -2} \frac{x}{(x+2)^2} = +\infty
   \]

    \textbf{N.B.} Se il limite fosse stato $\displaystyle\lim_{x\to -2}\frac{x}{x+2}$ il teorema non sarebbe applicabile, perchè $x+2$ non è positiva definitivamente per $x\to -2$, perchè un qualsiasi intorno dalla parte sinistra sarebbe negativo e invece la parte destra sarebbe positiva. Pertanto non si può applicare il teorema ($vi$). Per risolverlo è necessario calcolare o il limite destro o sinistro, infatti in quei intorni ($x+2$) è positivo definitivamente. Quindi $\displaystyle\lim_{x\to -2^-}\frac{x}{x+2} = +\infty$ e $\displaystyle\lim_{x\to -2^+}\frac{x}{x+2} = -\infty$. E da questo notiamo che $\nexists \displaystyle\lim_{x\to -2}\frac{x}{x+2}$ perchè il limite destro e sinistro sono diversi.
\end{proof}


\addcontentsline{toc}{subsection}{Forme Indeterminate}
\begin{definizione}{Forme Indeterminate}
    SSi dicono \textbf{Forme Indeterminate} tutti i limiti che hanno come risultato 

   \[
\begin{array}{@{\qquad}c@{\qquad}c@{\qquad}c@{\qquad}}
\bigl[\dfrac{\infty}{\infty}\bigr] & \bigl[\dfrac{0}{0}\bigr] & \bigl[\infty \cdot 0\bigr] \\[6pt]
\bigl[+\infty - \infty\bigr] & \bigl[\infty^{0}\bigr] & \bigl[1^{0}\bigr]
\end{array}
\]


    E il risultato effettivo del limite non si può determinare subito, ma sono necessarie altre operazioni.

    \textbf{N.B.} Pertanto due limiti che hanno inizialmente la stessa forma indeterminata posso avere limiti diversi, vediamo degli esempi.
\end{definizione}



\addcontentsline{toc}{subsection}{Primi Esercizi sulle Forme Indeterminate}

\begin{esercizio}{}{}
    \[
    \lim_{x\to +\infty} \frac{2x^2+3x-1}{x^2 - 1}
    \]
\end{esercizio}


\begin{proof}
    Se proviamo a calcolare il limite vediamo che il numeratore tende a $+\infty$ e lo stesso si può dire per il denominatore. Quindi caschiamo nella forma indeterminata del tipo $\bigl[\dfrac{\infty}{\infty}\bigr]$. Pertanto dobbiamo fare delle manipolazioni, proviamo raccogliendo il grado maggiore ($x^2$) al numeratore e lo stesso facciamo anche a demonimatore
    \[
    \lim_{x\to +\infty} \frac{2x^2+3x-1}{x^2 - 1} = \lim_{x\to +\infty} \frac{x^2\left(2+\frac{3}{x}-\frac{1}{x^2}\right)}{x^2\left(1-\frac{1}{x^2}\right)}
    \]

    Notiamo che il termine $x^2$ si può semplificare

    \[
     \lim_{x\to +\infty} \frac{\cancel{x^2}\left(2+\frac{3}{x}-\frac{1}{x^2}\right)}{\cancel{x^2}\left(1-\frac{1}{x^2}\right)} = \lim_{x\to +\infty} \frac{2+\frac{3}{x}-\frac{1}{x^2}}{1-\frac{1}{x^2}}
    \]

    Ora possiamo calcolare il limite infatti i termini $\frac{3}{x}$, $\frac{1}{x^2}$ tendono a 0 quando $x\to \infty$ (questo grazie alle forme determinate) e quindi 

    \[
        \lim_{x\to +\infty}
        \frac{2+\circled[red!75!black]{\frac{3}{x}}-\circled[red!75!black]{\frac{1}{x^2}}}{1-\circled[red!75!black]{\frac{1}{x^2}}} = \frac{2 + 0 - 0}{1 - 0} = 2
        \]


    Quindi 

     \[
    \lim_{x\to +\infty} \frac{2x^2+3x-1}{x^2 - 1} = 2
    \]

    Quindi noi siamo partiti con una forma indeterminata e siamo arrivati a una soluzione che è 2. Ora vediamo che un altro limite sempre con la stessa forma indeterminata, ma avremo un altro risultato.
\end{proof}

\newpage

\begin{esercizio}{}{}
    \[
    \lim_{x\to +\infty} \frac{x^3+5x}{x^2 +7x - 1}
    \]
\end{esercizio}

\begin{proof}
    Notiamo subito che esce la stessa forma indeterminata: $\bigl[\dfrac{\infty}{\infty}\bigr]$ e quindi proviamo a fare la stessa tecnica di prima

    \[
    \lim_{x\to +\infty} \frac{x^3+5x}{x^2 +7x - 1} = \frac{x^3\left(1 + \frac{5}{x^2}\right)}{x^2\left(1 + \frac{7}{x}-\frac{1}{x^2}\right)} = \frac{x\left(1 + \frac{5}{x^2}\right)}{1 + \frac{7}{x}-\frac{1}{x^2}} 
    \]

    Ora come prima i termini con la $x$ a denominatore tendono a 0, però a numeratore è rimasto una $x$ che tende a $+\infty$ quindi il numeratore, per la proprietà ($iii$) delle forme determinate, tende a $+\infty$, il denominatore invece tende a 1, e quindi per la proprietà ($iv$) il limite tende a $+\infty$.
\[
    \lim_{x\to +\infty} \frac{x^3+5x}{x^2 +7x - 1} = +\infty
    \]

    \textbf{N.B.} inizialmente anche questo limite era della forma $\bigl[\dfrac{\infty}{\infty}\bigr]$ ma abbiamo avuto un risultato diverso da prima, e quindi quando ci troviamo davanti una forma indetermnata sappiamo che dobbiamo rimaneggiare i termini.
\end{proof}


\begin{esercizio}{}{}
    \[
    \lim_{x\to +\infty} \sqrt{x^2+x+1} - x
    \]
\end{esercizio}

\begin{proof}
    Proviamo a calcolare il limite ma notiamo subito che viene fuori una forma indeterminata della forma $\bigl[+\infty - \infty\bigr]$ e quindi dobbiamo fare dei rimaneggiamenti. Ricordandoci la formula della somma per differenza ($(A+B)(A-B)=A^2-B^2$) possiamo moltiplicare e dividere per il binomio coniugato
    \begin{align*}
        \lim_{x\to +\infty} \sqrt{x^2+x+1} - x &=     \lim_{x\to +\infty} (\sqrt{x^2+x+1} - x) \cdot \frac{\sqrt{x^2+x+1} + x}{\sqrt{x^2+x+1} + x} \\ 
        &=  \lim_{x\to +\infty}   \frac{(\sqrt{x^2+x+1} - x)(\sqrt{x^2+x+1} + x)}{\sqrt{x^2+x+1} + x} \\ 
        &= \lim_{x\to +\infty} \frac{(\sqrt{x^2+x+1})^2 - x^2}{\sqrt{x^2+x+1} + x} \\
        &= \lim_{x\to +\infty} \frac{ x^2+x+1 - x^2}{\sqrt{x^2+x+1} + x} \\
        &= \lim_{x\to +\infty} \frac{ x+1 }{\sqrt{x^2+x+1} + x}
    \end{align*}
    
    Dopo tutti questi maneggiamenti sembra che abbiamo solo che complicato il limite, però li abbiamo fatto diventare in un limite nella forma $\bigl[\dfrac{\infty}{\infty}\bigr]$ che però abbiamo già visto come risolvere, infatti basta che raccogliamo il grado maggiore 

    \[
    \lim_{x\to +\infty} \frac{ x+1 }{\sqrt{x^2+x+1} + x} = \lim_{x\to +\infty} \frac{ x\left(1+\frac{1}{x}\right) }{\sqrt{x^2\left(1+\frac{1}{x}+\frac{1}{x^2}\right)} + x}
    \]

    Ora per "tirare fuori" $x^2$ dalla radice, dobbiamo ricordarci di mettere il modulo (perchè $\sqrt{x^2} = |x|$), però dato che noi stiamo analizzando per $x\to +\infty$ siamo sicuri che $x>0$ (per definizione di limite) e pertanto $|x| = x$

    \[
    \lim_{x\to +\infty} \frac{ x\left(1+\frac{1}{x}\right) }{x\sqrt{\left(1+\frac{1}{x}+\frac{1}{x^2}\right)} + x} = \lim_{x\to +\infty} \frac{ x\left(1+\frac{1}{x}\right) }{x\cdot\left(\sqrt{1+\frac{1}{x}+\frac{1}{x^2}} + 1\right)} = \lim_{x\to +\infty} \frac{ 1+\frac{1}{x}}{\sqrt{1+\frac{1}{x}+\frac{1}{x^2}} + 1}
    \]

    Possiamo calcolare il limite 

    \[
    \lim_{x\to +\infty} \frac{ 1+\circled[red!75!black]{\frac{1}{x}}}{\sqrt{1+\circled[red!75!black]{\frac{1}{x}}+\circled[red!75!black]{\frac{1}{x^2}}} + 1} = \frac{1 + 0}{\sqrt{1 + 0 +0 } +1} = \frac{1}{2}
    \]
\end{proof}

\begin{esercizio}{}{}
    Dato un generico polinomio $P(x) = \displaystyle\sum_{i=0}^{n} a_i x^i$ dove $a_i$ sono i coefficenti del polinomio e $n$ il grado del polinomio. Con $a_n > 0$
    \[
        \lim_{x\to +\infty} P(x)
    \]
\end{esercizio}


\begin{proof}
    Il limite è nella forma $[+\infty -\infty]$ e quindi procediamo raccogliendo il grado maggiore ($x^n$)

    \[
        \lim_{x\to +\infty} P(x) = \lim_{x\to +\infty} (a_0 + a_1x + ... + a_nx^n) = \lim_{x\to +\infty} x^n\left(\frac{a_0}{x^n} + \frac{a_1}{x^{n-1}} + ... + a_n\right) 
    \]

    Ora possiamo calcolare il limite infatti tutti i termini dentro le parentesi infatti tendonon tutti a 0. Quindi il termine dentro le parentesi tende a $a_0$ e che quindi moltiplicato per $x^n\to +\infty$  tente a $ +\infty$. Se invece $a_n<0$ il limite tendeva a $-\infty$.

    \[
        \lim_{x\to +\infty} x^n\left(\circled[red!75!black]{\frac{a_0}{x^n}} + \circled[red!75!black]{\frac{a_1}{x^{n-1}}} + \circled[red!75!black]{...} + a_n\right)  = +\infty
    \]

    Quindi con questo iniziamo a capire che nei polinomi quello che ci interessa quando $x\to \infty$ è il termine con il grado più alto ($x^n$), intatti per risolvere questo esercizio i termini più piccoli di $x^n$ è come se li avessimo trascurati. Infatti è vera la seguente equazione $\displaystyle\lim_{x\to \infty} P(x) =\lim_{x\to \infty} a_nx^n$ per qualsiasi polinomio $P(x)$ e $\forall a_n \in \mathbb{R}\setminus \{0\}$. 
\end{proof}


\addcontentsline{toc}{subsection}{Teorema del Cambio di Variabile}
\begin{teorema}{Cambio di Variabile}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $\varnothing \ne B \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $g:B \to \mathbb{R}$ e $x_0 \in \mathbb{R} \cup \{\pm \infty\}$ un punto di accumulazione in $f(A)\cap B$ allora se $\exists \displaystyle\lim_{x\to x_0} f(x) = y_0$, con $y_0$ punto di accumulazione in $B$   e se è vera almeno una delle due proposizioni 
    \begin{itemize}
        \item $f(x) \ne y_0$ definitivamente per $x\to x_0$
        \item $\displaystyle \lim_{y\to y_0} g(y) = g(y_0)$ \;\;\;\;\; (continuità di $g(x)$)
    \end{itemize}

    Allora 
    \[
        \lim_{x\to x_0} g(f(x)) = \lim_{y\to y_0} g(y)
    \]
\end{teorema}


\begin{esercizio}{}{}
    Ora vediamo perchè è fondamentale che almeno una dei due requisiti sia vero, proviamo con un controesempio. Infatti sia $f(x)=5$ e $g(x)=\begin{cases}
        2 & \text{se } x\ne 5 \\
        1  & \text{se } x= 5 \\
    \end{cases}$ e vediamo subito che nessuna delle due proposizioni è vera. 
\end{esercizio}

\begin{proof}
    Infatti il limite effettivo, senza usare il teorema del cambio di variabile è
    \[
    \lim_{x\to x_0} g(f(x)) = \lim_{x\to x_0} g(5) = \lim_{x\to x_0} 1 = 1 
    \]

    Invece se proviamo a usare il cambio di variabile, dobbiamo prima calcolare $y_0$
    \[
    \lim_{x\to x_0} f(x) = 5\;\;\; [=y_0]
    \]

    Ora il limite diventa
    \[
    \lim_{x\to x_0} g(f(x)) = \lim_{y\to 5} g(y) = 2
    \]

    Quindi usando solo le funzioni composte il limite è uscito 1, mentre con il teorema del cambio di variabile è venuto fuori 2, cosa impossibile per il teorema di unicità del limite e pertanto il teorema del cambio di variabile non si può applicare in questo esercizio, proprio perchè mancavano i criteri richiesti dal teorema stesso. 
    
\end{proof}


\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} \frac{\sin(x^2)}{x^2}
    \]
\end{esercizio}


\begin{proof}
    Vediamo che assomiglia molto al limite $\displaystyle\lim_{x\to 0} \frac{\sin(x)}{x} = 1$ l'unica cosa che cambia è che abbiamo $x^2$ anzichè $x$, quindi proviamo a cambiare la variabile $x^2$ con $y$, quindi dobbiamo calcolare 
    \[
        \lim_{x\to 0} x^2 = 0\;\;\; [=y_0]
    \]
    Visto che sono valide tutte le condizioni del teorema del cambio di variabile, infatti $x^2 \ne 0$ in un intorno di $0$. Mentre l'altra condizione non è valida infatti non si può calcolare in $0$ la funzione $g(y)=\frac{\sin(y)}{y}$, però non ci interessa perchè il teorema richiede almeno una delle due proposizioni. 

    Quindi il limite diventa
    \[
        \lim_{x\to 0} \frac{\sin(x^2)}{x^2} = \lim_{y\to 0} \frac{sin(y)}{y} = 1
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to +\infty} e^{\frac{1}{x^2-x}}
    \]
\end{esercizio}

\begin{proof}
    Al denominatore abbiamo una forma del tipo $\bigl[+\infty - \infty \bigr]$, quindi proviamo a vedere come si comporta quel denominatore per $x\to+\infty$. Per calcolarlo possiamo usare la proprietà dei polinomi che abbiamo visto nell'esercizio dei polinomi, infatti basta tenere il grado maggiore ($x^2$)
    \[
        \lim_{x\to +\infty} x^2 - x = \lim_{x\to +\infty} x^2  = +\infty \;\;\; [=y_0]
    \]
    Vediamo che il denominatore ha limite e quindi possiamo fare il cambio di variabile e possiamo applicarlo perchè è valida la prima condizione, infatti $x^2-x \ne +\infty$ sempre, mentre la seconda non può mai essere vera perchè non possiamo calcolare $g(+\infty)$, perchè ricordiamo che $\pm \infty$ non sono punti di nessun dominio
    \[
    \lim_{x\to +\infty} e^{\frac{1}{x^2-x}} = \lim_{y\to +\infty} e^{\frac{1}{y}}
    \]
    Ora possiamo riutilizzare il teorema del cambio di variabile, visto che non siamo ancora in un limite noto, e quindi vediamo come si comporta la frazione all'esponente
    \[
    \lim_{y\to+\infty} \frac{1}{y} = 0 \;\;\; [=z_0]
    \]
    Visto che ha limite e rispetta sempre il primo criterio e anche il secondo del teorema del cambio di variabile, allora possiamo riapplicare il teorema e finalmente calcolare il limite.
    \[
    \lim_{y\to +\infty} e^{\frac{1}{y}} = \lim_{z\to 0} e^z = 1
    \]
    Quindi 
    \[
    \lim_{x\to +\infty} e^{\frac{1}{x^2-x}} = 1
    \]
\end{proof}

\newpage

\addcontentsline{toc}{subsection}{Limite di funzioni Monotone caso Finito}
\begin{teorema}{Limite di funzioni Monotone}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $I\subseteq \mathbb{R}$ intorno di $x_0 \in \mathbb{R}$ tale che $f(x)$ è monotona in $I$

    \begin{itemize}
        \item Se $f(x)$ è monotona crescente allora 
        \[
        \lim_{x\to x_0^-} f(x) = \sup\{f(A \cap I \cap (-\infty, x_0))\}
        \]
        \[
        \lim_{x\to x_0^+} f(x) = \inf\{f(A \cap I \cap (x_0, +\infty))\}
        \]
        \item Se $f(x)$ è monotona decrescente allora 
        \[
        \lim_{x\to x_0^-} f(x) = \inf\{f(A \cap I \cap (-\infty, x_0))\}
        \]
        \[
        \lim_{x\to x_0^+} f(x) = \sup\{f(A \cap I \cap (x_0, +\infty))\}
        \]
    \end{itemize}
\end{teorema}

\begin{proof}
    Faremo la dimostrazione del caso $f(x)$ è crescente e per il limite sinistro, gli altri casi sono analoghi.

    Per ipotesi chiaramente supponiamo che esista $S=\sup\{f(A \cap I \cap (-\infty, x_0))\}$, quindi per definizione di superiore, sappiamo che il superiore ($S$) è più grande di qualsiasi elemento nell'insieme (cioè $f(x)$), quindi
    \begin{equation}\label{eq:extr1}
        f(x) \leq S \;\;\;\;\; \forall x  \in A \cap I \cap (-\infty, x_0)
    \end{equation}

    Ora usando la caratterizzazione degli estremi e sappiamo che 
    \[
    f(\hat{x}) > S - \varepsilon \;\;\; \forall \varepsilon > 0 \;\; \exists \hat{x} \in A \cap I \cap (-\infty, x_0)
    \]
    Poi, per monotonia della funzione sappiamo che se $\hat{x} < x$ allora 
    \begin{equation}\label{eq:extr2}
        f(\hat{x}) < f(x) \implies S - \varepsilon < f(\hat{x}) < f(x) \;\;\; \forall \varepsilon > 0
    \end{equation}
    Ora combinando le informazioni (\ref{eq:extr1}) e (\ref{eq:extr2}) sappiamo che
    \[
    S-\varepsilon < f(x) < S \;\;\; \forall \varepsilon > 0 \;\; \forall x \in A \cap I \cap (-\infty, x_0)
    \]
    Visto che $\varepsilon >0$ sappiamo che $S < S+ \varepsilon$ e quindi 
    \[
    S-\varepsilon < f(x) < S < S+ \varepsilon
    \]
    \[
    S-\varepsilon < f(x) < S + \varepsilon \;\;\; \forall \varepsilon > 0 \;\; \forall x \in A \cap I \cap (-\infty, x_0)
    \]
    E questa non è altro che la definizione di limite
    \[
        \lim_{x\to x_0^-} f(x) = \sup\{f(A \cap I \cap (-\infty, x_0))\}
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to x_0} a^x = a^{x_0}
    \]
\end{esercizio}

\begin{proof}
    Questo è vero proprio perchè se $a>1$ la funzione $a^x$ è monotona crescente, se $a=1$ è costante e invece se $a<1$ la funzione è monotona decrescente, quindi si può sempre applicare il teorema.
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to x_0} \log_ax = \log_ax_0 \;\; \;\; \; \forall x_0 > 0
    \]
\end{esercizio}

\begin{proof}
    Possiamo fare lo stesso ragionamento del per il logaritmo, infatti se $a>1$ la funzione $\log_ax$ è monotona crescente, se $a=1$ è costante e invece se $a<1$ la funzione è monotona decrescente, quindi si può sempre applicare il teorema. L'unica cosa che cambia dall'esercizio precedente è che $x_0$ deve essere positivo, perche il dominio di $\log_ax$ è $\forall x > 0$, e di conseguenza qualsiasi punto $x_0 < 0$ non è punto di accumulazione e pertanto non può essere calcolato il limite in quel punto.
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to x_0} x^\alpha = x_0^\alpha \;\; \;\; \; \forall x_0 \ne 0 \;\; \forall \alpha \in \mathbb{R}
    \]
\end{esercizio}

\begin{proof}
    Se $\alpha > 0$ avremo una potenza che è sempre monotona crescente per $x_0 > 0$, mentre se $\alpha$ è pari allora la funzione sarà decrescente per $x_0 < 0$ mentre se $\alpha$ è dispari la funzione è crescente anche per $x < 0 $ . Se $\alpha = 0$ allora avremo una funzione costante e se $\alpha<0$ la funzione sarà del tipo $\frac{1}{x^\alpha}$ che sarà monotona decrescente.     
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to 0^+} x^\alpha = \begin{cases}
            0 & \text{se } \alpha > 0 \\
            +\infty & \text{se } \alpha < 0 \\
        \end{cases}
    \]
\end{esercizio}

\begin{proof}
    Questo è un caso particolare dell'esercizio precedente, infatti se $x\to 0^+$ allora con $\alpha>0$ avremo una forma del tipo $0^\alpha$ che chiaramente tende a 0, mentre se $\alpha < 0$ la funzione diventa $\frac{1}{x^{|\alpha|}}$ che fa tendere il denominatore a $0^+$ e che quindi fa tendere la funzione a $+\infty$.    
\end{proof}


\addcontentsline{toc}{subsection}{Limite di funzioni Monotone caso Infinito}
\begin{teorema}{Limite di funzioni Monotone caso Infinito}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $I\subseteq \mathbb{R}$ intorno di $\pm \infty $ tale che $f(x)$ è monotona in $I$

    \begin{itemize}
        \item Se $f(x)$ è monotona crescente allora 
        \[
        \lim_{x\to +\infty} f(x) = \sup\{f(A \cap I)\}
        \]
        \[
        \lim_{x\to -\infty} f(x) = \inf\{f(A \cap I )\}
        \]
        \item Se $f(x)$ è monotona decrescente allora 
        \[
        \lim_{x\to +\infty} f(x) = \inf\{f(A \cap I )\}
        \]
        \[
        \lim_{x\to -\infty} f(x) = \sup\{f(A \cap I)\}
        \]
    \end{itemize}
\end{teorema}

\begin{esercizio}{}{}
    \[
        \lim_{x\to +\infty} a^x = \begin{cases}
            +\infty & \text{se } a > 1 \\
            1 & \text{se } a = 1 \\
            0 & \text{se } 0< a < 1 \\
        \end{cases}
    \]
\end{esercizio}

\begin{esercizio}{}{}
    \[
        \lim_{x\to -\infty} a^x = \begin{cases}
            0 & \text{se } a > 1 \\
            1 & \text{se } a = 1 \\
            +\infty & \text{se } 0< a < 1 \\
        \end{cases}
    \]
\end{esercizio}

\begin{esercizio}{}{}
    \[
        \lim_{x\to +\infty} log_a(x) = \begin{cases}
            +\infty & \text{se } a > 1 \\
            -\infty & \text{se } 0< a < 1 \\
        \end{cases}
    \]
\end{esercizio}

\begin{esercizio}{}{}
    \[
        \lim_{x\to \pm\infty} |x|^\alpha = \begin{cases}
            +\infty & \text{se } \alpha > 0 \\
            1 & \text{se } \alpha = 0 \\
            0 & \text{se } \alpha < 0 \\
        \end{cases}
    \]
\end{esercizio}



\newpage
\begin{teorema}{Potenza di Funzioni}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$ e $f,g:A\to \mathbb{R}$, $x_0$ punto di accumulazione in $A$

    \[
        \lim_{x\to x_0} f(x) ^ {g(x)} 
    \]
\end{teorema}

\begin{proof}
    Per calcolare questo limite possiamo usare la continuità dell'esponenziale. Perchè il limite lo possiamo calcolare come
    \[
        f(x) ^ {g(x) }  = e ^ {\log\bigl({f(x) ^ {g(x)}}\bigr)} = e ^ {g(x) \cdot \log({f(x)}) }
    \]

    ora con il cambio di variabile possiamo fare 
    \[
    \lim_{x\to x_0} g(x) \cdot \log({f(x)}) = y_0
    \]
    \[
        \lim_{x\to x_0} f(x) ^ {g(x)} = \lim_{x\to x_0} e ^ {\log\bigl({f(x) ^ {g(x)}}\bigr)} = \lim_{y\to y_0} e^y = e^{y_0}
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to +\infty} x ^ {\frac{1}{\log(x+1)}} 
    \]
\end{esercizio}

\begin{proof}
    Usiamo il ragionamento dell'esercizio precedente, con il caso $f(x) = x$, $g(x) = \frac{1}{log(x+1)}$
    \[
        x ^ {\frac{1}{\log(x)}} = e ^ {\log\bigl(x ^ {\frac{1}{\log(x+1)}}\bigr)}= e ^ {\frac{1}{\log(x+1)} \cdot \log(x)} =e ^ {\frac{\log(x)}{\log(x+1)} } 
    \]
    ora calcoliamo il limite dell'esponente
    \[
        \lim_{x\to +\infty} = \frac{log(x)}{log(x+1)}
    \]
    Questo limite è della forma $\bigl[\frac{\infty}{\infty}\bigr]$ e pertanto proviamo a raggruppare come nei polinomio
    \[
        \lim_{x\to +\infty} = \frac{\log(x)}{\log(x+1)}  
        = \lim_{x\to +\infty}  \frac{\log(x)}{\log(x\bigl(1+\frac{1}{x}\bigr))} 
    \]
    \[
         = \lim_{x\to +\infty}\frac{\log(x)}{\log (x) + \log\bigl(1+\frac{1}{x}\bigr)} 
         = \lim_{x\to +\infty}\frac{1}{1 + \frac{\log\bigl(1+\frac{1}{x}\bigr)}{\log(x)}}
    \]
    Ora il termine $\log\bigl(1+\frac{1}{x}\bigr)$ tende a 0, invece $\log(x)$ tende a $+\infty$, quindi complessivamente la frazione tende a 0 e quindi possiamo calcolare il limite e sostituirlo  
    \[
    \lim_{x\to +\infty}\frac{1}{1 + \circled[red]{\frac{\log\bigl(1+\frac{1}{x}\bigr)}{\log(x)}}} = \frac{1}{1+0} = 1 \implies \lim_{y\to 1} e^y = e^1 = e
    \]
\end{proof}


\begin{definizione}{Numero di Nepero ($e$)}{}
    \[
        e := \lim_{x\to +\infty} \left(1 + \frac{1}{x}\right)^{x}
    \]
\end{definizione}
\begin{esercizio}{}{}
    \[
    \lim_{x\to -\infty} \left(1 + \frac{1}{x}\right)^{x}
    \]
\end{esercizio}
\begin{proof}
    Per vedere come tende la funzione a $-\infty$ possiamo provare usando il cambio di variabile con $y=-x$ per provare a ricondurci alla definizione del numero di Nepero
    \[
        \lim_{x\to -\infty} \left(1 + \frac{1}{x}\right)^{x} = \lim_{y\to +\infty} \left(1 + \frac{1}{-y}\right)^{-y}  
    \]
    Ora racciamo qualche riarrangiamento 
\[
        \lim_{y\to +\infty} \left(1 + \frac{1}{-y}\right)^{-y}  =  \lim_{y\to +\infty} \left(\frac{-y + 1}{-y}\right)^{-y} =   \lim_{y\to +\infty} \left(\frac{y - 1}{y}\right)^{-y} = \lim_{y\to +\infty} \left(\frac{y}{y-1}\right)^{y}
    \]
    Il denominatore $y-1$ è molto scomodo, quindi proviamo a sostituirlo con $z=y-1$
    \[
\lim_{y\to +\infty} \left(\frac{y}{y-1}\right)^{y} = \lim_{z\to +\infty} \left(\frac{z+1}{z}\right)^{z+1} = \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z+1}
    \]
    Per sistemare l'esponente basta usare la proprietà degli esponenti e l'algebra dei limiti per il prodotto
    \[
        \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z+1} = \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z} \cdot \left(1 + \frac{1}{z}\right) = \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z} \cdot \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)
    \]
    Il primo limite tende a $e$ per la definizione di numero di Nepero, mentre nel secondo limite il termine ($\frac{1}{z}$) tende a 0 e quindi complessivamente il limite tende a 1
    \[
    \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z} \cdot \lim_{z\to +\infty} \left(1 + \circled[red]{\frac{1}{z}}\right) = e \cdot 1 = e
    \]
    Quindi notiamo che il limite tende ad $e$ anche per $x\to-\infty$, pertanto possiamo modificare la definizione con
    \[
    e = \lim_{x\to \pm\infty} \left(1 + \frac{1}{x}\right)^{x}
    \]
\end{proof}
\newpage

\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} = (1+x)^{\frac{1}{x}}
    \]
\end{esercizio}

\begin{proof}
    Per risolvere questo limite dobbiamo usare un cambio di variabile con $y = \frac{1}{x}$ però dobbiamo stare attenti infatti per valori di $x\to 0^+ \implies y\to +\infty$ mentre $x\to 0^- \implies y\to -\infty$ quindi dobbiamo studiare in due casi separati. Indichiamo con $(i)$ per il caso $y\to+\infty$  e $(ii)$ per il caso $y\to-\infty$ 
    \[
        (i) \;\;\; \lim_{x\to 0^+} = (1+x)^{\frac{1}{x}} = \lim_{y\to +\infty} = \left(1+\frac{1}{y}\right)^{y} = e
    \] 
     \[
        (ii) \;\;\; \lim_{x\to 0^-} = (1+x)^{\frac{1}{x}} = \lim_{y\to -\infty} = \left(1+\frac{1}{y}\right)^{y} = e
    \]
    
    Vediamo che nonostante abbiamo dovuto dividere in due casistiche separate il limite tende allo stesso valore, e che quindi per il teorema della relazione tra limite e limite destro/sinistro sappiamo che
    \[
    \lim_{x\to 0} = (1+x)^{\frac{1}{x}} = e
    \]
\end{proof}

\addcontentsline{toc}{subsection}{Limiti Notevoli}
\begin{definizione}{Limiti Notevoli}{}
    Si definiscono Limiti Notevoli tutti i limiti della seguente forma. (Le dimostrazione le vediamo subito dopo)
    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $\displaystyle\lim_{x\to 0} \frac{\sin(x)}{x} = 1$ (Già dimostrato)
        \item $\displaystyle\lim_{x\to 0} \frac{1-\cos(x)}{x^2} = \frac{1}{2}$
        \item $\displaystyle\lim_{x\to 0} \frac{\tan(x)}{x} = 1$
        \item $\displaystyle\lim_{x\to +\infty} \left(1 + \frac{\alpha}{x}\right)^{x} = e^\alpha$  $\forall \alpha \in \mathbb{R}$
        \item $\displaystyle\lim_{x\to 0} \frac{\log(1+x)}{x} = 1$
        \item $\displaystyle\lim_{x\to 0} \frac{e^x-1}{x} = 1$ 
    \end{enumerate}
\end{definizione}
\newpage
\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{1-\cos(x)}{x^2} = \frac{1}{2}
    \]
\end{esercizio}
\begin{proof}
    Si vede subito che è una forma $\bigl[\frac{0}{0}\bigr]$ e però non sembra riconducibile a nessun limite tra quelli che abbiamo visto, però proviamo a a "trasformare" il coseno in seno , visto che del seno sappiamo un limite notevole $(i)$. Per farlo dobbiamo ricordarci la formula fondamentale della trigonometria: $\cos^2(x) + \sin^2(x) = 1$
    \begin{align*}
    \lim_{x\to 0} \frac{1-\cos(x)}{x^2} &= \lim_{x\to 0} \frac{1-\cos(x)}{x^2} \cdot \frac{1+\cos(x)}{1+\cos(x)} = \lim_{x\to 0} \frac{1-\cos^2(x)}{x^2} \cdot \frac{1}{1+\cos(x)} \\
    &= \lim_{x\to 0} \frac{\sin^2(x)}{x^2} \cdot \frac{1}{1+\cos(x)} = \lim_{x\to 0} \left(\frac{\sin(x)}{x}\right)^2 \cdot \frac{1}{1+\cos(x)}
    \end{align*}
    Ora il primo termine, visto che è il limite notevole $(i)$, tende a 1, mentre il secondo tende a $\frac{1}{2}$
    \[
    \lim_{x\to 0} \left(\frac{\sin(x)}{x}\right)^2 \cdot \frac{1}{1+\cos(x)} = (1)^2 \cdot \frac{1}{1+1} = \frac{1}{2}
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
    \frac{\tan(x)}{x} = 1
    \]
\end{esercizio}
\begin{proof}
    Questo è molto semplice infatti basta usare la definizione di tangente ($\tan(x) = \frac{\sin(x)}{\cos(x)}$)
    \[
    \lim_{x\to 0}\frac{\tan(x)}{x} = \lim_{x\to 0}\frac{\frac{\sin(x)}{\cos(x)}}{x} = \lim_{x\to 0}\frac{\sin(x)}{x} \cdot \frac{1}{\cos(x)} = 1\cdot \frac{1}{1} = 1
    \]
\end{proof}

\begin{esercizio}{}{}
 \[
 \lim_{x\to +\infty} \left(1 + \frac{\alpha}{x}\right)^{x}
 \]   
\end{esercizio}

\begin{proof}
    Questo chiaramente assomiglia molto alla definizione di $e$, soltanto che c'è un $\alpha$ di troppo. Possiamo provare a sostituire ma notiamo una cosa, infatti se vogliamo sostituire $\alpha y = x$ dobbiamo distinguere i casi $\alpha > 0$, $\alpha = 0$, $\alpha < 0$.  Studiamo i singoli casi e numeriamoli rispettivamente $(i)$,$(ii)$,$(iii)$
    \[
    (i) \;\;\; \lim_{x\to +\infty} \left(1 + \frac{\alpha}{x}\right)^{x} = \lim_{y\to +\infty} \left(1 + \frac{1}{y}\right)^{\alpha y} = \lim_{y\to +\infty} \left(\left(1 + \frac{1}{y}\right)^{y}\right)^\alpha = (e)^\alpha = e^\alpha
    \]

    \[
    (ii) \;\;\; \lim_{x\to +\infty} \left(1 + \frac{0}{x}\right)^{x} =  \lim_{x\to +\infty} 1 ^{x} = 1 \;\;\; [=e^0]
    \]

    \[
    (iii) \;\;\; \lim_{x\to +\infty} \left(1 + \frac{\alpha}{x}\right)^{x} = \lim_{y\to -\infty} \left(1 + \frac{1}{y}\right)^{\alpha y} = \lim_{y\to -\infty} \left(\left(1 + \frac{1}{y}\right)^{y}\right)^\alpha = (e)^\alpha = e^\alpha
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{\log(1+x)}{x} = 1
    \]
\end{esercizio}

\begin{proof}
    Per risolvere questo basta usare le proprietà dei logaritmi
    \[
\lim_{x\to 0} \frac{\log(1+x)}{x} = \lim_{x\to 0} \frac{1}{x} \cdot \log(1+x) = \lim_{x\to 0} \log\left((1+x)^{\frac{1}{x}}\right) = \log(e) = 1
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} \frac{e^x-1}{x} = 1
    \]
\end{esercizio}

\begin{proof}
    Questo invece è un pò più complicato, infatti non abbiamo visto limiti di questo. Però possiamo provare con una sostituzione $y = \log(x)$ e vediamo che succede, ricordandoci che se $x\to0$ allora $y\to -\infty$
    \[
        \lim_{x\to 0} \frac{e^x-1}{x} = \lim_{y\to -\infty} \frac{e^{\log(y)}-1}{\log(y)} = \lim_{y\to -\infty} \frac{y-1}{\log(y)}
    \]
    Ora assomiglia al limite dell'esercizio precedente, soltanto che all'interno dell'logaritmo abbiamo $y$ e non $y+1$, quindi per farlo "sbucare" fuori possiamo fare un'altra sostituzione $z=y+1$
    \[
    \lim_{y\to -\infty} \frac{y-1}{\log(y)} = \lim_{z\to -\infty} \frac{z}{\log(z+1)}
    \] 
    Adesso il limite è riconducibile a limite notevole $(v)$
    \[
    \lim_{z\to -\infty} \frac{z}{\log(z+1)} = \lim_{z\to -\infty} \frac{1}{\frac{\log(z+1)}{z}} = \frac{1}{1} = 1
    \]
\end{proof}

\newpage

\addcontentsline{toc}{subsection}{Definizione di Funzioni Asintotiche}
\begin{definizione}{Funzioni Asintotiche}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di accumulazione in $A$ e se

    \begin{itemize}
        \centering
        \item $f(x)\ne 0$, $g(x) \ne 0$ definitivamente per $x\to x_0$
        \item $\displaystyle \exists \lim_{x\to x_0} \frac{f(x)}{g(x)} = 1$
    \end{itemize}

    Allora diciamo che "$f(x)$ è asintotica per $x\to x_0$ a $g(x)$" e lo indichiamo con il simbolo
    \[
        f(x) \sim g(x) \;\;\;\;\; x\to x_0
    \]
\end{definizione}

\begin{esempio}{}{}
    \[
        \sin(x) \sim x \;\;\;\;\; x\to 0
    \]
\end{esempio}

\begin{proof}
    Dai limiti notevoli sappiamo che $\displaystyle\lim_{x\to 0} \frac{\sin(x)}{x}$, sappiamo inoltre che $\sin(x) \ne 0$ definitivamente per $x\to 0$ e lo stesso vale per $x \ne 0$. Pertanto possiamo dire che 
    \[
        \sin(x) \sim x \;\;\;\;\; x\to 0
    \] 

    \textbf{N.B.} questo ragionamento lo possiamo fare per tutti i limiti notevoli, quindi sarà vero anche $\log(1+x) \sim x$ per $x\to 0$, $\tan(x) \sim x$ per $x\to 0$, $e^x -1 \sim x$ per $x\to 0$ (che lo possiamo scrivere anche come $e^x \sim 1+ x$).  
\end{proof}

\begin{esempio}{}{}
    \[
        \cos(x) \sim 1-\frac{x^2}{2} \;\;\;\;\; x\to 0
    \]
\end{esempio}

\begin{proof}
    Per il limite notevole del coseno dobbiamo fare qualche ragionameto in più, infatti il limite fa come risultato $\frac{1}{2}$ e non 1, quindi non possiamo dire nulla sull'asintoticità, ma possiamo fare qualche mageggio, infatti
    \[
        \lim_{x\to 0} \frac{1-\cos(x)}{x^2} = \frac{1}{2} \implies \lim_{x\to 0} \frac{1-\cos(x)}{\frac{x^2}{2}} = 1
    \]
    Dopo questo riarrangiamento possiamo dire che 
    \[
    1 - \cos(x) \sim \frac{x^2}{2} \;\;\;\;\; x \to 0
    \]
    Che possiamo riscrivere come
    \[
    \cos(x) \sim 1-\frac{x^2}{2} \;\;\;\;\; x\to 0
    \]
\end{proof}
\newpage

\addcontentsline{toc}{subsection}{Teorema delle Proprità delle Funzioni Asintotiche}
\begin{teorema}{Proprietà delle Funzioni Asintotiche}{}
     Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g, h, \hat{f}, \hat{g}:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ 


     $(i)$ se $f \sim g$ allora è vera una delle due proposizioni
    \begin{itemize}
        \item $f$ e $g$ hanno entrambe limite in $x_0$
        \item $f$ e $g$ entrambe non hanno limite in $x_0$
    \end{itemize}
    $(ii)$ se $f \sim g$ e $h \sim g$ per $x\to x_0$ allora è vero che $f \sim h$ per $x\to x_0$
\vspace{0.2cm}

    $(iii)$ se $f \sim \hat{f}$ per $x\to x_0$, $g \sim \hat{g}$ per $x\to x_0$ allora sono vere entrambe le equivalenze:
\[
\begin{array}{c  @{\qquad\qquad}  c}
f\cdot g \sim \hat{f}\cdot \hat{g} 
&
\dfrac{f}{g} \sim \dfrac{\hat{f}}{\hat{g}}
\end{array}
\]
\end{teorema}

\begin{proof}
    Segue la dimostrazione del punto $(ii)$ e $(iii)$.

    $(ii)$ Noi sappiamo che $f \sim g$ e che $g \sim h$ ma vogliamo vedere se è vero  che $f \sim h$, e se fosse vera quest'ultima equivalenza allora dovrebbe essere vero che 
    \[
        f \sim h \iff \lim_{x\to x_0} \frac{f(x)}{h(x)} = 1
    \]
    Quindi proviamo a vedere se il limite fa 1, e per farlo dividiamo e moltiplichiamo per $g(x)$
    \[
    \lim_{x\to x_0} \frac{f(x)}{h(x)} = \lim_{x\to x_0} \frac{f(x)}{h(x)} \cdot \frac{g(x)}{g(x)}  =  \lim_{x\to x_0} \frac{f(x)}{g(x)} \cdot \frac{g(x)}{h(x)}=  \lim_{x\to x_0}  \frac{f(x)}{g(x)} \cdot \frac{1}{\frac{h(x)}{g(x)}} 
    \]
    Ma visto che per ipotesi $f \sim g$ e $g \sim h$, allora i rapporti varranno 1, e pertanto il teorema è verificato
    \[
        \lim_{x\to x_0}  \frac{f(x)}{g(x)} \cdot \frac{1}{\frac{h(x)}{g(x)}}  = 1 \cdot \frac{1}{1} = 1
    \]

    $(iii)$ Riscriviamo $f\cdot g \sim \hat{f}\cdot \hat{g}$ usando la definizione di funzione asintotica
    \[
        f(x)\cdot g(x) \sim \hat{f}(x)\cdot \hat{g}(x) \iff \lim_{x\to x_0} \frac{f(x)\cdot g(x)}{\hat{f}(x)\cdot \hat{g}(x)} = 1
    \]
    Quindi per dimostrare il teorema basta controllare che il limite faccia 1, ma è molto semplice infatti se spezziamo la frazione e grazie alle ipotesi ($f \sim \hat{f}$ e $g \sim \hat{g}$) allora 
    \[
    \lim_{x\to x_0} \frac{f(x)\cdot g(x)}{\hat{f}(x)\cdot \hat{g}(x)} = \lim_{x\to x_0} \frac{f(x)}{\hat{f}(x)} \cdot \frac{g(x)}{\hat{g}(x)}  = \lim_{x\to x_0} \frac{f(x)}{\hat{f}(x)} \cdot  \lim_{x\to x_0}  \frac{g(x)}{\hat{g}(x)}  = 1 \cdot 1 = 1
    \]
    Il ragionamento è analogo per $\dfrac{f}{g} \sim \dfrac{\hat{f}}{\hat{g}}$.
\end{proof}


\addcontentsline{toc}{subsection}{Gerarchia degli Infiniti}
\begin{teorema}{Gerarchia degli Infiniti}{}
    Siano $a > 1$, $\alpha > 0$, $\beta >0$ allora sono vere
    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $\displaystyle\lim_{x\to +\infty} \frac{x^\alpha}{a^x} = 0 $
        \item $\displaystyle\lim_{x\to +\infty} \frac{(\log_{a}x)^\beta}{x^\alpha} = 0 $
    \end{enumerate}

\end{teorema}

\begin{esempio}{}{}
    \[
        \lim_{x\to 0^+} x \log(x)
    \]
\end{esempio}

\begin{proof}
    Si può notare come sia un limite nella forma $\bigl[0\cdot\infty\bigr]$ e quindi dobbiamo fare degli riarrangiamenti. Partimo portando il termine $x$ a denominatore.
    \[
     \lim_{x\to 0^+} x \log(x) =  \lim_{x\to 0^+} \frac{\log(x)}{\frac{1}{x}}
    \]
    Ora possiamo fare una sostituzione con $y = \frac{1}{x}$, e quindi se $x\to 0^+$ allora $y\to +\infty$
    \[
\lim_{x\to 0^+} \frac{\log(x)}{\frac{1}{x}} = \lim_{y\to +\infty} \frac{\log(\frac{1}{y})}{y}
    \]
    Usando le proprietà dei logaritmi abbiamo 
    \[
    \lim_{y\to +\infty} \frac{\log(\frac{1}{y})}{y} = \lim_{y\to +\infty} \frac{\log(y^{-1})}{y} = \lim_{y\to +\infty} -\frac{\log(y)}{y}
    \]
    E che quindi con la gerarchia degli Infiniti
    \[
    \lim_{y\to +\infty} -\frac{\log(y)}{y} = 0
    \]
\end{proof}

\begin{esempio}{}{}
    \[
        \lim_{x\to 0^+} x^x
    \]
\end{esempio}

\begin{proof}
    Per svolgere questo limite possiamo usare la regola delle potenze di funzioni per riarrangiarla e possiamo usare il limite dell'esempio precedente per calcolarlo
    \[
        \lim_{x\to 0^+}x^x =\lim_{x\to 0^+} e^{\log(x^x)} = \lim_{x\to 0^+}e^{x\log(x)} = e^0 = 1
    \]
\end{proof}

\section{Simboli di Landau}

\addcontentsline{toc}{subsection}{Definizione di o-piccolo}
\begin{definizione}{o-piccolo}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$  e se 
    \[
        \lim_{x\to x_0} \frac{f(x)}{g(x)} = 0
    \]

    Allora diciamo che "$f(x)$ è un o-piccolo di $g(x)$ per $x\to x_0$" e lo indichiamo con il simbolo
    \[
        f(x) = o(g(x))\;\;\;\;\; x\to x_0
    \]
\end{definizione}
Ora seguiranno alcuni esempi per familiarizzare con il simbolo 

\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{x^3}{x} = 0 \iff x^3 = o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}
\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{\sin^2(x)}{x} = 0 \iff \sin^2(x) = o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}

\textbf{N.B.} anche se $\sin^2(x) = o(x)$ e $x^3 = o(x)$ per $x\to 0$ \textbf{NON} possiamo dire che $sin^2(x) = x^2$. Anche perche $sin^2(x)$ e $x^2$ sono due cose separate. Infatti con la nozione di o-piccolo sappiamo che una funzione è più grande di un'altra, ma se due funzioni sono entrambe o-piccolo di una funzione, non possiamo dire nulla delle due funzioni.
\begin{esempio}{}{}
    \[
        \lim_{x\to +\infty} \frac{x}{x^2} = 0 \iff x = o(x^2) \;\;\;\;\; x\to+\infty
    \]
\end{esempio}
\begin{esempio}{}{}
    \[
        \lim_{x\to 0} x = 0 \iff x = o(1) \;\;\;\;\; x\to 0
    \]
\end{esempio}

Questo perchè possiamo riscrivere $x$ come $\frac{x}{1}$ e per questo possiamo scrivere $x = o(1)$. Difatto in generale se $\displaystyle\lim_{x\to x_0}f(x) = 0$ allora possiamo dire che $f(x) = o(1)$ per $x\to x_0$.
\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{x^3}{x^2} = 0 \iff x^3 = o(x^2) \;\;\;\;\; x\to 0
    \]
\end{esempio}

\newpage

\addcontentsline{toc}{subsection}{Proprietà degli o-piccoli \textit{I}}
\begin{teorema}{Proprietà o-piccoli}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$  allora 

    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $\displaystyle\lim_{x\to x_0} \frac{o(f(x))}{f(x)} = 0$
        \item $o(f(x)) \pm o(f(x)) = o(f(x)) \;\;\;\;\; x\to x_0$
        \item $o(f(x)) \cdot o(g(x)) = o(f(x)\cdot g(x)) \;\;\;\;\; x\to x_0$
        \item $|o(f(x))|^\alpha = o(|f(x)|^\alpha) \;\;\;\;\; x\to x_0 \;\;\; \forall \alpha > 0$
    \end{enumerate}
    Se $g(x)\ne 0$  definitivamente per $x\to x_0$, allora 

    

    \begin{enumerate}[label=(\roman*)]
        \setcounter{enumi}{4}
        \centering
        \item $o(f(x)) \cdot g(x) = o(f(x) \cdot g(x)) \;\;\;\;\; x\to x_0$
    \end{enumerate}
    Se, oltre a $g(x)\ne 0$, è vero che $|g(x)|$ è limitata definitivamente allora vale

    

    \begin{enumerate}[label=(\roman*)]
        \setcounter{enumi}{5}
        \centering
        \item $o(f(x)) \cdot g(x) = o(f(x)) \;\;\;\;\; x\to x_0$
    \end{enumerate}

    
\end{teorema}

\begin{proof}
    ($i$) sia una qualsiasi funzione allora $g(x) = o(f(x))$, allora per definizione di o-piccolo sappiamo che 
    \[
        g(x) = o(f(x)) \iff \lim_{x\to x_0}    \frac{g(x)}{f(x)} = 0
    \]  
    Ora però possiamo sostituire $g(x)$ con $o(f(x))$ visto che è vera per ipotesi
    \[
    \lim_{x\to x_0}    \frac{g(x)}{f(x)} = 0 \implies \lim_{x\to x_0}    \frac{o(f(x))}{f(x)} = 0
    \]
    E con questo abbiamo verificato il primo teorema.

    $(ii)$ Sia $g_1(x) = o(f(x))$ e $g_2(x) = o(f(x))$ allora proviamo a vedere se è vero il teorema, quindi usiamo la definizione di o-piccolo
    \[
    g_1(x) \pm g_2(x) = o(f(x)) \iff \lim_{x\to x_0} \frac{g_1(x) \pm g_2(x)}{f(x)} = \lim_{x\to x_0} \frac{g_1(x)}{f(x)}\pm \frac{g_2(x)}{f(x)} 
    \]
    Ora visto che $g_1(x) = o(f(x))$ e $g_2(x) = o(f(x))$ allora se vengono divise per $f(x)$ tenderanno a 0 e quindi
    \[
    \lim_{x\to x_0} \circled[red]{\frac{g_1(x)}{f(x)}}\pm \circled[red]{\frac{g_2(x)}{f(x)}} = 0 \pm 0 = 0
    \]
    E quindi il teorema è verificato visto che è venuto proprio 0. 
    
    \textbf{N.B.} anche $o(f(x)) - o(f(x)) = o(f(x))$  e NON si eliminano gli o-piccoli.
\newpage

    ($iii$) Come nel punto $(ii)$, basta che controlliamo se è vero usando la definizione di o-piccolo
    \[
        o(f(x)) \cdot o(g(x)) = o(f(x)\cdot g(x)) \iff \lim_{x\to x_0} \frac{o(f(x)) \cdot o(g(x)) }{f(x)\cdot g(x)}
    \]
    Ora possiamo fare degli riarrangiamenti 
    \[
    \lim_{x\to x_0} \frac{o(f(x)) \cdot o(g(x)) }{f(x)\cdot g(x)} = \lim_{x\to x_0} \frac{o(f(x))}{f(x)} \cdot \frac{o(g(x)) }{g(x)}
    \]
    Ora usiamo la proprietà $(i)$ 
    \[
    \lim_{x\to x_0} \frac{o(f(x))}{f(x)} \cdot \frac{o(g(x)) }{g(x)} = 0\cdot 0 = 0
    \]
    E quindi il teorema è verificato.

    $(iv)$  Usiamo la definizione di o-piccolo
    \[
    |o(f(x))|^\alpha = o(|f(x)|^\alpha) \iff \lim_{x\to x_0} \frac{|o(f(x))|^\alpha}{|f(x)|^\alpha}
    \]
    Usiamo le proprietà dei moduli e delle potenze e la proprietà $(i)$
    \[
    \lim_{x\to x_0} \frac{|o(f(x))|^\alpha}{|f(x)|^\alpha} = \lim_{x\to x_0} \left|\frac{o(f(x))}{f(x)}\right|^\alpha = 0 ^ \alpha = 0
    \]
    L'ultimo passaggio è valido perchè abbiamo definito $\alpha > 0$ e quindi non reca alcun problema l'esponente. Il modulo serve solo per poter farlo anche di radici, così è valido anche per radici negative.


    $(v)$ Usiamo la definizione di o-piccolo
    \[
     o(f(x)) \cdot g(x) = o(f(x)\cdot g(x)) \iff \lim_{x\to x_0} \frac{o(f(x)) \cdot g(x) }{f(x)\cdot g(x)}
    \]
    Notiamo che il termine $g(x)$ si può semplificare e poi possiamo usare la proprietà $(i)$
    \[
    \lim_{x\to x_0} \frac{o(f(x)) \cdot g(x) }{f(x)\cdot g(x)} = \lim_{x\to x_0} \frac{o(f(x))}{f(x)} = 0
    \]

    $(vi)$ Usiamo la definizione di o-piccolo
    \[
     o(f(x)) \cdot g(x) = o(f(x)) \iff \lim_{x\to x_0} \frac{o(f(x)) \cdot g(x) }{f(x)} = \lim_{x\to x_0} \frac{o(f(x)) }{f(x)} \cdot g(x) 
    \]
    Ora notiamo che il termine $\frac{o(f(x)) }{f(x)}$ tende a 0 per proprietà $(i)$, quindi l'unico caso a cui dobbiamo stare attenti è quando $h(x)$ tende a $\infty$, perchè qualora fosse avremmo una forma indeterminata $\big[0\cdot \infty\big]$, però per ipotesi noi sappiamo che $|g(x)|$ è limitata, allora possiamo usare la proprietà $(ii)$ dell'algebra dei limiti finiti per scoprire che tende a 0, grazie al fatto che $|g(x)|$ è limitata
    \[
    \lim_{x\to x_0} \frac{o(f(x)) }{f(x)} \cdot g(x)  = 0
    \]
    Pertanto il teorema è verificato.
\end{proof}

\addcontentsline{toc}{subsection}{Relazione tra o-piccolo e Asintoticità}
\begin{teorema}{Relazione tra o-piccolo e Asintoticità}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ e $g(x)\ne 0$ definitivamente per $x\to x_0$

    \[
        f(x) \sim g(x) \;\;\; x\to x_0 \implies f(x) = g(x) + o(g(x))  \;\;\; x\to x_0
    \]
\end{teorema}
\begin{proof}
    Usando la definizione di funzione asintotica sappiamo che 
    \[
    f(x) \sim g(x) \iff  \lim_{x\to x_0} \frac{f(x)}{g(x)} = 1
    \]
    ora possiamo portare 1 dall'altra parte e facciamo denominatore comune 
    \[
    \lim_{x\to x_0} \frac{f(x)}{g(x)} = 1 \implies \lim_{x\to x_0} \big[\frac{f(x)}{g(x)} - 1\big]= 0 \implies \lim_{x\to x_0} \frac{f(x)-g(x)}{g(x)} = 0 
    \]
    A questo punto possiamo usare la definizione di o-piccolo, visto che abbiamo un limite che tende a 0
    \[
        \lim_{x\to x_0} \frac{f(x)-g(x)}{g( x)}= 0 \iff f(x)-g(x) = o(g(x))
    \]
    Facendo qualche riarrangiamento abbiamo 
    \[
    f(x)-g(x) = o(g(x)) \implies f(x) = g(x) + o(g(x)) \;\;\; x\to x_0
    \]
\end{proof}

Ora vediamo l'applicazione di questo teorema sui limiti notevoli
\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{\sin(x)}{x} = 1 \implies \sin(x) = x + o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}

Per il limite del coseno usiamo lo stesso trick che abbiamo usato l'ulima volta, ovvero di dividere per $\frac{1}{2}$ in modo che ora il limite tenda a 1

\begin{esempio}{}{}
    \[
        \lim_{x\to x_0} \frac{1-\cos(x)}{\frac{1}{2}x^2} = 1 \implies 1-\cos(x) = \frac{1}{2}x^2 + o(x^2) \implies \cos(x) = 1 -\frac{1}{2}x^2 + o(x^2) 
    \]
\end{esempio}

\text{N.B.} non ho scritto $o(\frac{1}{2}x^2)$ perchè ho usato la proprietà $(vi)$ delle proprietà degli o-piccolo, e lo abbiamo potuto applicare perchè la funzione $g(x)= \frac{1}{2}$ è sempre limitata.

\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{\tan(x)}{x} = 1 \implies \tan(x) = x + o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}


\begin{esempio}{}{}
    \[
        \lim_{x\to x_0} \frac{\log(1+x)}{x} = 1 \implies \log(1+x) = x + o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}

\begin{esempio}{}{}
    \[
        \lim_{x\to x_0} \frac{e^x-1}{x} = 1 \implies e^x -1  = x + o(x) \implies e^x  = 1 + x + o(x)  \;\;\;\;\; x\to 0
    \]
\end{esempio}


\addcontentsline{toc}{subsection}{Teorema del Cambio di variabile con o-piccolo}
\begin{teorema}{Cambio di variabile con o-piccolo}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g_1, g_2$ a valori reali tali che $g_1 \circ f,g_2 \circ f :A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ e supponiamo che
    \begin{itemize}
        \item $\displaystyle\lim_{x\to x_0} f(x) = y_0$
        \item $g_1(y) = g_2(y) + o(g_2(y))\;\;\;\;\; y\to y_0$
        \item $f(x) \ne y_0$ definitivamente per $x\to x_0$
    \end{itemize}

    Allora 
    \[
        g_1(f(x)) = g_2(f(x)) + o(g_2(f(x)))\;\;\;\;\; y\to y_0
    \]
\end{teorema}


\begin{esempio}{}{}
    Proviamo a fare qualche applicazione pratica di questo teorema, per esempio proviamo con $f(x) = x^2$, $g_1(y) = \sin(y)$ e $x_0=0$
\end{esempio}

In primis dobbiamo controllare che esista il limite di $f(x)$
\[
\lim_{x\to 0} x^2 = 0 \;\;\; [= y_0]
\]
Ora proviamo a sviluppare $g_1(y)$, e possiamo usare proprio lo sviluppo che abbiamo scoperto prima ($\sin(y) = y + o(y)$) per $y\to 0$, seguendo la notazione del teorema, $g_2(y) = y$. Ora basta controllare se $x^2 \ne 0$ definitivamente ed effettivamente lo è visto che $x^2>0$ $\forall x \ne 0$ e quindi possiamo applicare il teorema e scopriamo che 
\[
\sin(x^2) = x^2 + o(x^2) \;\;\;\;\; x\to 0
\]

\begin{esempio}{}{}
    Ora proviamo a fare un altro esempio con $f(x) = \sin(x)$, $g_1(y) = e^y$ e $x_0=0$, vediamo subito che 
    \[
    \lim_{x\to 0} \sin(x) = 0 \;\;\; [= y_0]
    \]
    Sappiamo in oltre lo svilutto di $e^y = 1+ y +o(y)$ per $y\to 0$, in oltre sappiamo che $\sin(x) \ne 0$ in un intorno di 0 e di conseguenza possiamo applicare il teorema e scopriamo che 
    \[
        e^{\sin(x)} = 1 + \sin(x) + o(\sin(x)) \;\;\;\;\; x\to 0
    \]
\end{esempio}


\addcontentsline{toc}{subsection}{Principio di Sostituzione}
\begin{teorema}{Principio di Sostituzione}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g, \hat{f}, \hat{g}:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ e $g(x)\ne 0$,$\hat{f}(x)\ne 0$,$\hat{g}(x)\ne 0$  definitivamente per $x\to x_0$ e se 
    \[
        f(x) = \hat{f}(x) + o(\hat{f}(x)) \;\;\;\;\; x\to x_0
    \]
     \[
        g(x) = \hat{g}(x) + o(\hat{g}(x)) \;\;\;\;\; x\to x_0
    \]
    Allora
    \[
        \lim_{x\to x_0} \frac{f(x)}{g(x)} = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)}  
    \]
\end{teorema}

\begin{proof}
    Per capire quanto vale $\displaystyle\lim_{x\to x_0} \frac{f(x)}{g(x)} $ basta che facciamo una sostituzione con le ipotesi
    \[
    \lim_{x\to x_0} \frac{f(x)}{g(x)}  = \lim_{x\to x_0} \frac{\hat{f}(x) + o(\hat{f}(x))}{\hat{g}(x) + o(\hat{g}(x)) } 
    \]
    Ora possiamo raccogliere a numeratore $\hat{f}(x)$ e a denominatore $\hat{g}(x)$
    \[
    \lim_{x\to x_0} \frac{\hat{f}(x) + o(\hat{f}(x))}{\hat{g}(x) + o(\hat{g}(x)) }  = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)} \cdot \frac{1 + \frac{o(\hat{f}(x))}{\hat{f}(x)}}{ 1+ \frac{o(\hat{g}(x))}{\hat{g}(x)} } 
    \]
    Ora possiamo dividere per l'algebra dei limiti e possiamo usare la proprietà $(i)$ degli o-piccoli
    \[
    \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)} \cdot \frac{1 + \frac{o(\hat{f}(x))}{\hat{f}(x)}}{ 1+ \frac{o(\hat{g}(x))}{\hat{g}(x)} }  = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)} \cdot \lim_{x\to x_0}\frac{1 + \circled[red]{\frac{o(\hat{f}(x))}{\hat{f}(x)}}}{ 1+ \circled[red]{\frac{o(\hat{g}(x))}{\hat{g}(x)}} } = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)} \cdot \frac{1 + 0}{ 1+0 } = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)}   
    \]
\end{proof}

Questo teorema è molto forte infatti vediamo un esercizio dove lo applichiamo

\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{1-\cos(x)}{e^{x^2}-1}
    \]
\end{esercizio}

Per applicare il teorema dobbiamo trovare quelle equivalenze con gli o-piccoli, e infatti le conosciamo sia per $1-\cos(x) = \frac{1}{2}x^2 + o(x^2)$ che per $e^{x^2}-1 = x^2 +o(x^2)$, e di conseguenza possiamo calcolare il limite come
\[
\lim_{x\to 0} \frac{1-\cos(x)}{e^{x^2}-1} = \lim_{x\to 0} \frac{\frac{1}{2}x^2}{x^2} = \frac{1}{2}
\]
Questo esercizio si sarebbe potuto svolgere anche con i limiti notevoli ma avrebbe richiesto molti calcoli in più.


\addcontentsline{toc}{subsection}{Proprietà degli o-piccoli \textit{II}}
\begin{teorema}{Ulteriori proprietà degli o-piccoli}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$  allora 

    \begin{enumerate}[label=(\roman*)]
        \setcounter{enumi}{6}
        \centering
        \item $o(o(f(x))) = o(f(x))$ per $x\to x_0$
        \item $o(f(x) + o(f(x))) = o(f(x))$ per $x\to x_0$
    \end{enumerate}
\end{teorema}


\begin{proof}
    $(vii)$ Usiamo la definizione di o-piccolo 
    \[
        o(o(f(x))) = o(f(x)) \iff \lim_{x\to x_0} \frac{o(o(f(x)))}{f(x)}
    \]
    Per calcolare e controllare che il limite faccia 0, dobbiamo moltiplicare e dividere per $o(f(x))$
    \[
    \lim_{x\to x_0} \frac{o(o(f(x)))}{f(x)} = \lim_{x\to x_0} \frac{o(o(f(x)))}{f(x)} \cdot \frac{o(f(x))}{o(f(x))} = \lim_{x\to x_0} \frac{o(o(f(x)))}{o(f(x))} \cdot \frac{o(f(x))}{f(x)}
    \]
    Il secondo termine ($\frac{o(f(x))}{f(x)}$) per la proprietà $(i)$ degli o-piccoli sappiamo che tende a 0, e lo stesso vale per il primo termine, infatti la regola generale per questi casi è $\frac{o(f(x))}{f(x)} \to 0$ però se scelgo $f(x)=o(g(x))$, allora il limite diventa $\frac{o(o(g(x)))}{o(g(x))} \to 0$, pertanto
    \[
        \lim_{x\to x_0} \frac{o(o(f(x)))}{o(f(x))} \cdot \frac{o(f(x))}{f(x)} = 0\cdot 0 = 0
    \]
    E visto che il limite viene 0, la proprietà è verificata.

    $(viii)$ Usiamo la definizione di o-piccolo 
    \[
    o(f(x) + o(f(x))) = o(f(x)) \iff \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x)}
    \]
    Possiamo moltiplicare e dividere per $f(x) + o(f(x))$
    \begin{align*}
        \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x)} &= \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x)} \cdot \frac{f(x) + o(f(x))}{f(x) + o(f(x))} \\
        &=  \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x) + o(f(x))} \cdot \frac{f(x) + o(f(x))}{f(x)} \\
        &= \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x) + o(f(x))} \cdot \left(1 +\frac{o(f(x))}{f(x)}     \right)
    \end{align*}
    Possiamo usare lo stesso ragionamento usato per il punto $(vii)$ dicendo che $\frac{o(f(x) + o(f(x)))}{f(x) + o(f(x))}\to 0$ 
    \[
    \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x) + o(f(x))} \cdot \left(1 +\frac{o(f(x))}{f(x)}     \right) = 0 \cdot ( 1+ 0) = 0
    \]
    Di conseguenza il teorema è verificato.
\end{proof}


\addcontentsline{toc}{subsection}{Esercizi con o-piccolo}
\begin{esercizio}{}{}
    \[
    \lim_{x\to 0^+} \frac{e^{\sin(x)} - e^{-x}}{1-\cos(\sqrt{x})}
    \]
\end{esercizio}

\begin{proof}
    Per risolvere questo esercizio dobbiamo usare gli sviluppi degli o-piccoli, in questo esercizio ci basta quello di $e^x$ e di $1-\cos(x)$ e quindi sappiamo che 
    \[
        e^{-x} = 1 + -x + o(x)
    \]
    \[
        1-\cos(\sqrt{x}) = \frac{1}{2} (\sqrt{x})^2 + o((\sqrt{x})^2 )
    \]
    Semplificando lo sviluppo del coseno abbiamo che $1-\cos(\sqrt{x}) = \frac{1}{2}x + o(x )$, non serve mettere il modulo perchè $x\to 0^+$ e quindi $x$ è positivo. Ora però dobbiamo capire quanto vale $e^{\sin(x)}$ e intanto usiamo lo sviluppo di $e^x$
    \[
        e^{\sin(x)} = 1 + \mathunderline{red}{\sin(x)} + o(\mathunderline{blue}{\sin(x)})
    \]
    Però il seno lo possiamo sviluppare a sua volta come $\sin(x) = x+ o(x)$ e di conseguenza
    \[
    e^{\sin(x)} = 1 +  \mathunderline{red}{x + o(x)} + o(\mathunderline{blue}{x+ o(x)})
    \]
    E quindi grazie alla proprietà $(viii)$ sappiamo che possiamo riscrivere $(o(x+ o(x)))$ come $o(x)$
    \[
    e^{\sin(x)} = 1 +  x + o(x) + o(x) 
    \]
    Poi per la proprietà $(ii)$ sappiamo che 
    \[
    e^{\sin(x)} = 1 +  x + o(x)
    \]
    Ora possiamo sostituire gli sviluppi nell'esercizio 
    \[
    \lim_{x\to 0^+} \frac{\mathunderline{red}{e^{\sin(x)}}- \mathunderline{blue}{e^{-x}}}{\mathunderline{green}{1-\cos(\sqrt{x})}} = \frac{\mathunderline{red}{1 +  x + o(x)} - \mathunderline{blue}{(1 -  x + o(x))}}{\mathunderline{green}{\frac{1}{2}x + o(x )}} = \lim_{x\to 0^+} \frac{2x + o(x)}{\frac{1}{2}x + o(x)}
    \]
    Per il prinicipio di sostituzione sappiamo che 
    \[
    \lim_{x\to 0^+} \frac{2x + o(x)}{\frac{1}{2}x + o(x)} = \lim_{x\to 0^+} \frac{2x }{\frac{1}{2}x} = \lim_{x\to 0^+} \frac{2}{\frac{1}{2}} = 4
    \]
    Se il numeratore fosse stato $e^{\sin(x)} - e^{x}$, allora dopo lo svilutto il numeratore sarebbe diventato $o(x)$ e quindi la frazione sarebbe stata $\frac{o(x)}{\frac{1}{2}x + o(x)}$ e quindi dividento tutto per $x$ avremmo avuto 
    \[
     \lim_{x\to 0^+}\frac{\frac{o(x)}{x}}{\frac{1}{2} + \frac{o(x)}{x}}
    \]
    E che quindi per la proprietà $(i)$ avremmo avuto 
    \[
     \lim_{x\to 0^+}\frac{\circled[red]{\frac{o(x)}{x}}}{\frac{1}{2} + \circled[red]{\frac{o(x)}{x}}} = \frac{0}{\frac{1}{2} + 0} = 0
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} \frac{e^{\tan(\sin(x))} - 1}{e^{\sin(\tan(x))} - 1}
    \]
\end{esercizio}

\begin{proof}
    Iniziamo analizzando il numeratore, vediamo che possiamo fare lo sviluppo di $e^x-1$
    \[
    e^{\tan(\sin(x))} -1  = \mathunderline{red}{\tan(\sin(x))} + o(\mathunderline{blue}{\tan(\sin(x))})
    \]
    Adesso facciamo lo sviluppo di $\tan(\sin(x)) = \sin(x) + o(\sin(x))$
    \[
    e^{\tan(\sin(x))} -1  = \mathunderline{red}{\sin(x)+ o(\sin(x))} + o(\mathunderline{blue}{\sin(x)+ o(\sin(x))})
    \]
    Possiamo usare la proprietà $(viii)$ e $(ii)$
    \[
    e^{\tan(\sin(x))} -1  = \sin(x)+ o(\sin(x)) +  o(\sin(x)) = \mathunderline{green}{\sin(x)}+ o(\mathunderline{purple}{\sin(x)})
    \] 
    Sviluppiamo anche il seno
    \[
    e^{\tan(\sin(x))} -1  = \mathunderline{green}{x + o(x)} + o(\mathunderline{purple}{x+o(x)})
    \]
    Ripetendo le proprietà $(viii)$ e $(ii)$
    \[
    e^{\tan(\sin(x))} -1  = x + o(x) + o(x) = x + o(x)
    \]
    Sistemato il numeratore, dobbiamo fare gli stessi passaggi al denominatore, ma invertendo il passaggio dello sviluppo del seno con quello della tangente
    \begin{align*}
        e^{\sin(\tan(x))} - 1 &= \mathunderline{red}{\sin(\tan(x))} + o(\mathunderline{blue}{\sin(\tan(x))}) \\
        &= \mathunderline{red}{\tan(x)+ o(\tan(x))} + o(\mathunderline{blue}{\tan(x)+ o(\tan(x))}) \\
        &= \tan(x)+ o(\tan(x)) +  o(\tan(x))  \\
        &= \mathunderline{green}{\tan(x)}+ o(\mathunderline{purple}{\tan(x)}) \\
        &= \mathunderline{green}{x + o(x)} + o(\mathunderline{purple}{x+o(x)})\\ 
        &= x + o(x) + o(x) \\
        &= x + o(x)
    \end{align*}

    E quindi sostituendo questi sviluppi nel limite abbiamo che 
    \[
    \lim_{x\to 0} \frac{e^{\tan(\sin(x))} - 1}{e^{\sin(\tan(x))} - 1} = \lim_{x\to 0} \frac{x + o(x)}{x + o(x)} = \lim_{x\to 0} \frac{x }{x } = 1
    \]
\end{proof}

\newpage
\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)\cos(3x^2)}
    \]
\end{esercizio}

\begin{proof}
    In primis notiamo che il termine $\cos(3x^2) \to 1$ per $x\to 0$, pertanto possiamo staccarlo dal limite e calcolarlo a parte 
    \[
    \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)\cos(3x^2)} = \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)} \cdot \frac{1}{\cos(3x^2)} = \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)} \cdot 1
    \] 
    Dopo questo, vediamo che abbiamo un $\log(1+x)$ e $\tan(x)$ che possiamo sviluppare
    \[
    \log(1 + x\sin^2(2x)) = x\sin^2(2x) + o(x\sin^2(2x)) 
    \]
    Facciamo lo stesso per $\sin(2x) = 2x+ o(x)$
    \[
    \log(1 + x\sin^2(2x)) = x \cdot (2x+ o(x))^2 + o(x\cdot (2x+ o(x))^2) 
    \] 
    ora capiamo il come calcolare il termine $(2x+ o(x))^2$ infatti proviamo a sviluppare il quadrato di binomio e troviamo che $4x^2 + 4x \cdot o(x) + (o(x))^2$, per la proprietà $(iv)$ possiamo riscrivere $(o(x))^2$ come $(o^2)$, mentre il termine $4x\cdot o(x)$ possiamo usare la proprietà $(iii)$ per portare $4x$ dentro l'o-piccolo. Quindi il termine diventa $4x^2 + o(4x^2)$. Di conseguenza il logaritmo diventa, usando le proprietà $(iii)$ e $(viii)$
    \begin{align*}
        \log(1 + x\sin^2(2x)) &= x \cdot (4x^2+ o(x^2)) + o(x\cdot (4x^2+ o(x^2))) \\
        &= 4x^3+ o(4x^3) + o( 4x^3+ o(4x^3)) \\ 
        &= 4x^3+ o(4x^3) + o(4x^3) \\
        &= 4x^3+ o(4x^3) 
    \end{align*}
    
    Per la tangente facciamo gli stessi procedimenti
    \begin{align*}
        \tan^3(x) &= (x+o(x))^3 \\
        &= x^3 + 3x^2 \cdot o(x) + 3x \cdot (o(x))^2 + (o(x))^3 \\
        &= x^3 +  o(3x^3) + 3x \cdot o(x^2) + o(x^3) \\
        &= x^3 +  o(x^3) +  o(3x^3) + o(x^3) \\
        &= x^3 +  o(x^3) + o(x^3) + o(x^3) \\
        &= x^3 +  o(x^3) \\
    \end{align*}

    E quindi sostituiendo gli sviluppi abbiamo che 
    \[
    \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)} = \lim_{x\to 0} \frac{4x^3+ o(4x^3) }{4(x^3 +  o(x^3))} = \lim_{x\to 0} \frac{4x^3+ o(4x^3) }{4x^3 +  o(4x^3)} = \lim_{x\to 0} \frac{4x^3}{4x^3 }  = 1
    \]
\end{proof}


Con l'esercizio precedente abbiamo scoperto che 

\addcontentsline{toc}{subsection}{Binomio con o-piccolo}
\begin{teorema}{Binomio con o-piccolo}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$  allora
    \[
        (f(x) + o(f(x)))^n = f^n(x) + o(f^n(x)) \;\;\;\;\; \forall n \in \mathbb{N}
    \] 
\end{teorema}
\begin{proof}
    Per dimostrare questo teorema dobbiamo usare il binomiale di Newton e quindi 
    \[
    (f(x) + o(f(x)))^n = \sum_{k=0}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot [o(f(x))]^{k} 
    \]
    Estraiamo il primo termine con $k=0$ visto che in quel caso $o(f(x))$ avrebbe esponente 0 e quindi non ci sarebbe
    \[
    \sum_{k=0}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot [o(f(x))]^{n} = f^{n}(x) +  \sum_{k=1}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot [o(f(x))]^{k}
    \]
    Ora possiamo usare le proprietà $(iv)$ per poter portare dentro l'esponente nel o-piccolo, poi la proprietà ($iii$) per portare il termine $[f(x)]^{n-k}$ dentro al o-piccolo
    \begin{align*}
        \sum_{k=1}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot [o(f(x))]^{k} &= \sum_{k=1}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot o(f^{n}(k)) \\
        &= \sum_{k=1}^{n} \binom{n}{k}  \cdot o([f(x)]^{n-k} \cdot f^{k}(x)) \\
        &= \sum_{k=1}^{n} \binom{n}{k}  \cdot o([f(x)]^{n-k+ k}) \\
        &= \sum_{k=1}^{n} \binom{n}{k}  \cdot o(f^n(x)) 
    \end{align*}
    Poi visto che $\binom{n}{k}$ è una costante la possiamo togliere per la $(vi)$, e poi dato che dentro la sommatoria non ci sono più termini rispetto a $k$ possiamo calcolarla ($\sum_{k=1}^{n} c = n\cdot c$)
    \[
        \sum_{k=1}^{n} \binom{n}{k}  \cdot o(f^n(x))  = \sum_{k=1}^{n} o(f^n(x)) = n\cdot o(f^n(x))
   \]
   Ora possiamo usare di nuovo la proprietà $(vi)$ per togliere $n$ e così il teorema è verificato
   \[
   n\cdot o(f^n(x)) = o(f^n(x))
   \]
   \[
   (f(x) + o(f(x)))^n = f^n(x) + o(f^n(x))
   \]

\end{proof}

\begin{esercizio}{}{}
    Sia $f:\mathbb{R} \to\mathbb{R} $  tale che $f(x) = o(x^2)$ per $x\to +\infty$, discutere il limite 
    \[
        \lim_{x\to +\infty} \frac{f(x)}{x^\alpha + x + 1}
    \]
    Al variare di $\alpha \in \mathbb{R}$
\end{esercizio}
\begin{proof}
    Visto che sappiamo che $f(x) = o(x^2)$, l'unica unformazione che sappiamo è che 
    \[
        \lim_{x\to +\infty} \frac{f(x)}{x^2} = 0
    \]
    Pertanto divisiamo numeratore e denominatore per $x^2$
    \[
    \lim_{x\to +\infty} \frac{f(x)}{x^\alpha + x + 1} = \lim_{x\to +\infty} \frac{\frac{f(x)}{x^2}}{x^{\alpha-2} +\frac{1}{x} + \frac{1}{x^2}}
    \]
    Ora il numeratore tende a 0, i termini $\frac{1}{x}$ e $\frac{1}{x^2}$ tendono a 0 per $x\to +\infty$, quindi ci manca da capire il termine $x^ {\alpha-2}$. Per questo dobbiamo dividere in 3 casi. 

    Se $\alpha > 2$ allora $\alpha -2 > 0$ e quindi $x^{\alpha -2} \to +\infty$ e quindi il denominatore complessivamente tende a $+\infty$ e pertanto di conseguenza il limite tende a 0
    \[
    \lim_{x\to +\infty} \frac{\frac{f(x)}{x^2}}{x^{\alpha-2} +\frac{1}{x} + \frac{1}{x^2}} = \frac{0}{+\infty +0 + 0} = \bigl[\frac{0}{+\infty}\bigr] = 0
    \]

    \textbf{N.B.} scrivere $\bigl[\frac{0}{+\infty}\bigr]$ è tecnicamente sbagliato perchè $\infty$ non è un numero ma un simbolo,  e pertanto non si possono fare le operazioni con quel numero. L'ho scritto soltato per enfatizzare il concetto di limite ma non è tecnicamente corretto scriverlo. Volendo essere più precisi, avremmo dovuto usare l'algebra dei limiti infiniti.

Se $\alpha = 2$ allora il termine $x^{\alpha -2} = x^{2 -2} = 1$ e quindi il limite diventare
    \[
    \lim_{x\to +\infty} \frac{\frac{f(x)}{x^2}}{x^{\alpha-2} +\frac{1}{x} + \frac{1}{x^2}}  = \frac{0}{1+0+0} = 0
    \]

    Se $\alpha < 2$ allora possiamo il termine $x^{\alpha -2} \to 0$ dato che l'esponente sarebbe negativo e quindi andrebbe a denominatore. E quindi il limite diventerebbe
    \[
    \lim_{x\to +\infty} \frac{\frac{f(x)}{x^2}}{x^{\alpha-2} +\frac{1}{x} + \frac{1}{x^2}}  = \frac{0}{0+0+0} = \bigl[\frac{0}{0}\bigr]
    \]
    In questo caso siamo incappati in una forma indeterminata, e che quindi con i dati che abbiamo a nostra disposizione non possiamo dire nulla in merito al limite.
    
\begin{center}
    $\displaystyle\lim_{x\to +\infty} \frac{f(x)}{x^\alpha + x + 1}=\begin{cases}
        0 & \text{se } \alpha \ge 2 \\
        \text{Non Definibile} & \text{se } \alpha < 2
    \end{cases}$
\end{center}
\end{proof}


\addcontentsline{toc}{subsection}{Definizione di O-grande}
\begin{definizione}{O-grande}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ . Se esiste $I \subseteq \mathbb{R}$ intorno di $x_0$  $\exists M >0$ tale che 
    \[
        |f(x)| \leq M|g(x)| \;\;\;\;\; \forall x \in A \cap I \setminus \{x_0\}
    \]
    Allora diciamo che "$f(x)$ è un O-grande di $g(x)$ per $x\to x_0$" e lo indichiamo con il simbolo
    \[
        f(x) = O(g(x))\;\;\;\;\; x\to x_0
    \]
\end{definizione}

\begin{esempio}{}{}
    Sia $f(x) = x\sin(\frac{1}{x})$ e possiamo vedere che 
    \[
    \left|x\sin\left(\frac{1}{x}\right)\right| \leq |x| \;\;\;\;\; \forall x \in \mathbb{R}
    \]
    Visto che $|\sin(x)| \leq 1$. Di conseguenza possiamo prendere un qualsiasi punto $x_0 \in \mathbb{R}$ e sapremo che 
    \[
    x\sin\left(\frac{1}{x}\right) = O(x) \;\;\;\;\; x\to x_0
    \]
\end{esempio}

\begin{esercizio}{}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$. Allora 
    \[
        f(x) = o(g(x)) \;\;\; x\to x_0 \implies f(x) = O(g(x)) \;\;\; x\to x_0
    \]
\end{esercizio}

\begin{proof}
    Per ipotesi sappiamo che $f(x) = o(g(x))$, che usando la definizione di o-piccolo sappiamo che 
    \[
    f(x) = o(g(x)) \iff \lim_{x\to x_0} \frac{f(x)}{g(x)} = 0
    \]
    Pertanto usando la definizione di limite sappiamo che 
    \[
        \left|\frac{f(x)}{g(x)}\right| < \varepsilon \;\;\; \forall x \in I
    \]
    Dove $I\subseteq \mathbb{R}$ è un intorno di $x_0$. Ora usiamo le proprietà dei modili
    \[
    \left|\frac{f(x)}{g(x)}\right| < \varepsilon  \implies \frac{|f(x)|}{|g(x)|} < \varepsilon \implies |f(x)| < \varepsilon |g(x)|
    \]
    Però la definizione O-grande richiedeva che ci fosse almeno un $M>0$ tale che $ |f(x)| < M|g(x)|$, però con la definizione di limite abbiamo trovato che vale $\forall \varepsilon >0$ e di conseguenza basta scegliere $M = \varepsilon$ e implicazione è verificata.

   
\end{proof}

 \newpage
    \textbf{ATTENZIONE} Non è vero il contrario, infatti lo vediamo con un esempio, infatti se  prendiamo $f(x)=x+1$ e $g(x)=x+2$ è facile vedere che 
    \[
    |x+1| \leq |x+2| \;\;\;\;\; \forall x \geq -1
    \]
    Pertanto possiamo scegliere un qualsiasi intorno di 0 della forma $I = (-\delta, +\delta)$ con $\delta\leq 1$, e di conseguenza è verificata la definizione di O-grande e quindi
    \[
        x+1 = O(x+2)\;\;\;\;\; \forall x \in I
    \]
    Però se proviamo a vedere se $f(x)=o(g(x))$ vediamo subito che non lo è dato che
    \[
        \lim_{x\to 0}\frac{f(x)}{g(x)} = \lim_{x\to 0}\frac{x+1}{x+2} = \frac{1}{2} \ne 0 
    \]
    Questo succede perchè O-grande ci dice che una funzione è più piccola di un'altra, mentre o-piccolo ci dice che una funzione è tanto più piccola di un'altra, a tal punto da rendere il limite uguale a 0.


\addcontentsline{toc}{subsection}{Confronti di Infiniti e Infinitesimi}
\begin{definizione}{Confronti di Infiniti}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ e se 
    \[
    \begin{array}{c @{\hspace{2cm}} c}
    \displaystyle\exists \lim_{x\to x_0}f(x) \in \{\pm \infty\} &  \displaystyle\exists \lim_{x\to x_0}g(x) \in \{\pm \infty\} 
    \end{array}
    \]
    Allora dichiamo che 
    \begin{itemize}
        \item "$f(x)$ è infinito dello stesso ordine di $g(x)$" se
        \[
        \exists \lim_{x\to x_0}\frac{f(x)}{g(x)} \in \mathbb{R} \setminus \{0\}
        \] 
        \item "$f(x)$ è infinito ordine di ordine inferiore $g(x)$"  se
        \[
        \exists \lim_{x\to x_0}\frac{f(x)}{g(x)} = 0
        \] 
         \item "$f(x)$ è infinito ordine di ordine superiore $g(x)$" se
        \[
        \exists \lim_{x\to x_0}\frac{g(x)}{f(x)} = 0
        \] 
        \item "$f(x)$ è infinito di ordine non confrontabile a $g(x)$" se
        \[
        \nexists \lim_{x\to x_0}\frac{f(x)}{g(x)} 
        \] 
    \end{itemize}
\end{definizione}

\begin{definizione}{Confronti di Infinitesimi}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 $ punto di acc. in $A$ e se

    \begin{itemize}
        \item $x_0 \in \mathbb{R}$ allora diciamo che 
        
        \begin{itemize}
            \item "$f(x)$ è infinitesima di ordine $\alpha \in \mathbb{R}$ per $x\to x_0$" se
            \[
                \exists \lim_{x\to x_0}\frac{f(x)}{|x-x_0|^\alpha} \in \mathbb{R} \setminus \{0\}
            \]
            \item "$f(x)$ è infinita di ordine $\alpha \in \mathbb{R}$ per $x\to x_0$" se
            \[
                \exists \lim_{x\to x_0}f(x)|x-x_0|^\alpha \in \mathbb{R} \setminus \{0\}
            \]
        \end{itemize}

        \item $x_0 \in \{\pm \infty\}$ allora diciamo che 
        
        \begin{itemize}
            \item "$f(x)$ è infinitesima di ordine $\alpha \in \mathbb{R}$ per $x\to x_0$" se
            \[
                \exists \lim_{x\to x_0}f(x)|x|^\alpha \in \mathbb{R} \setminus \{0\}
            \]
            \item "$f(x)$ è infinita di ordine $\alpha \in \mathbb{R}$ per $x\to x_0$" se
            \[
                \exists \lim_{x\to x_0}\frac{f(x)}{|x|^\alpha} \in \mathbb{R} \setminus \{0\}
            \]
        \end{itemize}
         
    \end{itemize}

\end{definizione}

\begin{esercizio}{}{}
    Calcolare l'ordine di infinitesimo per $x\to 0^+$ di $f(x)=\sqrt{1+x^2} - \cos(x)$,
    \end{esercizio}
    \begin{proof}
     per farlo dobbiamo usare la definzione di funzione infinitesima
    \[
        \lim_{x\to 0^+} \frac{\sqrt{1+x^2} - \cos(x)}{x^\alpha}
    \]
    Per capire quanto vale dobbiamo fare una razionalizzazione
   \begin{align*}
    \lim_{x\to 0^+} \frac{\sqrt{1+x^2} - \cos(x)}{x^\alpha} &= \lim_{x\to 0^+} \frac{\sqrt{1+x^2} - \cos(x)}{x^\alpha} \cdot \frac{\sqrt{1+x^2} + \cos(x)}{\sqrt{1+x^2} + \cos(x)} \\ 
    &= \lim_{x\to 0^+} \frac{1+x^2 - \cos^2(x)}{x^\alpha (\sqrt{1+x^2} + \cos(x))}
    \end{align*}

    Il termine $\sqrt{1+x^2} + \cos(x)\to 2$ quindi possiamo levarlo visto che non influisce sul grado della funzione. Notiamo in oltre che possiamo spaccare la funzione nel seguente modo
    \[
    \lim_{x\to 0^+} \frac{1+x^2 - \cos^2(x)}{x^\alpha (\sqrt{1+x^2} + \cos(x))} = \lim_{x\to 0^+} \frac{x^2 }{x^\alpha} + \frac{1 - \cos^2(x)}{x^\alpha}
    \]
    Ed entrambi convergono solo se $\alpha = 2$, e quindi il gradi infinitesimale di $f(x)$ è 2.
    Se fosse stato $f(x)=\sqrt{1-x^2} - \cos(x)$ era necessario usare gli sviluppi dell'o-piccolo.
\end{proof}

\section{Successioni e Serie}
\addcontentsline{toc}{subsection}{Definizione di Successione}
\begin{definizione}{Successioni}{}
    Si definisce successione una funzione a variabile naturale a valori reali, $a: \mathbb{N} \to \mathbb{R}$ e si indica con $(a_n)_{n\in \mathbb{N}}$ oppure $\{a_n\}_{n\in \mathbb{N}}$, mentre il ternime generale si indica con $a(n)$ oppure $a_n$. Il dominio può essere $\mathbb{N}$, $\mathbb{N}_0$ oppure $A = \{n \in \mathbb{N}: n \geq n_0\}$, dove $n_0$ è un qualsiasi numero naturare dal quale inizia la successione. 
\end{definizione}

vediamo qualche esempio di successione 
\begin{esempio}{}{}
    \begin{itemize}
        \item $a_{n} = n! \;\;\; \forall n \in \mathbb{N}_0$
        \item $a_n = (n-5)! \;\;\;\forall n \geq 5$
    
        Qui chiaramente il fattoriale è definitio per numeri non negativi, quindi la successione deve partire da 5, perchè prima non è definita.
        \item $a_n = \frac{1}{n} \;\;\;\forall n \in \mathbb{N}$
        
        In questo caso basta togliere soltanto il caso $n=0$
        \item $a_n = \frac{1}{n-7} \;\;\;\forall n \geq 8$
        
        Teoricamente in questo caso basterebbe levare il caso $n=7$, ma per evitare di avere un "buco" nella sequenza, la facciamo partire da $n=8$.
    \end{itemize}
\end{esempio}

\addcontentsline{toc}{subsection}{Punti di Accumulazione per le Successioni }
\begin{teorema}{Punti di Accumulazione per le Successioni}{}
    Dato che le successioni sono delle funzioni, allora saranno validi tutti i teoremi visti in precedenza, però abbiamo qualche particolarità, in fatti nelle successioni esiste un unico punto di accumulazione: $+\infty$, dato che se prendiamo un qualsiasi altro punto vedremo che è isolato. Infatti per ogni punto del dominio posso sempre trovare un intervallo vuoto, basta scegliere un raggio $< 1$. Quindi nelle successioni possiamo fare solo ed esclusivamente il seguente limite 
    \[
    \lim_{n\to +\infty} a_n =l
    \]
    Di conseguenza possiamo scrivere anche soltanto $\lim a_n = l$ oppure ancora più semplicemente $a_n \to l$.
\end{teorema}

\addcontentsline{toc}{subsection}{Convergenza, Divergenza e Irregolarità delle successioni }
\begin{definizione}{Convergenza, Divergenza e Irregolarità delle successioni}{}
    Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ allora
    \begin{itemize}
        \item Se $\lim  a_n = l\in \mathbb{R}$ allora diciamo che $(a_n)_{n\in A}$ è convergente a $l$
        \item Se $\lim  a_n = l\in \{\pm \infty\}$ allora diciamo che $(a_n)_{n\in A}$ è divergente a $\{\pm \infty\}$
        \item Se $\nexists\lim  a_n$  allora diciamo che $(a_n)_{n\in A}$ è irregolare
 
    \end{itemize}
     
\end{definizione}


\addcontentsline{toc}{subsection}{Monotonia e Limitatezza delle Successioni}
\begin{definizione}{Monotonia e Limitatezza delle Successioni}{}
    Dato che le successioni sono delle funzioni, riprendiamo le prinicipali definizioni delle funzioni a variabili reali e le analiziamo per le successioni

    Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ allora diciamo che 
    \begin{itemize}
        \item $(a_n)_{n\in A}$ è monotona crescente se $a_n \leq a_{n+1} \;\;\; \forall n \in A$
        \item $(a_n)_{n\in A}$ è monotona decrescente se $a_n \geq a_{n+1} \;\;\; \forall n \in A$
    \end{itemize}
    E chiaramente ci sarà anche la monotonia crescente stretta con $a_n < a_{n+1}$ e uguale per la descescenza. In oltre possiamo dire che è definitivamente crescente se è crescente $\forall n \geq \bar{n}$, ragionamento analogo alla decrescenza.

    \begin{itemize}
        \item $(a_n)_{n\in A}$  è limitata se $\exists M \geq 0$ tale che $|a_n| \leq M \;\;\; \forall n \in A$
        \item $(a_n)_{n\in A}$  è limitata definitivamente se $\exists M \geq 0$ tale che $|a_n| \leq M \;\;\; \forall n \geq \bar{n}$
    \end{itemize}

    \textbf{N.B.} come dicevamo le successioni sono vere e proprie funzioni e pertanto saranno validi tutti i seguenti Teoremi: Teorema dell'unicità del limite, Relazione d'ordine \textit{I} e \textit{II}, Teorema dei due carabinieri, algebra dei limiti finiti e infiniti.
\end{definizione}


\addcontentsline{toc}{subsection}{Definizione di Successione Ricorsiva}
\begin{definizione}{Successioni Ricorsive}{}
     Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ allora diciamo che $a_n$ è ricorsiva se esiste una funzione tale che 
     \[
     a_n = f(a_{n-1})
     \]
     Un esempio classico è il fattoriale, infatti il fattoriale lo possiamo scrivere come
     \begin{align*}
        n! &= n\cdot (n-1)! \\
        a_n &= n\cdot a_{n-1}
     \end{align*}
\end{definizione}

\addcontentsline{toc}{subsection}{Limitatezza delle succesioni quando esiste Limite}
\begin{teorema}{Limitatezza delle succesioni quando esiste Limite}{}
    Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ e se $a_n \to l \in \mathbb{R}$ allora $(a_n)_{n\in A}$ è limitata globalmente.
     
\end{teorema}
\begin{proof}
    Nelle funzioni a variabili reali sapevamo che se esiste il limite allora la funzione era limitata definitivamente, proprio perche se $\displaystyle\exists\lim_{x\to x_0} f(x) = l$ sapevamo che 
    \[
        |f(x) - l| < \varepsilon \;\;\;\; \forall x \in I
    \] 
    E da questo potevamo dedurre che 
    \begin{align*}
        |f(x)| &= |f(x) - l + l| \\
        &\leq |f(x) - l| + |l|\\ 
        &< \varepsilon + |l|
    \end{align*}
    E di conseguenza noi sappiamo che $f(x)$ era limitata definitivamente, e quindi questo sarà vero anche per le successioni, ma con le successio possiamo dire qualcosa di più. Infatti se $a_n \to l$ per lo stesso ragionamento possiamo dire che 
    \[
        |a_n| < \varepsilon + |l|  \;\;\; \forall n \geq \bar{n}
    \]
    Ora possiamo prendere tutti i numeri prima di $\bar{n}$ e $\varepsilon + |l|$ e prendere il massimo tra questi valori
    \[
    M = \max\{|a_1|, |a_2|, |a_3|, ... ,|a_{\bar{n}}|,  \varepsilon + |l|\}
    \]
    Importante specificare che $M \not \in \{\pm \infty\}$, perche tutti i valori con $n\leq \bar{n}$ sono valori finiti, e lo stesso vale per $\varepsilon + |l|$. Ora notiamo che per i valori di $n\leq\bar{n}$ saranno sicuramente più piccoli del massimo tra loro. mentre per i valori con $n> \bar{n}$ abbiamo che saranno sempre minori di $\varepsilon + |l|$, di conseguenza $|a_n| < M$ per $\forall n \in A$ che è la definizione di limitata globarlmente, e non solamente definitivamente.
\end{proof}


\addcontentsline{toc}{subsection}{Definizione di Progressione Geormetrica}

\begin{definizione}{Progressione Geormetrica}{}
    Fissato $r\in \mathbb{R}$ detta ragione, allora si dice progressione geometrica la successione definita come
    \[
        a_n = r^n \;\;\;\;\; \forall n\in \mathbb{N}_0
    \]
    \begin{center} il cui limite è $\displaystyle\lim a_n=\begin{cases} +\infty & \text{se } r > 1 \\ 1 & \text{se } r = 1 \\0 & \text{se } -1<r < 1 \\ \text{Non Esiste} & \text{se } r \leq -1 \end{cases}$ \end{center}
    
\end{definizione}


\addcontentsline{toc}{subsection}{Definizione di Sottosuccessione}
\begin{definizione}{Sottosuccessione}{}
    Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$  e una funzione $\varphi: \mathbb{N} \to \mathbb{N}$ strettamente crescente, allora si dice Sottosuccessione, o estratta, di $(a_n)_{n \in A}$ la successione $(a_{\varphi(k)})_{k \in A}$
\end{definizione}

        Vediamo un esempio di sottosuccessione
    \begin{esempio}{}{}
        \[
    \begin{array}{c @{\hspace{2cm}} c}
    a_n = (-1)^n &  \varphi: n \to 2n
    \end{array}
    \]
    \end{esempio}
    \begin{proof}
        Notiamo subito che la funzione $\varphi(n)$ è strettamente crescente, quindi la possiamo usare. Proviamo quindi a trovare la sotto successione $(a_{\varphi(k)})_{k \in \mathbb{N}}$, per farlo basta sostituire la funzione nella $n$ nella funzione originale.
        \[
        a_{\varphi(n)} = (-1)^{2n} \implies a_{\varphi(n)} = ((-1)^{2})^{n} \implies a_{\varphi(n)} = 1^n \implies a_{\varphi(n)} = 1
        \]
        Quindi questa sottosuccessione è semplicemente una sequenza di 1. Da notare che le sottosuccessioni sono come le funzioni composte nelle funzioni a variabile reale. In oltre in questo esempio possiamo notare che se avessimo scelto $\varphi(n) = 2n+1$ la sottosuccessione risultante sarebbe stata  $a_{\varphi(n)} = -1$, e questo vedremo presto implica una cosa molto importante.
    \end{proof}


\addcontentsline{toc}{subsection}{Relazione tra Successione e le sue Sottosuccessioni}
    \begin{teorema}{Relazione tra Successione e le sue Sottosuccessioni}{}
        Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ allora $\exists a_n \to l \in \mathbb{R} \cap \{\pm \infty\}$  se e solo se tutte le sottosuccessioni hanno limite $l$
    \end{teorema}

    Il teorema per com'è scritto non è molto utile, infatti è molto difficile controllare se tutte le possibili sottosuccessioni facciano lo stesso limite, ma è molto più utile la negazione di questo teorema, infatti se è vero che tutte le sottosuccessioni devono avere limite $l$, è anche vero che ne basta trovare almeno una che hanno limite diverso per dire che la successione originale non ha limite.

    \begin{esempio}{}{}
        \[
            a_n = (-1)^n
        \]
    \end{esempio}
    \begin{proof}
        Infatti come avevamo visto se usiamo la sottosuccessione con $\varphi(n) = 2n$, veniva fuori $a_{\varphi(n)} = 1$, mentre se prendevo la sottosuccessione $\varphi(n) = 2n +1$ diventerebbe $a_{\varphi(n)} = -1$, di conseguenza due sottosuccessioni hanno limite diverso allora la successione $a_n = (-1)^n$ non ha limite.
    \end{proof}


\addcontentsline{toc}{subsection}{Teorema di Bolzano-Weierstrass}
    \begin{teorema}{Teorema di Bolzano-Weierstrass}{}
        Se $(a_n)_{n\in A}$ è limitata, allora esiste almeno una sottosuccessione $(a_{\varphi(n)})_{n\in A}$ che converge
    \end{teorema}

    Un esempio lo abbiamo visto appena adesso con la successione $ a_n = (-1)^n$, visto che è una funzione limitata allora almeno una, noi ne abbiamo trovate 2, sottosuccessione converge.

\addcontentsline{toc}{subsection}{Caratterizzazione sequenziale del Limite}
    \begin{teorema}{Caratterizzazione sequenziale del Limite}{}
        Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0$ punto di acc. in $A$ allora 
        \[
            \exists \lim_{x\to x_0} f(x) = l \in \mathbb{R} \cap \{\pm\infty\} \iff  \lim_{n\to +\infty} f(a_n) = l
        \]
        Per ogni successione $(a_n)_{n\in \mathbb{N}}$ tale che
        \begin{itemize}
            \centering
            \item $a_n \ne 0$ definitivamente per $n \to +\infty$
            \item $\lim a_n \to x_0$
        \end{itemize}
    \end{teorema}

    Anche qua come nella relazione tra Successione e Sottosuccessioni, è impossibile trovare qualsiasi successione e controllare che tutte facciano $l$. Questo teorema è molto ultile usare la sua negazione, infatti basta trovare due successioni che rendono il limite diverso per capire che il limite della funzione originale non esiste. Infatti proviamo a calcolare un limite che è difficile da dimostrare, ma che con questo teorema diventa molto semplice.
    
\addcontentsline{toc}{subsection}{Esempi di applicazione della Caratterizzazione sequenziale del Limite}
    \newpage
    \begin{esercizio}{}{}
        \[
            \lim_{x\to +\infty} \sin(x)
        \]
    \end{esercizio}
    \begin{proof}
        Possiamo scegliere $a_n = \frac{\pi}{2} + 2\pi n$ e possiamo vedere che possiamo applicare il teorema perchè $a_n \ne 0$ definitivamente, e che $a_n \to +\infty$, quindi vediamo come diventa il limite
        \[
            \lim_{n\to +\infty} \sin(a_n) = \lim_{n\to +\infty} \sin\left(\frac{\pi}{2} + 2\pi n\right)
        \]
        Usando le proprietà del seno sappiamo che $\sin(\frac{\pi}{2} + 2\pi n) = 1 \;\;\; \forall n \in \mathbb{Z}$ e quindi 
        \[
        \lim_{n\to +\infty} \sin\left(\frac{\pi}{2} + 2\pi n\right) = \lim_{n\to +\infty } 1 = 1
        \]
        Ora ripetiamo lo stesso procedimento per $b_n = \frac{3\pi}{2} + 2\pi n$, ricordanto che per le proprietà del seno $\sin(\frac{3\pi}{2} + 2\pi n) = -1 \;\;\; \forall n \in \mathbb{Z}$
        \[
                   \lim_{n\to +\infty} \sin(b_n) = \lim_{n\to +\infty} \sin\left(\frac{3\pi}{2} + 2\pi n\right) = \lim_{n\to +\infty } -1 = -1
        \]
        Abbiamo trovato quindi due successione che fanno tendere la funzione a due limiti diversi e di conseguenza $\displaystyle\nexists            \lim_{x\to +\infty} \sin(x)$
    \end{proof}

    \begin{esercizio}{}{}
        \[
            \lim_{x\to 0} \frac{1}{x}
        \]
    \end{esercizio}

    \begin{proof}
        Anche se lo abbiamo già dimostrato che questo limite non esiste, proviamo a dimostrarlo grazie a questo teorema, infatti se scelgo $a_n = \frac{1}{n}$, è valida perchè $a_n \to 0$ e di condeguenza il limite diventa
        \[
        \lim_{x\to 0} \frac{1}{x} = \lim_{n\to +\infty} \frac{1}{a_n} = \lim_{n\to +\infty} \frac{1}{\frac{1}{n}} = \lim_{n\to +\infty} n = +\infty
        \] 

        Invece se provia con la successione $b_n = \frac{-1}{n}$ succede che
        \[
        \lim_{x\to 0} \frac{1}{x} = \lim_{n\to +\infty} \frac{1}{b_n} = \lim_{n\to +\infty} \frac{1}{\frac{-1}{n}} = \lim_{n\to +\infty} -n = -\infty
        \] 
        Anche qua vediamo che con la sucessione $a_n$ il nostro limite tende a $+\infty$, mentre con $b_n$ tende a $-\infty$ e pertanto $\displaystyle\nexists \lim_{x\to 0} \frac{1}{x}$, come avevamo già dimostrato.
    \end{proof}
    \newpage
    \begin{esercizio}{}{}
        Sia $f:\mathbb{R} \to \mathbb{R}$ una funzione periodica non costante, Allora
        \[
            \nexists\lim_{x\to +\infty} f(x)
        \]
    \end{esercizio}
    \begin{proof}
        In primis capiamo che ipotesi abbiamo infatti una funzione è periodica se $\exists T > 0$ tale che $f(x+nT) = f(x)\;\;\; \forall n \in \mathbb{Z}, \forall x\in \mathbb{R}$. In più sappiamo che non è costante, che vuol dire  $\exists x_1 \ne x_2$ tali che $f(x_1) \ne f(x_2)$. 

        Quindi se scelgo $a_n = x_1 + nT$, e posso farlo perchè $a_n \to +\infty$ visto che $T >0$ per ipotesi, allora posso sostituirlo
        \[
        \lim_{x\to +\infty} f(x) = \lim_{n\to +\infty} f(a_n) = \lim_{n\to +\infty} f(x_1 + nT)
        \]
        Per definizione di funzione periodica sappiamo che $f(x_1 + nT) = f(x_1)$
        \[
        \lim_{n\to +\infty} f(x_1 + nT) = \lim_{n\to +\infty} f(x_1) = f(x_1)
        \]
        Ora posso scegliere $b_n = x_2 + nT$, e per la stessa logica di $a_n$
        \[
        \lim_{x\to +\infty} f(x) = \lim_{n\to +\infty} f(b_n) = \lim_{n\to +\infty} f(x_2 + nT) = \lim_{n\to +\infty} f(x_2) = f(x_2)
        \]
        Notiamo che in un caso il limite tende a $f(x_1)$ e in un altro tende a $f(x_2)$, e visto che per ipotesi $f(x_1) \ne f(x_2)$ allora $\displaystyle\nexists\lim_{x\to +\infty} f(x)$ 
    \end{proof}

\addcontentsline{toc}{subsection}{Gerarchia degli Infinito per le Successioni}
    \begin{teorema}{Gerarchia degli Infinito per le Successioni}{}
        Visto che le successioni sono funzioni ci portiamo dietro i seguenti teoremi
        
        Sia $\alpha>0, \beta>0, a>1$ 
        \begin{enumerate}[label=(\roman*)]
            \centering
            \item $\displaystyle\lim_{n\to+\infty} \frac{(\log(n))^\beta}{n^\alpha}= 0$
            \item $\displaystyle\lim_{n\to+\infty} \frac{n^\alpha}{a^n}= 0$ 
        \end{enumerate}

        In più, visto che le successioni sono definite per i numeri interi, allora possiamo usare anche il fattoriale, e quindi abbiamo due limiti nuovi
        \begin{enumerate}[label=(\roman*)]
            \setcounter{enumi}{2}
            \centering
            \item $\displaystyle\lim_{n\to+\infty} \frac{a^n}{n!}= 0$
            \item $\displaystyle\lim_{n\to+\infty} \frac{n!}{n^n}= 0$ 
        \end{enumerate}

    \end{teorema}
    \begin{proof}
        I primi due sono verificati perchè sono vere per le funzioni a variabili reali, e di conseguenza valgono anche per le successioni. Iniziamo dimostrando la $(iv)$.
\newpage
        $(iv)$ Notiamo subito che sia $n!\ge 0$ che $n^n \ge 0$ e di conseguenza $\frac{n!}{n^n} \geq 0$. Ora riscriviamo la frazione usando la definizione di fattoriale
        \[
            \frac{n!}{n^n} = \frac{n \cdot (n-1)\cdot (n-2)\cdot ... \cdot 2\cdot 1}{n \cdot n \cdot n \cdot ...\cdot n  \cdot n } = \frac{n}{n} \cdot \frac{n-1}{n} \cdot \frac{n-2}{n} \cdot ...\cdot \frac{2}{n} \cdot \frac{1}{n}     
        \]
        Notiamo subito che il primo termine $\frac{n}{n} = 1$ quindi possiamo toglierlo, in più tutti i termini sono $\leq 1$, quindi 
        \begin{align*}
            \frac{n-1}{n} \cdot \frac{n-2}{n} \cdot ...\cdot \frac{2}{n} \cdot \frac{1}{n} &\leq 1 \cdot \frac{n-2}{n} \cdot ...\cdot \frac{2}{n}  \cdot \frac{1}{n} \\ 
            &\leq 1 \cdot 1 \cdot ... \cdot \frac{2}{n} \cdot \frac{1}{n} \\
            &\leq ... \\
            &\leq 1 \cdot 1 \cdot 1 \cdot 1 \cdot \frac{1}{n} \\
            &\leq \frac{1}{n}
        \end{align*}
        Quindi abbiamo scoperto che
        \[
            0 \leq \frac{n!}{n^n} \leq \frac{1}{n}
        \]
        Visto che $\displaystyle\lim_{n\to+\infty}\frac{1}{n} = 0$, per il teorema dei carabinieri
        \[
        \lim_{n\to+\infty} \frac{n!}{n^n}= 0
        \]

        $(iii)$ Come prima, sappiamo che $a^n \geq 0$ e che $n! \geq 0$ e quindi $\frac{a^n}{n!} \ge 0$. Visto che $a>1$ allora $\exists N \in \mathbb{N}$ tale che $N>a$. Quindi potremo riscrivere la sequenza con $n\geq N$ come        
        \[
        \frac{a^n}{n!} =  \mathrect{red}{\frac{a}{1} \cdot \frac{a}{2} \cdot \frac{a}{3} \cdot ... \frac{a}{N-1} \cdot \frac{a}{N}} \cdot \mathrect{blue}{\frac{a}{N+1} \cdot .. \frac{a}{n-1}} \cdot \frac{a}{n} 
        \]
        Notiamo che tutti i termini nel riquadro rosso sono tutti $\leq a$, dato che $N>1$, mentre tutti i termini nel riquadro blu sono $\leq 1$ perchè $N\ge a \implies 1 \ge \frac{a}{N}$, quindi
        \begin{align*}
        \frac{a^n}{n!}  &\leq \mathrect{red}{a \cdot a \cdot a \cdot ... \cdot a \cdot a } \cdot \mathrect{blue}{1 \cdot ... \cdot 1} \cdot \frac{a}{n} \\    
            &\leq a^N \cdot 1 \cdot \frac{a}{n} \\
            &\leq \frac{a^{N+1}}{n}
    \end{align*}
        Quindi abbiamo scoperto che
        \[
            0 \leq \frac{a^n}{n!} \leq \frac{a^{N+1}}{n}
        \]
        Visto che $a^{N+1}$ è un numero ben definito e non è dipendente da $n$ allora $\displaystyle \lim_{n\to+\infty} \frac{a^{N+1}}{n} = 0$, e quindi per il teorema dei carabinieri 
        \[
        \lim_{n\to+\infty} \frac{n!}{n^n}= 0
        \]
        
    \end{proof}

\addcontentsline{toc}{subsection}{Formula di Stirling}
    \begin{teorema}{Formula di Stirling}{}
        \[
         n! \sim \left(\frac{n}{e}\right)^n \sqrt{2\pi n} \;\;\;\;\; \text{ per } n\to+\infty
        \]
    \end{teorema}
 Vediamo qualche esercizio per allenarci con questa formula
    \begin{esercizio}{}{}
        \[
        \lim_{n\to +\infty} \sqrt[n]{n!} 
        \]
    \end{esercizio}
    \begin{proof}
        Visto che è una equivalenza asintotica possiamo sistutire $n!$
        \[
        \lim_{n\to +\infty} \sqrt[n]{n!} = \lim_{n\to +\infty} \sqrt[n]{ \left(\frac{n}{e}\right)^n \sqrt{2\pi n}} 
        \]
        Facciamo qualche riarrangiamento
        \begin{align*}
             \lim_{n\to +\infty} \sqrt[n]{ \left(\frac{n}{e}\right)^n \sqrt{2\pi n}}  &= \lim_{n\to +\infty} \sqrt[n]{ \left(\frac{n}{e}\right)^n} \cdot \sqrt[n]{ \sqrt{2\pi n}} \\ 
          &= \lim_{n\to +\infty} \left(\frac{n}{e}\right) \cdot \sqrt[2n]{2\pi n} \\
          &= e^{-1} \lim_{n\to +\infty} n\sqrt[2n]{2\pi n}
        \end{align*}
        
        Ora possiamo usare la tecnica delle potenze di funzioni e  applichiamo le regole dei logaritmi
        \begin{align*}
            \lim_{n\to +\infty} n\sqrt[2n]{2\pi n} &= \lim_{n\to +\infty} e^{\log(n\sqrt[2n]{2\pi n})}\\
                &= \lim_{n\to +\infty} e^{\log(n)+ \log((2\pi n)^\frac{1}{2n})} \\ 
                 &= \lim_{n\to +\infty} e^{\log(n)+ \frac{1}{2n}\log(2\pi n)} \\ 
                 &= \lim_{n\to +\infty} e^{\log(n)+ \frac{1}{2}\left(\frac{\log(2)}{n} +\frac{\log(\pi)}{n} +\frac{\log(n)}{n} \right)} 
        \end{align*}
         Ora i termini $\frac{\log(2)}{n}$ e $\frac{\log(\pi)}{n}$ è facile vedere che tendono a 0, poi per la gerarchia degli infiniti anche $\frac{\log(n)}{n}\to 0$, mentre il termine $\log(n) \to +\infty$, quindi l'esponente complessivamente tende a $+\infty$, quindi l'esponenziale tende anche lui a $+\infty$, e di conseguenza 
         \[
         \lim_{n\to +\infty} \sqrt[n]{n!}  = +\infty
         \]
    \end{proof}
\newpage

\addcontentsline{toc}{subsection}{Criterio di convergenza per le Successione}
    \begin{teorema}{Criterio di convergenza per le Successione}{}
        Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ e $(a_n)_{n\in A}$ definitivamente positiva allora se 
        \[
            \exists \lim \frac{a_{n+1}}{a_n} = l \in [0, +\infty) \cup \{+\infty\}
        \]
        allora 
        \begin{enumerate}[label=(\roman*)]
            \centering
            \item $l \in [0, 1) \implies \lim a_n = 0 $
            \item $l \in (1, +\infty) \cup \{+\infty\} \implies \lim a_n = +\infty$
            \item $l = 1 \implies$ nulla si può dire
        \end{enumerate}
    \end{teorema}

    \begin{proof}
        Per dimostrare dobbiamo riprendere la definizione di limite, infatti se esiste il limite allora sappiamo che
        \[
            \lim \frac{a_{n+1}}{a_n} = l \iff \frac{a_{n+1}}{a_n} \in  (l-\varepsilon, l+\varepsilon) \;\;\;\;\; \forall x \in I
        \]
        Riscriviamo il termine
        \[
        \frac{a_{n+1}}{a_n} \in  (l-\varepsilon, l+\varepsilon) \iff l-\varepsilon < \frac{a_{n+1}}{a_n} < l+\varepsilon \implies (l-\varepsilon)a_n < a_{n+1}< (l+\varepsilon)a_n 
        \]
        $(i)$ Per dimostrare il punto $(i)$ ci basta prendere un qualsiasi $N \in \mathbb{N}$ e la disequazione  $\mathunderline{red}{a_{N+1}\leq (l+\varepsilon)a_N }$, infatti dato che è vera possiamo dire anche che $\mathunderline{blue}{a_{N+2}\leq (l+\varepsilon)a_{N+1} }$ e anche $a_{N+3}\leq (l+\varepsilon)a_{N+2} $ e così via, e possiamo dire che  %e seguendo questo ragionamento possiamo dire che  che $a_{n+k+1}< (l+\varepsilon)a_{n+k} $
        \begin{align*}
            a_{N+3} \leq(l+\varepsilon)\mathunderline{blue}{a_{N+2} \leq } & (l+\varepsilon)\mathunderline{blue}{(l+\varepsilon)a_{N+1}} \\
             \leq &(l+\varepsilon)^2a_{N+1}   \\
            a_{N+3} \leq (l+\varepsilon)^2\mathunderline{red}{a_{N+1} \leq} & (l+\varepsilon)^2\mathunderline{red}{(l+\varepsilon)a_{N}} \\
             \leq &(l+\varepsilon)^3a_{N}   \\
            a_{N+3}  \leq &(l+\varepsilon)^3a_{N}
        \end{align*}
        Seguendo questo ragionamento possiamo dire che 
        \[
            a_{N+k} \leq  (l+\varepsilon)^k a_N \;\;\;\;\; \forall k \in \mathbb{N}
        \]
        Ora noi per ipotesi sappiamo che $l \in [0, 1)$, quindi se scegliamo $\varepsilon \in [0, 1-l)$ in modo tale che il termine $(l+\varepsilon)< 1$. Ricordiamo anche che la funzione, per ipotesi, è definitivamente positiva, quindi sarà vero che $a_{N+k}\geq 0$. Ora se mandiamo al limite la seguente disequazione 
        \[
             0 \leq \lim_{k \to +\infty} a_{N+k} \leq \lim_{k \to +\infty} (l+\varepsilon)^k a_N
        \]
        Scopriamo che il termine di destra tende a 0 per le regole degli esponenziali, e quindi complessivamente il termine di destra $0\cdot a_N \to 0$, visto che $a_N$ è un numero finito. In più nel termine centrale possiamo togliere $N$ visto che $k \sim k + N$ per $k\to+\infty$, pertanto per il teorema dei due carabinieri abbiamo che 
        \[
        \lim_{k \to +\infty} a_{k} = 0
        \]
        
        $(ii)$ Per dimostrare il secondo punto ci serve riprendere la disequazione impostata all'inizio della dimostrazione 
        \[
        (l-\varepsilon)a_n < a_{n+1}
        \]
        Ora per lo stesso ragionamento del punto scorso possiamo scegliere un $N \in \mathbb{N}$ e potremo dire che
        \[
         (l-\varepsilon)^k a_N \leq  a_{N+k} \;\;\;\;\; \forall k \in \mathbb{N}
        \] 
        Se scegliamo un $\varepsilon \in [0, l-1)$ avremo che $l-\varepsilon > 1$ e quindi se lo portiamo al limite avremo che $(l-\varepsilon)^k \to +\infty$, e di conseguenza per il corollario del teorema dei due carabinieri abbiamo che 
        \[
        \lim_{k \to +\infty} a_{k} = +\infty
        \] 
        \text{N.B.} è impossibile che  $a_N=0$, infatti $a_N$ è il termine generale di una serie, quindi ammeno che non sia la serie $a_N = 0$ che in quel caso converge, non ci sono problemi che venisse fuori una forma indeterminata $\bigl[\infty \cdot 0\bigr]$.
    \end{proof}


\addcontentsline{toc}{subsection}{Teorema dell'Esistenza del limite di funzioni Monotone}
    \begin{teorema}{Esistenza del limite di funzioni Monotone}{}
        Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ e $(a_n)_{n\in A}$ definitivamente monotona allora $\displaystyle\lim a_n \in \mathbb{R} \cup \{\pm \infty\}$ e possiamo dire anche che 
        \begin{itemize}
            \item Se  $(a_n)_{n\in A}$ è monotona crescente allora
            \[
                \lim a_n = \sup\{a_n : n \in A\}
            \]
            \item Se  $(a_n)_{n\in A}$ è monotona decrescente allora
            \[
                \lim a_n = \inf\{a_n : n \in A\}
            \]
        \end{itemize}
        
    \end{teorema}
        Questo teorema è la versione del Teorema del limite di funzioni monotone applicato alle successioni, quindi non serve la dimostrazione perchè è la stessa delle funzioni.
    

\addcontentsline{toc}{subsection}{Definizione di Serie Numerica}
    \begin{definizione}{Serie Numeriche}{}
        Sia $(a_n)_{n\in\mathbb{N}}$ un successione a valori reali, definiamo la successione delle \textbf{somme parziali} $(S_k)_{k \in \mathbb{N}}$ definita come
        \[
            S_k = \sum_{n=1}^{k} a_n
        \]
        Diciamo "\textbf{serie numerica con termine generale $a_n$}" il limite della successione delle somme parziali, e la indichiamo come
        \[
            \sum_{n=1}^{+\infty} a_n = \lim_{k\to +\infty} S_k \in \mathbb{R} \cup \{\pm \infty\}
        \]
    \end{definizione}
    
    Visto che $S_k$ è una successione, allora anche nelle serie ereditiamo i termini già visti: \textbf{Convergenza},\textbf{Divergenza} e \textbf{Irregolare}
        
\addcontentsline{toc}{subsection}{Convergenza, Divergenza e Irregolarità delle Serie}
    \begin{definizione}{Convergenza, Divergenza e Irregolarità delle Serie}{}
        Sia $(a_n)_{n\in \mathbb{N}}$ una successione, allora diciamo che la serie numerica $\displaystyle\sum_{n=1}^{+\infty} a_n $ è
         \begin{itemize}
        \item Convergente se  $\lim  S_k = l\in \mathbb{R}$
        \item Divergente se $\lim  S_k = l\in \{\pm \infty\}$
        \item Irregolare se $\nexists\lim  S_k$  
    \end{itemize}
     
    \end{definizione}
    
\addcontentsline{toc}{subsection}{Serie Geometrica}
    \begin{esempio}{}{}
        Se prendiamo $a_n = r^n$, con $r \in \mathbb{R}$, per induzione si può dimostrare che la successione delle somme parziale è definita come 
        \[
        S_k = \sum_{n=1}^{k} r^n = \frac{1-r^{k+1}}{1-r} 
        \]
        Pertanto se portiamo tutto al limite abbiamo che 
        \begin{center}
            $\displaystyle\sum_{n=1}^{k} r^n = \begin{cases}
                \frac{1}{1-r} &\text{se } r \in (-1, 1)\\
                +\infty &\text{se } r \geq 1 \\ 
                \nexists &\text{se } r \leq 1 \\ 
            \end{cases}$
        \end{center}
        
        E questa è detta \textbf{Serie Geometrica}.
    \end{esempio}

\addcontentsline{toc}{subsection}{Serie Armonica Generalizzata}
    \begin{teorema}{Serie Armonica Generalizzata }{}
        Dato $\alpha \in [0, +\infty)$ allora con \textbf{Serie Armonica Generalizzata} intendiamo la serie
        \begin{center}
            $\displaystyle\sum_{n=1}^{+\infty} \frac{1}{n^\alpha} = \begin{cases}
                \text{Divergente a } +\infty & \text{se } \alpha \in [0,1] \\
                \text{Convergente}  & \text{se } \alpha > 1 
            \end{cases}$
        \end{center}
    \end{teorema}
    
    \begin{proof}
        (caso $\alpha \in [0,1)$) Scriviamo le somme parziali per capire meglio
        \[
        S_k = \sum_{n=1}^{k} \frac{1}{n^\alpha}= 1 + \frac{1}{2^\alpha} + \frac{1}{3^\alpha} + ... + \frac{1}{k^\alpha}
        \]
        Notiamo che $\forall n\leq k$ vale $\frac{1}{n^\alpha} \geq \frac{1}{k^\alpha}$, quindi possiamo dire che 
         \begin{align*}
            1 + \frac{1}{2^\alpha} + \frac{1}{3^\alpha} + ... + \frac{1}{k^\alpha} &\geq \frac{1}{k^\alpha}+ \frac{1}{k^\alpha} + \frac{1}{k^\alpha} + ... + \frac{1}{k^\alpha} \\
            &= \frac{k}{k^\alpha} = k^{1-\alpha}
         \end{align*}
         Dato che $\alpha \in [0,1)$, appiamo che $1-\alpha > 0$, e quindi se portiamo la sommatoria al limite abbiamo che $k^{1-\alpha} \to +\infty$, e visto che la serie armonica è maggiore di una somma divergente, anche lei diventa divergente.

        (caso $\alpha = 1$)  Notiamo che  la sommatoria $S_k$ è monotona crescente, infatti
        \begin{align*}
           S_{k+1} = \sum_{n=1}^{k+1} \frac{1}{n} &=  \left(\sum_{n=1}^{k} \frac{1}{n}\right) + \frac{1}{k+1} \\ 
         S_{k+1} &= S_k + \frac{1}{k+1}
        \end{align*}
        E notiamo che $\frac{1}{k+1} > 0$, $\forall k > 0$, quindi
       \begin{align*}
         S_{k+1} &= S_k + \frac{1}{k+1} > S_k  + 0\\ 
         S_{k+1}&> S_k
       \end{align*}
       E pertanto la serie è monotona crescente, e visto che la somma delle serie parziali è una successione, per il teorema dell'esistenza di funzioni monotone sappiamo che il limite esiste e deve valere $S \in [1, +\infty] \cup \{+\infty\}$. Osserviamo che
\begin{align*}
    S_{2k} - S_k &= \left(\sum_{n=1}^{2k} \frac{1}{n}\right) - \left(\sum_{n=1}^{k} \frac{1}{n}\right) \\
    &=  \left(\sum_{n=1}^{k} \frac{1}{n}\right) + \left(\sum_{n=k+1}^{2k} \frac{1}{n}\right) - \left(\sum_{n=1}^{k} \frac{1}{n}\right) \\
    &=    \sum_{n=k+1}^{2k} \frac{1}{n} =    \frac{1}{k+1} + \frac{1}{k+2} + \frac{1}{k+3} + ... + \frac{1}{2k} 
\end{align*}
    Come per il caso $\alpha \in [0,1)$, vediamo che $\frac{1}{k+n} \geq \frac{1}{2k}$, $\forall n\leq k$ e quindi
    \begin{align*}
        \frac{1}{k+1} + \frac{1}{k+2} + \frac{1}{k+3} + ... + \frac{1}{2k}  &\geq \frac{1}{2k} + \frac{1}{2k} + \frac{1}{2k} + ... + \frac{1}{2k} \\
        &=  k \cdot \frac{1}{2k} = \frac{1}{2}
    \end{align*}
    Quindi abbiamo scoperto che $S_{2k} - S_k \geq \frac{1}{2}$, quindi sappiamo anche che $S_{2k} \geq S_k + \frac{1}{2}$, ma se supponiamo che  $S\ne +\infty$ vediamo che per $S_{2k} \to S$ e anche $S_k \to S$ e quindi 
    \begin{align*}
        S_{2k}  &\geq S_k +\frac{1}{2}\\
        S &\geq S + \frac{1}{2} \\
        0 &\geq \frac{1}{2}
    \end{align*} 
    Chiaramente è impossibile che $0 \geq \frac{1}{2}$, e quindi vuole dire che l'ipotesi che $S \ne +\infty$ è sbagliata e di conseguenza abbiamo scoperto che $S = +\infty$ e che quindi la serie $\displaystyle\sum_{n=1}^{+\infty} \frac{1}{n}$ è divergente. 
    
\newpage
    (caso $\alpha > 1$) Come per il caso $\alpha = 1$ possiamo dire che la successione delle serie parziali è monotona crescente
    \[
     S_{k+1} = S_k + \frac{1}{(k+1)^\alpha} \geq S_k\;\;\;\;\; \forall k \in \mathbb{N}
    \]
    E di conseguenza il limite esiste e varrà $S \in [1, +\infty) \cup \{+\infty\}$, notiamo che
    \begin{align*}
        S_{2k+1} &= 1 + \frac{1}{2^\alpha} + \frac{1}{3^\alpha} + \frac{1}{4^\alpha} + \frac{1}{5^\alpha} + ... + \frac{1}{(2k)^\alpha} + \frac{1}{(2k+1)^\alpha} \\
        &= 1 + \left(\frac{1}{2^\alpha} + \frac{1}{3^\alpha}\right) + \left(\frac{1}{4^\alpha} + \frac{1}{5^\alpha}\right) + ... + \left(\frac{1}{(2k)^\alpha} + \frac{1}{(2k+1)^\alpha}\right)  \\
        &= 1+ \sum_{n=1}^{k} \left(\frac{1}{(2n)^\alpha} +\frac{1}{(2n+1)^\alpha} \right)
    \end{align*}
    
    Sappiamo che $\frac{1}{(2n+1)^\alpha} \le \frac{1}{(2n)^\alpha}$, $\forall n \in \mathbb{N}$ e quindi 
    \begin{align*}
        \sum_{n=1}^{k} \left(\frac{1}{(2n)^\alpha} +\frac{1}{(2n+1)^\alpha} \right) &\leq \sum_{n=1}^{k} \left(\frac{1}{(2n)^\alpha} +\frac{1}{(2n)^\alpha} \right) \\
        &= \sum_{n=1}^{k} \frac{2}{(2n)^\alpha} \\
        &= \sum_{n=1}^{k} \frac{2^{1-\alpha}}{n^\alpha} \\
        &= 2^{1-\alpha}\sum_{n=1}^{k}\frac{1}{n^\alpha} 
    \end{align*}
    Quindi abbiamo scoperto che 
    \[
        S_{2k+1} \leq 1 + 2^{1-\alpha}\sum_{n=1}^{k}\frac{1}{n^\alpha} 
    \]  
    Però ricordiamo che $\displaystyle\sum_{n=1}^{k}\frac{1}{n^\alpha} = S_k$ e quindi 
    \[
     S_{2k+1} \leq 1 + 2^{1-\alpha} S_k
    \]
    Prima abbiamo detto che la sommatoria ha limite visto che è monotona crescente, quindi supponiamo che $S \in [1, +\infty)$, pertanto se portiamo tutto al limite abbiamo che $S_{2k+1} \to S$ e $S_k \to S$.
    \[
        S \leq 1+2^{1-\alpha} S
    \]
    \[
        S \leq \frac{1}{1-2^{1-\alpha}}
    \]
    Pertanto abbiamo che il limite delle somme parziali è minore di un certo valore finito, quindi la serie converge.
   \end{proof}

   
\addcontentsline{toc}{subsection}{Linearità delle Serie}
    \begin{teorema}{Linearità delle Serie}{}
        Se $\displaystyle \sum_{n=1}^{+\infty} a_n$ e $\displaystyle \sum_{n=1}^{+\infty} b_n$ convergono allora anche $\displaystyle \sum_{n=1}^{+\infty}(c\cdot a_n +  d\cdot b_n)$ converge $(\forall c, d \in \mathbb{R})$. 
    \end{teorema}

    \begin{proof}
        Per dimostrarlo è necessario usare l'algebra dei limiti finiti, infatti se, per ipotesi, $\displaystyle \sum_{n=1}^{+\infty} a_n = A$ e $\displaystyle \sum_{n=1}^{+\infty} b_n = B$, allora sappiamo che 
        \begin{align*}
            \sum_{n=1}^{+\infty}(c\cdot a_n +  d\cdot b_n) &= \lim_{k\to +\infty} \sum_{n=1}^{k}(c\cdot a_n +  d\cdot b_n) \\
            &= \lim_{k\to +\infty} \sum_{n=1}^{k}(c\cdot a_n) +  \lim_{k\to +\infty} \sum_{n=1}^{k}(d\cdot b_n) \\
            &=\lim_{k\to +\infty} c\sum_{n=1}^{k}a_n +  \lim_{k\to +\infty} d\sum_{n=1}^{k}b_n \\
            &= c\cdot A + d\cdot B
        \end{align*}
        E chiaramente, visto che $A$ e $B$ sono dei numeri finiti, allora anche $c\cdot A + d\cdot B$ converge, e pertanto la sommatoria della combinazione lineare è convergente.
    \end{proof}
   
    
   
   \begin{teorema}{Termine Generale in funzione dalla  relativa Serie Numerica}{}
    Dato $(S_k)_{k \in \mathbb{N}}$ una successione delle somme parziali di una successione $(a_n)_{n \in \mathbb{N}}$, allora possiamo trovare l'espressione analitica del termine generale di $(a_n)_{n \in \mathbb{N}}$ come
    \[
        a_n = S_k - S_{k-1}
    \]
    \end{teorema}


\addcontentsline{toc}{subsection}{Condizione Necessaria per la Convergenza di una Serie}
\begin{teorema}{Condizione Necessaria per la Convergenza}{}
        Sia $(a_n)_{ n\in \mathbb{N}}$ una successione, e $(S_k)_{k \in \mathbb{N}}$ la successione delle somme parziali, allora se $(S_k)_{k \in \mathbb{N}}$ è convergente allora 
        \[
            \lim_{n\to +\infty} a_n = 0
        \]
    \end{teorema}

\begin{proof}
    Usando il  termine Generale in funzione dalla  relativa Serie Numerica sappiamo che $a_n = S_k - S_{k-1}$, ma visto che per ipotesi $S_k$ converge a un numero ($S$), allora $S_k \to S$ e anche $S_{k-1} \to S$ pertanto se portiamo quell'informazione al limite abbiamo che
    \begin{align*}
         \lim_{n\to +\infty} a_n &=\lim_{n\to +\infty}( S_k - S_{k-1}) \\
         \lim_{n\to +\infty} a_n &=S - S \\
          \lim_{n\to +\infty} a_n &= 0
    \end{align*}  
\end{proof}

Negli esercizi dovremmo trovare il carattere di una serie, quindi per come è posto questo teorema, non ci è molto utile. Però per le regole della implicazione logica, sappiamo anche che
\begin{center}
    $\displaystyle \lim_{n\to +\infty} a_n\ne 0 \implies \sum_{n=1}^{+\infty} a_n$ non converge.
\end{center} 

\textbf{N.B.} dall'implicazione sappiamo che la serie non converge, cioò vuol dire che o diverge o è irregolare, quindi attensione a non dire che diverge, perchè potrebbe essere irregolare. Per controllare se è divergente o convergente basta controllare se la serie è monotona, e in quel caso allora la serie è divergente.
    
Però se $\displaystyle \lim_{n\to +\infty} a_n = 0$, NON possiamo dire nulla, quindi per questi casi è necessario usare altri criteri. 
Faccendo un esempio:  sia $a_n = \frac{1}{n}$ che $b_n = \frac{1}{n^2}$ abbiamo che
\[
\begin{array}{c @{\qquad} c}
    \displaystyle\lim_{n\to +\infty} a_n =  \lim_{n\to +\infty} \frac{1}{n} = 0 & \displaystyle\lim_{n\to +\infty} b_n =  \lim_{n\to +\infty} \frac{1}{n^2} = 0
\end{array}
\]

Però abbiamo visto con la serie armonica generalizzata abbiamo che $\displaystyle \sum_{n=1}^{+\infty} \frac{1}{n}$ diverge, mentre la serie $\displaystyle \sum_{n=1}^{+\infty} \frac{1}{n^2}$ converge. 

\addcontentsline{toc}{subsection}{Definizione di Serie a Termini Positivi}
\begin{definizione}{Serie a Termini Positivi}{}
    Sia $(a_n)_{ n\in \mathbb{N}}$, se $a_n \geq 0$ $\forall n \in \mathbb{N}$ oppure $\forall n\geq \bar{n}$, allora diciamo che la serie $\displaystyle \sum_{n=1}^{+\infty} a_n$ è a \textbf{termini positivi} oppure a \textbf{termini definitivamente positivi}. 
\end{definizione}
\begin{teorema}{Proprietà delle Serie a Termini Positivi}{}
    Se $\displaystyle \sum_{n=1}^{+\infty} a_n$ è a termini positivi, allora ha limite $S \in [0, +\infty) \cup \{+\infty\}$.
\end{teorema}

\begin{proof}
    Visto che la serie è a termini definitivamente positivi, allora sappiamo che $a_n \geq 0$, $\forall n \geq \bar{n}$ allora sappiamo che
    \[
        S_{k} = S_{k-1} + a_{k} \geq S_{k-1} \;\;\;\;\; \forall n \geq \bar{n}
    \]
    \[
    S_{k} \geq S_{k-1}
    \]
    Abbiamo scoperto che se la serie è a termini positivi allora è anche anche monotona crescente, e dato che sappiamo che se una serie è monotona crescente allora ha limite, di conseguenza anche se una serie è a termini positivi avrà limite convergente o divergente.
\end{proof}

\addcontentsline{toc}{subsection}{Criterio del Confronto delle Serie}
\begin{teorema}{Criterio del Confronto delle Serie}{}
    Sia $(a_n)_{ n\in \mathbb{N}}$ e $(b_n)_{ n\in \mathbb{N}}$ due successioni tali che $0 \leq a_n \leq b_n$ abbiamo che 
    \begin{enumerate}[label=(\roman*)]
        \item se $\displaystyle  \sum_{n=1}^{+\infty} b_n$ converge a $B\in \mathbb{R}$ allora anche $\displaystyle  \sum_{n=1}^{+\infty} a_n$ converge a $A\in \mathbb{R}$ con $A \leq B$.
        \item se $\displaystyle  \sum_{n=1}^{+\infty} a_n$ diverge a $+\infty$ allora anche $\displaystyle  \sum_{n=1}^{+\infty} b_n$ diverge a $+\infty$
    \end{enumerate}
\end{teorema}

\begin{proof}
    Per dimostrarlo è necessario usare le somme parziali, infatti siano
    \[
    \begin{array}{c @{\qquad} c}
        \displaystyle A_k =  \sum_{n=1}^{k} a_n & \displaystyle B_k =  \sum_{n=1}^{k} b_n 
    \end{array}
    \]

    $(i)$  Poi dato che $a_n \leq b_n$ allora è vero anche che $A_k \leq B_n$, pertanto per il teorema di relazione d'ordine sappiamo che se $\displaystyle \sum_{n=1}^{+\infty} b_n= B$ allora   $\displaystyle  \sum_{n=1}^{+\infty} a_n = A$, in più, sempre per il teorema delle relazioni d'ordine sappiamo che $A \leq B$

     $(ii)$ Per dimostrare questo punto invece è necessario usare il corollario del teorema dei carabinieri, infatti se $A_k$ diverge allora anche $B_k$ diverge.
\end{proof}

\begin{esempio}{}{}
    Determinare il carattere della seguente serie
    \[
     \sum_{n=1}^{+\infty} \frac{1}{n(n+1)}
    \]
\end{esempio}

Intanto notiamo che assomiglia molto alla serie $\sum \frac{1}{n^2}$, quindi proviamo a vedere se una è maggiore dell'altra
\[
    \frac{1}{n(n+1)} \leq \frac{1}{n^2}
\]
\[
    n^2 \leq n^2+n
\]
\[
    0 \leq n
\]
Quindi sappiamo che $\frac{1}{n(n+1)} \leq \frac{1}{n^2}$ e quindi per il criterio del confronto sappiamo che se $\sum \frac{1}{n^2}$ convergesse allora anche $\sum \frac{1}{n(n+1)}$ converge. Però abbiamo visto con le serie armoniche che $\sum \frac{1}{n^2}$ converge, visto che l'esponente $2>1$ e di conseguenza anche $\displaystyle\sum_{n=1}^{+\infty} \frac{1}{n(n+1)}$ converge. Per questa serie lo potevamo scoprire anche usando le serie telescopische, infatti $\frac{1}{n(n+1)} = \frac{1}{n} - \frac{1}{n+1}$ e quindi $\displaystyle\lim\sum_{n=1}^{k} \frac{1}{n(n+1)} = \lim_{k\to+\infty} \left(1 - \frac{1}{k+1}\right) = 1$, e infatti la serie converge a 1.

\addcontentsline{toc}{subsection}{Definizione Serie Assolutamente Convergente}
\begin{definizione}{Serie Assolutamente Convergente}{}
    Sia $(a_n)_{ n\in \mathbb{N}}$ una successione, la serie $ \displaystyle \sum_{n=1}^{+\infty} a_n$ si dice \textbf{Assolutamente Convergente} se la serie $\displaystyle \sum_{n=1}^{+\infty} |a_n|$ converge. 
\end{definizione}

\begin{teorema}{Relazione Convergenza Assoluta e Semplice}{}
    Sia $(a_n)_{ n\in \mathbb{N}}$ una successione, la serie $ \displaystyle \sum_{n=1}^{+\infty} a_n$  è assolutamente convergente, allora è anche converge anche semplicemente.
\end{teorema}
\begin{proof}
    Proviamo a sviluppare la seguente serie, e applichiamo k-volte la diseguaglianza triangolare
    \begin{align*}
        \left|\sum_{n=1}^{k}a_n\right| &= |a_1 + a_2 + a_3 + ... + a_k| \\
        &\leq |a_1 |+ |a_2 + a_3 + ... + a_k|  \\
        &\leq |a_1 |+ |a_2| + |a_3 + ... + a_k|  \\
        &\leq ...  \\
        &\leq |a_1 |+ |a_2| + |a_3| + ... + |a_k|  \\
        \left|\sum_{n=1}^{k}a_n\right| &\leq \sum_{n=1}^{k} |a_n| 
    \end{align*}
    Pertanto se, per ipotesi, $\sum |a_n|$ converge a $S\in \mathbb{R}^+$, allora se portiamo al limite la disequazione abbiamo che  
    \[
    \left|\sum_{n=1}^{+\infty}a_n\right| \leq S \iff  \sum_{n=1}^{+\infty}a_n \in [-S, S]
    \]
    Però visto che $S$ è un numero finito, e sappiamo che la serie è compresa tra due valori finiti, allora di conseguenza anche la serie $\displaystyle\sum_{n=1}^{+\infty} a_n$ converge semplicemente.
\end{proof}

\textbf{N.B.} la covergenza assoluta implica la convergenza semplice, ma NON è vero il viceverca, vediamolo con un esempio.

\begin{esempio}{}{}
    Determinare il carattere della serie
    \[
        \sum_{n=1}^{+\infty} \frac{(-1)^n}{n}
    \]
\end{esempio}

Notiamo subito che la serie non converge assolutamente infatti 
\[
\left|\frac{(-1)^n}{n}\right| = \frac{1}{n}
\]
Che per la serie armonica sappiamo che diverge, quindi sappiamo che la serie $\sum \frac{(-1)^n}{n}$ diverge assolutamente. Però ora proviamo a vedere se converge o diverge semplicemente, per farlo proviamo analizando la serie delle somme parziali
\begin{align*}
    S_{2k} &= \frac{(-1)^1}{1} + \frac{(-1)^2}{2} + \frac{(-1)^3}{3} + \frac{(-1)^4}{4} + ... + \frac{(-1)^{2k-1}}{2k-1} +\frac{(-1)^{2k}}{2k}\\
    &= \left(1+ \frac{1}{2}\right) + \left(\frac{-1}{3}+ \frac{1}{4}\right)  + ... + \left(\frac{-1}{2k-1} +\frac{1}{2k}\right) \\
    &= \sum_{n=1}^{k} \left(\frac{-1}{2n-1} + \frac{1}{2n}\right)\\
    &= \sum_{n=1}^{k} \frac{-2n + 2n -1}{(2n-1)2n} =  -\sum_{n=1}^{k} \frac{1}{(2n-1)2n}
\end{align*}

Ora questa serie assomiglia molto alla serie armonica con $\alpha = 2$ che converge, quindi proviamo a vedere se può funzionare il criterio del confronto
\[
     \frac{1}{(2n-1)2n} \leq \frac{1}{n^2} \iff n^2 \leq 4n^2 - 2n
\]
\[
    \iff 2n \leq 3n^2 \iff 2 \leq 3n \iff n \geq \frac{2}{3}
\]
Quindi sappiamo che la nostra serie è minore della serie armonica, pertanto la serie $\sum \frac{(-1)^n}{n}$ converge semplicemente, ma diverge assolutamente.

\addcontentsline{toc}{subsection}{Criterio del Confronto Asintotico}
\begin{teorema}{Criterio del Confronto Asintotico}{}
    Siano $(a_n)_{ n\in \mathbb{N}}$ e $(b_n)_{ n\in \mathbb{N}}$ due successioni definitivamente positive tali che
    \[
        \exists \lim_{n\to +\infty} \frac{a_n}{b_n} = l \in [0, +\infty) \cup \{+\infty\}
    \]
    Allora 
    \begin{enumerate}[label=(\roman*)]
        \item se $l \in [0, +\infty)$ allora $\sum a_n$ e $\sum b_n$ hanno lo stesso carattere.
        \item se $l=0$ allora valgono:
        \begin{itemize}
            \item se $\sum b_n$ converge allora $\sum a_n$ converge
            \item  se $\sum a_n$ diverge allora $\sum b_n$ diverge
        \end{itemize}
        \item se $l=+\infty$ allora valgono:
        \begin{itemize}
            \item se $\sum a_n$ converge allora $\sum b_n$ converge
            \item  se $\sum b_n$ diverge allora $\sum a_n$ diverge
        \end{itemize}
    \end{enumerate}
\end{teorema}

\begin{proof}
    $(i)$ Usiamo la definizione di limite
    \[
        l-\varepsilon < \frac{a_n}{b_n} < l+\varepsilon \;\;\;\;\; \forall n \geq N
    \]
    Ora per comodità possiamo scegliere $\varepsilon = \frac{l}{2}$
    \[
        l-\frac{l}{2} < \frac{a_n}{b_n} < l+\frac{l}{2}
    \]
    \[
    \frac{l}{2} < \frac{a_n}{b_n} < \frac{3l}{2}
    \]
    \[
    \frac{l}{2}b_n< a_n < \frac{3l}{2}b_n
    \]
    E quindi 
    \[
        \mathunderline{red}{\frac{l}{2}\sum_{n=N}^{k}b_n < \sum_{n=N}^{k}} \mathunderline{blue}{a_n < \frac{3l}{2}\sum_{n=N}^{k}b_n}
    \]
    E quindi dalla disequazione sottolineata di rosso sappiamo, grazie al criterio del confronto, se $\sum b_n$ diverge allora $\sum a_n$ diverge, mentre con la disequazione in blu abbiamo che se $\sum b_n $ converge allora anche $\sum a_n$ converge. Pertanto notiamo che le due serie hanno sempre lo stesso carattere. 

    \textbf{N.B.} non possono essere irregolari visto che per ipotesi devono essere a termini positivi, che ricordiamo che per le proprietà delle serie a termini positivi, non possono essere irregolari. 

    $(ii)$ Usiamo la definizio di limite
    \[
         -\varepsilon < \frac{a_n}{b_n} < +\varepsilon \;\;\;\;\; \forall n \geq N
    \]
    Poi dato che $a_n$ e $b_n$ sono definitivamente positivi abbiamo che $\frac{a_n}{b_n} > 0$, in più possiamo scegliere $\varepsilon = 1$ e quindi abbiamo
    \[
    0 < \frac{a_n}{b_n} < 1 \implies a_n < b_n
    \]
    Pertanto $\sum a_n < \sum b_n$ e quindi per il teorema del confronto verifichiamo il teorema.

     $(iii)$ Usiamo la definizione di limite
     \[
     \frac{a_n}{b_n} > M \;\;\;\;\; \forall n \geq N
     \]
     Pertanto possiamo dedurre che
     \[ 
     a_n > M\cdot b_n \implies \sum a_n > M \sum b_n
     \]
     Ed ora possiamo il teorema del confronto per dimostrare questo punto.
\end{proof}
\newpage
\begin{esercizio}{}{}
    Determina il carattere della seguente serie
    \[
    \sum_{n=1}^{+\infty} \frac{\sqrt{n^2+1}-n}{n}
    \]
\end{esercizio}

\begin{proof}
    In primis proviamo a controllare la condizione necessaria per la convergenza
    \[
        \lim_{n\to+\infty} \frac{\sqrt{n^2+1}-n}{n} = \lim_{n\to+\infty} \frac{\sqrt{n^2+1}}{n} + \frac{n}{n} = \lim_{n\to+\infty} \sqrt{1+\frac{1}{n^2}} - 1 = 1-1 = 0
    \]
    E quindi non possiamo dire nulla con la condizione necessaria di convergenza, quindi proviamo razionalizzando
    \begin{align*}
        \frac{\sqrt{n^2+1}-n}{n} &= \frac{\sqrt{n^2+1}-n}{n} \cdot \frac{ \sqrt{n^2+1}+n}{\sqrt{n^2+1}+n} \\
        &= \frac{(n^2+1)-n^2}{n\bigl(\sqrt{n^2+1}+n\bigr)} = \frac{1}{n\bigl(\sqrt{n^2+1}+n\bigr)}
     \end{align*}
     Possiamo notare che la nuova frazione è $\sim \frac{1}{2n^2}$ infatti
     \[
        \lim_{n\to +\infty} \frac{\frac{1}{n\bigl(\sqrt{n^2+1}+n\bigr)}}{\frac{1}{2n^2}} = \lim_{n\to +\infty} \frac{\frac{1}{n^2\bigl(\sqrt{1+\frac{1}{n^2}}+1\bigr)}}{\frac{1}{2n^2}} = \lim_{n\to +\infty} \frac{\frac{1}{\sqrt{1+\frac{1}{n^2}}+1}}{\frac{1}{2}} = \frac{\frac{1}{1+1}}{\frac{1}{2}} = 1 
     \]
     Pertanto le serie avranno lo stesso carattere, ma $\frac{1}{2n^2}$ sappiamo che converge visto che è la serie armonica generalizzata con $\alpha=2$, e di conseguenza anche la serie $\sum \frac{\sqrt{n^2+1}-n}{n}$ convergerà per il criterio del confronto asintotico.
    \end{proof}

    \begin{esercizio}{}{}
    Determina il carattere della seguente serie
    \[
    \sum_{n=1}^{+\infty} \sin\left(\frac{1}{3^n}\right)\arcsin\left(\frac{1}{2^n}\right)
    \]
\end{esercizio}
\begin{proof}
    Per i confronti asintotici sappiamo che per $n\to +\infty$
    \[
    \begin{array}{c @{\qquad}@{\qquad} c}
       \displaystyle \sin\left(\frac{1}{3^n}\right) \sim \frac{1}{3^n} & \displaystyle\arcsin\left(\frac{1}{2^n}\right) \sim \frac{1}{2^n}
    \end{array}
    \]
    Pertanto $ \displaystyle\sin\left(\frac{1}{3^n}\right)\arcsin\left(\frac{1}{2^n}\right) \sim \frac{1}{3^n} \cdot  \frac{1}{2^n} = \frac{1}{6^n}$, ma noi sappiamo che $\displaystyle\sum \frac{1}{6^n}$ converge visto che è una serie geometrica di ragione $r = \frac{1}{6}$ e quindi converge, ma visto che la serie $\displaystyle\sum \sin\left(\frac{1}{3^n}\right)\arcsin\left(\frac{1}{2^n}\right)$ è asintotica a $\displaystyle\sum \frac{1}{6^n}$ allora anche l'altra serie è convergente.
\end{proof}

\begin{esercizio}{}{}
    Discutere il carattere della serie al variare di $\alpha \geq 0$
    \[
    \sum_{n=1}^{+\infty}\frac{1-\cos(1-\cos(\frac{1}{n}))}{e^{\tan(\frac{1}{n^\alpha})}-1}
    \]
\end{esercizio}

\begin{proof}
    Iniziamo a fare delle equivalenze asintotiche, infatti per $n\to+\infty$
    \[
        1-\cos\left(\frac{1}{n}\right) \sim \frac{1}{2n^2}
    \]
    E pertanto 
    \[
        1-\cos\left(1-\cos\left(\frac{1}{n}\right)\right) \sim 1-\cos\left(\frac{1}{2n^2}\right) \sim \frac{1}{8n^4}
    \]
    Mentre a denominatore iniziamo con 
    \[
        \tan\left(\frac{1}{n^\alpha}\right) \sim \frac{1}{n^\alpha}
    \]
    Di conseguenza
    \[
    e^{\tan(\frac{1}{n^\alpha})}-1 \sim e^{\frac{1}{n^\alpha}}-1 \sim \frac{1}{n^\alpha}
    \]
    Ricomponendo la frazione abbiamo che 
    \[
    \sum_{n=1}^{+\infty}\frac{1-\cos(1-\cos(\frac{1}{n}))}{e^{\tan(\frac{1}{n^\alpha})}-1} \sim \frac{\frac{1}{8n^4}}{\frac{1}{n^\alpha}} = \frac{1}{8n^{4-\alpha}}
    \]
    e per la serie armonica sappiamo che
    \[
    \sum \frac{1}{8n^{4-\alpha}} = \begin{cases}
        \text{Converge} & \text{se } 4-\alpha > 1 \\
        \text{Diverge} & \text{se } 4-\alpha \leq 1
    \end{cases} = \begin{cases}
        \text{Converge} & \text{se } \alpha < 3 \\
        \text{Diverge} & \text{se } \alpha \geq 3
    \end{cases}
    \]
    E chiaramente dato che le due serie sono asintotiche tra di loro, per forza devo avere lo stesso carattere, e quindi 
     \[
    \sum_{n=1}^{+\infty}\frac{1-\cos(1-\cos(\frac{1}{n}))}{e^{\tan(\frac{1}{n^\alpha})}-1} =\begin{cases}
        \text{Converge} & \text{se } \alpha < 3 \\
        \text{Diverge} & \text{se } \alpha \geq 3
    \end{cases}
    \]
\end{proof}
\newpage

\addcontentsline{toc}{subsection}{Criterio del Rapporto}
\begin{teorema}{Criterio del Rapporto}{}
    Sia $(a_n)_{n\in \mathbb{N}}$ tale che $a_n > 0$ definitivamente allora
    \begin{enumerate}[label=(\roman*)]
        \item Se $\exists r \in (0, 1)$ tale che $\displaystyle\frac{a_{n+1}}{a_n} \leq r$ definitivamente allora la serie $\sum a_n$ converge
         \item Se $\exists r\in [1, +\infty)$ tale che $\displaystyle\frac{a_{n+1}}{a_n} \geq r$ definitivamente allora la serie $\sum a_n$ diverge
    \end{enumerate}
\end{teorema}

\begin{proof}
    $(i)$ Per ipotesi sappiamo che è vero definitivamente che
    \[
        a_{N+1} \leq r\cdot a_N \;\;\;\;\; \text{con } N\in \mathbb{N}
    \]
    Per lo stesso ragionamento fatto per il criterio di convergenza delle Successioni possiamo dire che
    \[
    a_{N+k} \leq r^k\cdot a_N \;\;\;\;\; \forall k \geq 1
    \]
    Di conseguenza abbiamo che 
    \begin{align*}
        \sum a_{N+k} &\leq \sum r^k\cdot a_N \\
        \sum a_{N+k} &\leq a_N \sum r^k
    \end{align*}
   
    E chiaramente per $r \in (-1,1)$ abbiamo che la serie $ \sum r^k$ converge, e pertanto per il criterio del confronto abbiamo che anche la serie $\sum a_{N+k}$ converge. Dato che la successione è a termini positivi allora $\displaystyle\frac{a_{n+1}}{a_n} > 0$, pertanto è impossibile che $\displaystyle\frac{a_{n+1}}{a_n} \in (-1,0] $ e quindi possiamo trovare solamente valori di $r \in (0, 1)$.
    $(ii)$ è necessiario fare lo stesso ragionamento, chiaramente la disequazione sarà
    \[
     a_{N+k} \geq r^k\cdot a_N \;\;\;\;\; \forall k \geq 1
    \]
    E quindi la serie geometrica diverge per valori di $r\in [1,+\infty)$, e quindi la serie $\sum a_{N+k}$ diverge anch'essa.
\end{proof}

\addcontentsline{toc}{subsection}{Criterio del Rapporto Asintotico}
\begin{teorema}{Criterio del Rapporto Asintotico}{}
    Sia $(a_n)_{n\in \mathbb{N}}$ tale che $a_n > 0$ definitivamente allora
    \[
        \exists \lim_{n\to +\infty} \frac{a_{n+1}}{a_n} = l \in [0, +\infty) \cup \{ +\infty\}
    \]
    Allora 
    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $\displaystyle l < 1 \implies \sum a_n$ converge 
        \item $\displaystyle l > 1 \implies \sum a_n$ diverge
        \item $\displaystyle l = 1 $ Non possiamo dire nulla sulla serie   
    \end{enumerate}
\end{teorema}

\begin{esercizio}{}{}
    Determina il carattere della serie 
    \[
    \sum_{n=1}^{+\infty} \frac{(3n)!}{n^{3n}}
    \]
\end{esercizio}
\begin{proof}
    Proviamo il criterio del rapporto asintotico 
    \begin{align*}
        \lim_{n\to +\infty} \frac{a_{n+1}}{a_n} &= \frac{(3(n+1))!}{(n+1)^{3(n+1)}} \cdot \frac{n^{3n}}{(3n)!} = \frac{(3n+3)!}{(n+1)^{3n+3}} \cdot \frac{n^{3n}}{(3n)!} = \frac{(3n+3)!}{(n+1)^{3n+3}} \cdot \frac{n^{3n}}{(3n)!} \\
        &= \frac{(3n+3)(3n+2)(3n+1)(3n)!}{(n+1)^{3n} (n+1)^3} \cdot \frac{n^{3n}}{(3n)!} \\
        &= \frac{(3n+3)(3n+2)(3n+1)}{ (n+1)^3} \cdot \frac{n^{3n}}{(n+1)^{3n}} \\
        &=\frac{(3n+3)(3n+2)(3n+1)}{ (n+1)^3} \cdot \left(\frac{n}{n+1}\right)^{3n} \sim   \frac{(3n)(3n)(3n)}{ (n)^3} \cdot \left(\frac{n+1}{n}\right)^{-3n} \\
        &=\frac{3^3n^3}{ n^3} \cdot \left(1+\frac{1}{n}\right)^{-3n} = 3^3\cdot e^{-3} = \left(\frac{3}{e}\right)^3 > 1
    \end{align*}
    E pertanto per il criterio del rapporto asintotico abbiamo che la serie diverge.
\end{proof}

Questa serie si poteva risolvere anche con il criterio necessario per la convergenza e tramite la formula di Stirling, infatti
\begin{align*}
    \lim_{n\to+\infty} \frac{(3n)!}{n^{3n}} &\sim \lim_{n\to+\infty} \frac{\left(\frac{3n}{e}\right)^{3n} \sqrt{2\pi (3n)}}{n^{3n}} = \lim_{n\to+\infty} \frac{\left(\frac{3}{e}\right)^{3n} n^{3n} \sqrt{6\pi n}}{n^{3n}}   \\
&= \lim_{n\to+\infty} \left(\frac{3}{e}\right)^{3n}  \sqrt{6\pi n} = +\infty
\end{align*}
    \begin{esercizio}{}{}
        Discutere il carattere della serie al variare di $x\in \mathbb{R}$
        \[
            \sum_{n=1}^{+\infty}\frac{x^n}{n^32^n}
        \]
    \end{esercizio}
    \begin{proof}
        Dato che $x\in \mathbb{R}$ per valori negativi avremo che la nostra serie non è a termini positivi, e quindi non potremmo applicare il teorema del rapporto asintotico, quindi proviamo a vedere se converge la serie $\sum \left|\frac{x^n}{n^32^n}\right| = \sum \frac{|x|^n}{n^32^n}$, in questa maniera la serie è a termini positivi e quindi possiamo applicare il teorema del criterio asintotico. 
        \[
            \lim_{n\to+\infty} \left|\frac{x^{n+1}}{(n+1)^32^{n+1}}\right| \cdot \left|\frac{n^32^n}{x^n}\right| = \lim_{n\to+\infty}\left|\frac{x}{2}\cdot \frac{n^3}{(n+1)^3}\right|  \sim \lim_{n\to+\infty}\left|\frac{x}{2}\right|\cdot \frac{n^3}{(n)^3}= \frac{|x|}{2}
        \]
        Per il criterio del rapporto asintotico sappiamo che se $\frac{|x|}{2}>1$, cioè $x < -2 \lor x >2$ allora la serie diverge assolutamente, mentre per $\frac{|x|}{2}<1$, cioò per $-2 <x < 2$ la serie converge assolutamente e quindi anche semplicemente. Però ora mancano i caso $x=\pm2$, visto che il criterio non ci permette di dedurre nulla, e quindi dobbiamo fare altro. Però notiamo subito che se $x=2$ allora
        \[
        \sum_{n=1}^{+\infty}\frac{x^n}{n^32^n} = \sum_{n=1}^{+\infty}\frac{2^n}{n^32^n} = \sum_{n=1}^{+\infty}\frac{1}{n^3}  
        \] 
        Notiamo che viene fuori la somma armonica generalizzata con $\alpha = 3$, che sappiamo che converge, mentre per il caso $x=-2$, possiamo riutilizzare il criterio della convergenza assoluta, infatti $\sum \left|\frac{(-2)^n}{n^32^n}\right| = \sum \frac{|-2|^n}{n^32^n} = \sum \frac{2^n}{n^32^n} = \sum \frac{1}{n^3}$ che abbiamo appena visto che converge. Pertanto per il caso $x=-2$ la serie converge assolutamente e quindi anche semplicemente. 
        \[
           \sum_{n=1}^{+\infty}\frac{x^n}{n^32^n} = \begin{cases}
            \text{Diverge} & x < -2 \lor x > 2 \\
            \text{Converge Semplicemente e Assolutamente} & -2 \leq x \leq 2 \\
           \end{cases} 
        \]
        \textbf{N.B.} n caso $x < -2 \lor x > 2 $ ho scritto che diverge, e questo si può verificare con la condizione necessaria alla convergenza.  
    \end{proof}

    \begin{esercizio}{}{}
        Discutere il carattere della serie al variare di $\alpha\geq 0$
        \[
            \sum_{n=1}^{+\infty}\frac{n!\cdot \alpha^n}{n^n}
        \]
    \end{esercizio}
\begin{proof}
        Proviamo usando il criterio del rapporto asintotico
\begin{align*}
       \lim_{n\to+\infty}\frac{a_{n+1}}{a_n} &= \lim_{n\to+\infty} \frac{(n+1)! \cdot \alpha^{n+1}}{(n+1)^{n+1}} \cdot \frac{n^n}{n! \cdot \alpha^n} = \lim_{n\to+\infty} \frac{(n+1)n! \cdot \alpha}{(n+1)(n+1)^n} \cdot \frac{n^n}{n!} \\
       &= \lim_{n\to+\infty}\alpha \cdot \left(\frac{n}{n+1}\right)^{n} = \lim_{n\to+\infty}\alpha \cdot \left(\frac{n+1}{n}\right)^{-n} \\
       &= \lim_{n\to+\infty}\alpha \cdot \left(1+\frac{1}{n}\right)^{-n}= \alpha \cdot e^{-1} =\frac{\alpha}{e}
    \end{align*}
    Quindi se $\frac{\alpha}{e} > 1 \implies \alpha > e$ allora la serie diverge, mentre se $\frac{\alpha}{e} < 1 \implies \alpha < e$ la serie converge. Proprio come l'esercizio precedente dobbiamo studiare a parte il caso $\alpha = e$, e notiamo che la serie diventa
    \[
    \sum_{n=1}^{+\infty}\frac{n!\cdot \alpha^n}{n^n} = \sum_{n=1}^{+\infty}\frac{n!\cdot e^n}{n^n}
    \] 
Proviamo a vedere con la condizione necessaria
\[
\lim_{n\to +\infty} \frac{n!\cdot e^n}{n^n} \sim \lim_{n\to +\infty} \frac{\left(\frac{n}{e}\right)^n\sqrt{2\pi n}\cdot e^n}{n^n} = \lim_{n\to +\infty} \frac{\frac{n^n}{e^n}\sqrt{2\pi n} \cdot e^n}{n^n} = \lim_{n\to+\infty}\sqrt{2\pi n} = +\infty
\]
E quindi per condizione necessaria sappiamo che con $\alpha = e$ la serie diverge. E quindi
\[
\sum_{n=1}^{+\infty}\frac{n!\cdot \alpha^n}{n^n} = \begin{cases}
            \text{Converge } & \alpha < e \\
            \text{Diverge} & \alpha \geq e \\
           \end{cases} 
\]
        \end{proof}

        \newpage

\addcontentsline{toc}{subsection}{Criterio della Radice}
        \begin{teorema}{Criterio della Radice}{}
             Sia $(a_n)_{n\in \mathbb{N}}$ tale che $a_n > 0$ definitivamente allora
        \begin{enumerate}[label=(\roman*)]
            \item Se $\exists r \in (0, 1)$ tale che $\displaystyle\sqrt[n]{a_n} \leq r$ definitivamente allora la serie $\sum a_n$ converge
            \item Se $\exists r\in [1, +\infty)$ tale che $\displaystyle\sqrt[n]{a_n} \geq r$ definitivamente allora la serie $\sum a_n$ diverge
        \end{enumerate}
        \end{teorema}

        \begin{proof}
            $(i)$ Dato che $a_n$ è definitivamente positiva, e visto che $n \in \mathbb{N}$ possiamo dire che
            \[
            \sqrt[n]{a_n} \leq r \implies a_n \leq r^n \implies \sum a_n \leq \sum r^n
            \]
            Se portiamo tutto al limite abbiamo che
            \[
               \lim_{k\to+\infty} \sum_{n=1}^{k} a_n \leq \lim_{k\to+\infty} \sum_{n=1}^{k} r^n
            \]
            Come abbiamo già visto, la serie geometrica converge solamente se $-1 < r < 1$, e quindi per il confronto abbiamo che anche la serie $ \sum a_n$ converge. Però dato che la serie è a termini positivi è impossibile che $r \in (-1,0]$ e quindi potremo avere valore di $r \in (0, 1)$.
            
            $(ii)$ ragionamento analogo al caso $(i)$, e quindi possiamo dire che 
            \[
            \sqrt[n]{a_n} \geq r \implies \sum a_n \geq \sum r^n
            \]
            Se portiamo tutto al limite abbiamo che
            \[
               \lim_{k\to+\infty} \sum_{n=1}^{k} a_n \geq \lim_{k\to+\infty} \sum_{n=1}^{k} r^n
            \]
            Ma la serie geometrica diverge per $r \leq -1 \lor r \geq 1$ e di conseguenza anche la serie $\sum a_n$ diverge. Però dato che la serie è a termini positivi è impossibile che $r \leq -1$ e quindi potremo avere valore di $r \geq 1$.
        \end{proof}

\addcontentsline{toc}{subsection}{Criterio della Radice Asintotico}
        \begin{teorema}{Criterio della Radice Asintotico}{}
        Sia $(a_n)_{n\in \mathbb{N}}$ tale che $a_n > 0$ definitivamente allora
            \[
                \exists \lim_{n\to +\infty} \sqrt[n]{a_n} = l \in [0, +\infty) \cup \{ +\infty\}
            \]
            Allora 
            \begin{enumerate}[label=(\roman*)]
                \centering
                \item $\displaystyle l < 1 \implies \sum a_n$ converge 
                \item $\displaystyle l > 1 \implies \sum a_n$ diverge
                \item $\displaystyle l = 1 $ Non possiamo dire nulla sulla serie   
            \end{enumerate}
        \end{teorema}

        \begin{esercizio}{}{}
            Determinare il carattere della serie 
            \[
                \sum_{n=1}^{+\infty} \left(\frac{1}{2} + \frac{1}{2n}\right)^n
            \]
        \end{esercizio}
        \begin{proof}
            Se proviamo ad usare la condizione necessaria abbiamo che
            \[
            \lim_{n\to+\infty} \left(\frac{1}{2} + \frac{1}{2n}\right)^n \sim \lim_{n\to+\infty} \left(\frac{1}{2}\right)^n = 0
            \]
            E quindi non possiamo dedurre nulla. Quindi proviamo ad usare il criterio della radice
            \[
            \lim_{n\to+\infty} \sqrt[n]{\left(\frac{1}{2} + \frac{1}{2n}\right)^n} =  \lim_{n\to+\infty} \left(\frac{1}{2} + \frac{1}{2n}\right) =\frac{1}{2} +0 = \frac{1}{2} < 1 
            \]
            Notiamo che il risultato è minore di 1, e quindi per il criterio della radice sappiamo che la seria $\displaystyle\sum \left(\frac{1}{2} + \frac{1}{2n}\right)^n$ è convergente.
        \end{proof}
        \begin{esercizio}{}{}
            Determinare il carattere della serie al variare $\alpha \geq 0$
            \[
                \sum_{n=1}^{+\infty} \alpha^n \left(1+ \frac{2}{n}\right)^n
            \]
        \end{esercizio}

        \begin{proof}
            Utiliziamo il criterio della radice
            \[
            \lim_{n\to+\infty} \sqrt[n]{\alpha^n \left(1+ \frac{2}{n}\right)^n} = \lim_{n\to+\infty}\alpha \left(1+ \frac{2}{n}\right) = \alpha
            \]
            Quindi visto che il risultato del limite è $\alpha$, abbiamo che per $\alpha>1$ la serie diverge, mentre per $\alpha <1$ la serie converge. Come al solito dobbiamo studiare a parte il caso $\alpha = 1$. Abbiamo che
            \[
            \sum_{n=1}^{+\infty} \alpha^n \left(1+ \frac{2}{n}\right)^n = \sum_{n=1}^{+\infty} 1^n \left(1+ \frac{2}{n}\right)^n = \sum_{n=1}^{+\infty} \left(1+ \frac{2}{n}\right)^n
            \]
            Per questa serie è necessario usare la condizione necessaria
            \[
            \lim_{n\to+\infty}  \left(1+ \frac{2}{n}\right)^n = e^2 \ne 0
            \]
            E quindi abbiamo che la serie con $\alpha = 1$ diverge. E quindi abbiamo che
            \[
                \sum_{n=1}^{+\infty} \alpha^n \left(1+ \frac{2}{n}\right)^n = \begin{cases}
                    \text{Convergente} & 0 \leq \alpha < 1 \\
                    \text{Divergente} & \alpha \geq 1 
                \end{cases}
            \]
        \end{proof}

        \begin{esercizio}{}{}
            Determinare il carattere della serie al variare $x \in \mathbb{R}$
            \[
            \sum_{n=1}^{+\infty} \frac{x^n}{3^n\sqrt{n+1}}
            \]
        \end{esercizio}

        \begin{proof}
            Visto che $x\in  \mathbb{R}$, abbiamo che per $x < 0$ la serie non è più definitivamente positiva, quindi per poter usare i vari criteri è necessario che studiamo la serie dei valori assoluti: $\displaystyle\sum_{n=1}^{+\infty} \left|\frac{x^n}{3^n\sqrt{n+1}}\right| = \sum_{n=1}^{+\infty} \frac{|x|^n}{3^n\sqrt{n+1}}$. Dato che ora la serie è definitivamente positivi possiamo applicare il criterio della radice
            \[
            \lim_{n\to +\infty} \sqrt[n]{ \frac{|x|^n}{3^n\sqrt{n+1}}} =           \lim_{n\to +\infty} \frac{|x|}{3\sqrt[2n]{n+1}} 
            \]
            Per calcolare il denominatore dobbiamo fare un paio di riarrangiamenti
            \[
            \lim_{n\to+\infty} \sqrt[2n]{n+1} = \lim_{n\to+\infty} e^{log(\sqrt[2n]{n+1})} = \lim_{n\to+\infty} e^{log\left((n+1)^\frac{1}{2n}\right)}  = \lim_{n\to+\infty} e^{\frac{log(n+1)}{2n}}   = e^0 = 1
            \]
            E quindi abbiamo che
            \[
            \lim_{n\to +\infty} \frac{|x|}{3\sqrt[2n]{n+1}}  = \frac{|x|}{3}
            \]
            Quindi per $\frac{|x|}{3} < 1$, e cioè $ -3 < x < 3$, la serie converge assolutamente e quindi anche semplicemente. Per vedere l'altra casistica dobbiamo usare la condizione necessaria
            \[
            \lim_{n\to+\infty} \frac{x^n}{3^n\sqrt{n+1}} = \lim_{n\to+\infty} \frac{\left(\frac{x}{3}\right)^n}{\sqrt{n+1}} \sim \lim_{n\to+\infty} \frac{\left(\frac{x}{3}\right)^n}{\sqrt{n}}
            \]
            Ora dato che $|x| > 3$ (che è la casistica che ci manca), abbiamo che $\frac{x}{3} > 1$ e quindi per gerarchia degli infiniti abbiamo che 
            \[
            \lim_{n\to+\infty} \frac{\left(\frac{x}{3}\right)^n}{\sqrt{n}} = \infty
            \] 
            E quindi abbiamo che la serie per $|x| > 3$ è divergente. Ci mancano i casi $x = \pm3$, iniziamo con $x=3$ e abbiamo Che
            \[
            \sum_{n=1}^{+\infty} \frac{x^n}{3^n\sqrt{n+1}} = \sum_{n=1}^{+\infty} \frac{3^n}{3^n\sqrt{n+1}} = \sum_{n=1}^{+\infty} \frac{1}{\sqrt{n+1}} \sim \sum_{n=1}^{+\infty} \frac{1}{\sqrt{n}} = \sum_{n=1}^{+\infty} \frac{1}{n^{\frac{1}{2}}}
            \]
            Per la serie armonica abbiamo che la serie diverge. Per il caso $x=-3$ è necessario usare un criterio che dobbiamo ancora vedere (quello di Leibniz), e che quindi per ora lo lasciamo stare. Quindi
            \[
            \sum_{n=1}^{+\infty} \frac{x^n}{3^n\sqrt{n+1}}\begin{cases}
                    \text{Convergente} & 0 -3 < x < 3 \\
                    \text{Divergente} & x < -3 \lor x \geq 3
                \end{cases}
            \]
        \end{proof}

\addcontentsline{toc}{subsection}{Relazione tra criterio del Rapporto e della Radice}
        \begin{teorema}{Relazione tra criterio del Rapporto e della Radice}{}
            Sia $(a_n)_{n\in \mathbb{N}}$ una successione positiva, allora se
            \[
            \exists \lim_{n\to+\infty} \frac{a_{n+1}}{a_n} = l\in [0, +\infty) \cup \{+\infty\} \implies \lim_{n\to+\infty} \sqrt[n]{a_n} = l
            \]
        \end{teorema}
        \begin{proof}
            Per risolverlo dobbiamo dividere in due casi: $l \in [0,+\infty)$ e $l = +\infty$. Partiamo con la prima casistica
            
            $(i)$ Per ipotesi sappiamo che esiste il limite del rapposto (e fa un numero finito), e quindi usando la definizione di limite abbiamo che
            \[
                l-\varepsilon < \frac{a_{n+1}}{a_n} < l + \varepsilon \;\;\;\;\; \forall n \geq \bar{n}
            \]
            Dato che questo vale $\forall n \geq \bar{n}$, allora sarà vero anche che $ l-\varepsilon < \frac{a_{\bar{n} +1}}{ a_{\bar{n}}} < l + \varepsilon$, ma anche $l-\varepsilon < \frac{a_{\bar{n}+2}}{a_{\bar{n}+1}} < l + \varepsilon$, sarà valido anche per $\frac{a_{\bar{n}+3}}{a_{\bar{n}+2}}$, $\frac{a_{\bar{n}+4}}{a_{\bar{n}+3}}$ e così via fino a$\frac{a_{n}}{a_{n-1}}$, $\frac{a_{n+1}}{a_n}$. E dato che tutti i termini sono positivi, possiamo dire 
            \[
                l-\varepsilon < \frac{a_{n+1}}{a_n} < l + \varepsilon
            \] 
            \[
                (l-\varepsilon)\frac{a_{n}}{a_{n-1}} < \frac{a_{n+1}}{a_n} \frac{a_{n}}{a_{n-1}} < (l + \varepsilon) \frac{a_{n}}{a_{n-1}}
            \] 
            \[
                (l-\varepsilon)(l-\varepsilon) < (l-\varepsilon)\frac{a_{n}}{a_{n-1}} < \frac{a_{n+1}}{a_{n-1}} < (l + \varepsilon) \frac{a_{n}}{a_{n-1}} < (l + \varepsilon)(l + \varepsilon)
            \] 
            \[
            (l-\varepsilon)^2 <  \frac{a_{n+1}}{a_{n-1}}  < (l + \varepsilon)^2
            \]
            E quindi possiamo continuare con questo ragionamento moltiplicando per tutte le frazioni e abbiamo
            \[
            (l-\varepsilon)^{n-\bar{n}} <  \frac{a_{n+1}}{a_{\bar{n}}}  < (l + \varepsilon)^{n-\bar{n}}
            \]
            \[
            (l-\varepsilon)^{n-\bar{n}} a_{\bar{n}}<  a_{n+1} < (l + \varepsilon)^{n-\bar{n}}a_{\bar{n}}
            \]
            Ora se applichiamo la radice n-esima abbiamo che
             \[
            \sqrt[n]{(l-\varepsilon)^{n-\bar{n}} a_{\bar{n}}}<  \sqrt[n]{a_{n+1}} < \sqrt[n]{(l + \varepsilon)^{n-\bar{n}}a_{\bar{n}}}
            \]
             \[
            (l-\varepsilon)^{1-\frac{\bar{n}}{n}}\sqrt[n]{ a_{\bar{n}}}<  \sqrt[n]{a_{n+1}} < (l + \varepsilon)^{1-\frac{\bar{n}}{n}}\sqrt[n]{ a_{\bar{n}}}
            \]
            Portando tutto al limite abbiamo abbiamo che $n+1 \to n$, $\frac{\bar{n}}{n} \to 0$ e $\sqrt[n]{a_{\bar{n}}}\to 1$ 
            \[
            (l-\varepsilon)<  \sqrt[n]{a_{n}} < (l + \varepsilon)
            \]
            Questo verifica il limite
            \[
                \lim_{n\to+\infty} \sqrt[n]{a_{n}} = l
            \]
            $(ii)$ basta fare lo stesso ragionamento ma il limite di partenza sara $\frac{a_{n+1}}{a_n} > M$, e poi il ragionamento è analogo. 
        \end{proof}

        \textbf{N.B.} quindi se durante un esercizio fai il criterio del rapporto è inutile che provi quello della radice perchè darà lo stesso risultato. Il criterio della radice va applicato dopo quello del rapporto solo se il limite del rapporto non esiste.
\newpage

        \begin{esercizio}{}{}
            Determinare il carattere della serie
            \[
                \sum_{n=1}^{+\infty} 2^{n+(-1)^n}
            \]
        \end{esercizio}


        
        \begin{proof}
            Se proviamo con il criterio del rapporto abbiamo che 
            \[
            \lim_{n\to +\infty}    \frac{2^{n+1+(-1)^{n+1}}}{2^{n+(-1)^n}} = \lim_{n\to +\infty}  2^{n+1+(-1)^{n+1} -n-(-1)^n-} = \lim_{n\to +\infty} 2^{1 +(-1)^{n+1}-(-1)^{n}}
            \]
            Però per il teorema della caratterizzazione sequenziale dei limiti e se scegliamo $a_n = 2n$ e $b_n = 2n+1$ abbiamo che il nostro limite tende a due valori diversi con $a_n$ il limite tende a $\frac{1}{2}$, mentre con $b_n$ tende a $8$, e quindi il limite non esiste.  In questo caso ha senso usare il criterio della radice, e quindi 
        \[
        \lim_{n\to +\infty}  \sqrt[n]{2^{n+(-1)^n}} = \lim_{n\to +\infty}  2^{\frac{n+(-1)^n}{n}} =  \lim_{n\to +\infty}  2^{ 1 + \frac{(-1)^n}{n}} = 2^{1+0} = 2 > 1
        \]
        Quindi la serie diverge.
        \end{proof}


\addcontentsline{toc}{subsection}{Criterio Condensazione di Cauchy}
        \begin{teorema}{Criterio Condensazione di Cauchy}{}
            Sia $(a_n)_{n\in \mathbb{N}}$ tale che
            \begin{itemize}
                \centering
                \item $a_n \geq 0$ $\forall n \in \mathbb{N}$ (non negativa)
                \item $a_{n+1} \leq a_n $ $\forall n \in \mathbb{N}$  ( decrescente)
            \end{itemize}
            Allora le due serie 
            \[
            \begin{array}{c @{\qquad}@{\qquad} c}
                \displaystyle\sum_{n=1}^{+\infty} a_n & \displaystyle\sum_{n=1}^{+\infty} 2^na_{2^n} 
            \end{array}
            \]
            Hanno lo stesso carattere e vale
            \[
            \frac{1}{2}\sum_{n=1}^{+\infty} 2^na_{2^n}  \leq\sum_{n=1}^{+\infty} a_n \leq \sum_{n=1}^{+\infty} 2^na_{2^n} 
            \]
        \end{teorema}
        Questo teorema può essere utilizzato per dimostrare la serie armonica generalizzata, infatti se $a_n = \frac{1}{n^\alpha}$, sappiamo che $a_n \geq 0$ e vale anche $\frac{1}{(n+1)^\alpha} \leq \frac{1}{n^\alpha}$, e quindi possiamo applicare il teorema e avremo
        \[
            \sum_{n=1}^{+\infty} \frac{1}{n^\alpha} \implies \sum_{n=1}^{+\infty} 2^n \cdot \frac{1}{(2^n)^\alpha} =\sum_{n=1}^{+\infty} \frac{2^n}{2^{\alpha n}} =  \sum_{n=1}^{+\infty} (2^{1-\alpha})^n 
        \]
        Che per la serie geometrica abbiamo che
        \[
\sum_{n=1}^{+\infty} (2^{1-\alpha})^n = \begin{cases}
    \text{Convergente } & 2^{1-\alpha} < 1 \\
    \text{Divergente } & 2^{1-\alpha} \geq 1
\end{cases}        = \begin{cases}
    \text{Convergente } & \alpha > 1 \\
    \text{Divergente } & \alpha \leq 1
\end{cases}   
        \]
        Difatti abbiamo ritrovato lo stesso risultato trovato precedentemente.
        \newpage

    \begin{esercizio}{}{}
    Definire il carattere della seguente serie al variare di $\alpha \in \mathbb{R}^+$
    \[
        \sum_{n=3}^{+\infty} \dfrac{1}{n\log_{2}(n)\log^\alpha_{2}(\log_2(n))}
    \]
\end{esercizio}
Applichiamo il criterio di condensazione
\[
    \sum_{n=3}^{+\infty} \dfrac{1}{n\log_{2}(n)\log^\alpha_{2}(\log_2(n))} \implies \sum_{n=3}^{+\infty} \dfrac{2^n}{2^n\log_{2}(2^n)\log^\alpha_{2}(\log_2(2^n))} = \sum_{n=3}^{+\infty} \dfrac{1}{n\log^\alpha_{2}(n)}
    \]
Ora possiamo riapplicare il criterio
\[
\sum_{n=3}^{+\infty} \dfrac{1}{n\log^\alpha_{2}(n)} \implies \sum_{n=3}^{+\infty} \dfrac{2^n}{2^n\log^\alpha_{2}(2^n)} = \sum_{n=3}^{+\infty} \dfrac{1}{n^\alpha} \implies \begin{cases}
    \text{Convergente} & \alpha > 1 \\
    \text{Divergente} & \alpha \leq 1 
\end{cases}
\]
\begin{esercizio}{}{}
    Definire il carattere della seguente serie al variare di $p,q \in \mathbb{R}^+$
    \[
        \sum_{n=2}^{+\infty} \dfrac{1}{n^p\log^q_{2}(n)}
    \]
\end{esercizio}
Applichiamo il criterio di condensazione
\[
\sum_{n=2}^{+\infty} \dfrac{1}{n^p\log^q_{2}(n)} \implies \sum_{n=2}^{+\infty} \dfrac{2^n}{(2^n)^p\log^q_{2}(2^n)} = \sum_{n=2}^{+\infty} \dfrac{1}{2^{(p-1)n}(n\log_{2}(2))^q} = \sum_{n=2}^{+\infty} \dfrac{1}{2^{(p-1)n}n^q}
\]
A questa nuova serie applichiamo il teorema del rapporto 
\[
\lim_{n\to+\infty} \dfrac{2^{(p-1)n}n^q}{2^{(p-1)(n+1)}(n+1)^q} = \lim_{n\to+\infty} \dfrac{1}{2^{p-1}} \cdot \dfrac{n^q}{(n+1)^q} = \dfrac{1}{2^{p-1}} 
\]
Quindi se 
\[
\dfrac{1}{2^{p-1}} > 1 \implies 2^{p-1} < 1 \implies p-1 < 0  \implies p< 1 \implies \text{ Divergente}
\]
\[
\dfrac{1}{2^{p-1}} < 1 \implies 2^{p-1} > 1 \implies p-1 > 0  \implies p> 1 \implies \text{ Convergente}
\]
Ci manca da studiare il caso $p=1$, quindi possiamo riapplicare il criterio di condensazione
\[
\sum_{n=2}^{+\infty} \dfrac{1}{n\log^q_{2}(n)} \implies \sum_{n=2}^{+\infty} \dfrac{2^n}{2^n\log^q_{2}(2^n)} = \sum_{n=2}^{+\infty} \dfrac{1}{n^q} \implies \begin{cases}
    \text{Convergente} & q > 1 \\
    \text{Divergente} & q \leq 1 
\end{cases}
\]


\newpage
\addcontentsline{toc}{subsection}{Definizione Serie a Segno Alterno}
        \begin{definizione}{Serie a Segno Alterno}{}
            Una serie a \textbf{segno alterno} è detta una qualsiasi serie esprimibile come
            \[
                \sum_{n=1}^{+\infty} (-1)^n a_n
            \]
            Con  $(a_n)_{n\in \mathbb{N}}$ una qualsiasi successione definitivamente positiva
        \end{definizione}

        Con queste tipologie di serie non possiamo applicare nessuno dei teoremi visti prima, perchè la serie non è mai a definitivamente positiva (per via del $(-1)^n$). L'unico che potremmo applicare sarebbe la condizione necessaria o il criterio del valore assoluto. Quindi le serie a segno alterno esiste un solo criterio usabile: quello di Leiniz.

\addcontentsline{toc}{subsection}{Criterio di Leibniz}
        \begin{teorema}{Criterio di Leibniz}{}
            Sia $(a_n)_{n\in \mathbb{N}}$ una successione, se
            \begin{itemize}
                \centering
                \item $a_n \geq 0$ $\forall n \in \mathbb{N}$ (non negativa)
                \item $a_{n+1} \leq a_n $ $\forall n \in \mathbb{N}$  (decrescente)
                \item $\displaystyle\lim_{n\to+\infty} a_n = 0$ (infinitesima) 
            \end{itemize}
            Allora la serie $\displaystyle \sum_{n=1}^{+\infty} (-1)^n a_n $
            è convergente.
        \end{teorema}

            Studiamo il caso
            \[
            \sum_{n=1}^{+\infty} \frac{(-1)^n}{n^\alpha}
            \]
        Ora per il criterio con il valore assoluto sappiamo Che
        \[
            \sum_{n=1}^{+\infty} \left|\frac{(-1)^n}{n^\alpha}\right| = \sum_{n=1}^{+\infty} \frac{1}{n^\alpha}
        \]
        Questa sappiamo che converge per $\alpha >1$, e quindi la nostra serie originale converge anche semplicemente, ma per $\alpha \in (0, 1]$, non sappiamo nulla, quindi usiamo il criterio di Leibniz. Notiamo che $a_n = \frac{1}{n^\alpha}$ e che 
        \[
        \begin{array}{c  @{\qquad}@{\qquad} c @{\qquad}@{\qquad} c}
            \displaystyle\frac{1}{n^\alpha} \geq 0\;\;\;\;\forall n \in \mathbb{N} & \displaystyle\frac{1}{(n+1)^\alpha} \leq \frac{1}{n^\alpha}\;\;\;\; \forall n \in \mathbb{N}  & \displaystyle\lim_{n\to+\infty}\frac{1}{n^\alpha}= 0
        \end{array}
    \]  
    Quindi la serie a segni alterni converge, e quindi per $\alpha \in (0, 1]$ abbiamo che la serie 
    \[
    \sum_{n=1}^{+\infty} \frac{(-1)^n}{n^\alpha}
    \]
    Converge semplicemente, ma diverge assolutamente. Mentre per $\alpha > 1$ converge sia semplicemente che assolutamente.
       
    
    
    
    
    
   \section{Continuità delle Funzioni} 

\addcontentsline{toc}{subsection}{Definizione di Funzione Continua}
    \begin{definizione}{Definizione di Funzione Continua}{}
        Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$ diciamo che \textbf{$f$ è continua in $x_0$} se vale una delle due proposizioni: 
        \begin{itemize}
            \item $x_0$ è un punto isolato in $A$
            \item $x_0$ è un punto di accumulazione in $A$ e vale
            \[
                \lim_{x\to x_0} f(x) = f(x_0)
            \] 
        \end{itemize}
        Se $f$ è continua $\forall x \in A$, diciamo che è \textbf{continua nel suo dominio} e lo indichiamo con il simbolo $f \in C^0(A)$, e lo definiamo come
        \[
            C^0(A) = \{f:A \to \mathbb{R} \text{ }| \text{ } f \text{ è continua in } A\}
        \] 
    \end{definizione}
    
    Usando questa definizione possiamo dimostrare che alcune funzioni sono continue, infatti
    \begin{itemize}
        \item $f(x) = c\in \mathbb{R}$, è continua in $\mathbb{R}$ infatti, come abbiamo già dimostrato 
        \[
            \lim_{x\to x_0} f(x) = \lim_{x\to x_0} c = c = f(x_0) \;\;\;\;\; \forall x_0 \in \mathbb{R}
        \] 
        \item $f(x) = \sum_{i} a_i x^i$, ogni polinomio è continuo, come abbiamo già dimostrato
        \item $\displaystyle f(x)=\frac{P(x)}{Q(x)}$, con $P, Q$ polinomi, abbiamo che è continua in $A = \{x : Q(x) \ne 0\}$
        \item $f(x)=a^x$ con $a > 0$
        \item $f(x) = \sin(x)$
        \item $f(x) = \cos(x)$
    \end{itemize}
    
    \begin{center}
    \begin{tikzpicture} 
         \begin{axis}[
        xmin=-2, xmax=3,
        ymin=-2, ymax=3,
        axis equal,
        axis lines=middle,
        enlargelimits=false,
        clip=false,
        axis line style={-stealth, thick},
        width=8cm
    ]
            \addplot[black, ultra thick, samples=50, domain=-2:2]({x}, {x}); 

            \addplot[
            only marks,
            mark=*,
            mark size=2pt,
            black
        ] coordinates {(3,1)};

        \node[black] at (axis cs:2,2.5) {$y=f(x)$};
        \end{axis} 
    \end{tikzpicture}
    \end{center}

    Notiamo che il dominio di questa funzione è $\mathbb{D}(f) = [-2, 2] \cup \{3\}$, e la funzione è continua nel suo dominio, questo perche $\forall x \in [-2,2]$ la vale la seconda proposizione di funzione continua, mentre per il punto $x=3$ dato che è un punto isolato è continuo per definizione.
    \newpage
    Proviamo a vedere se è continua la funzione parte intera: $f(x)= \lfloor x \rfloor$, il cui grafico è 

    
   

\begin{center}
\begin{tikzpicture} 
    \begin{axis}[
        xmin=-4, xmax=4,
        ymin=-4, ymax=4,
        axis equal,
        axis lines=middle,
        enlargelimits=false,
        clip=false,
        axis line style={-stealth, thick},
        width=10cm
    ]
        % Draw horizontal steps manually
        \foreach \x in {-4,-3,...,3} {
            % Horizontal segment
            \addplot[black, ultra thick] coordinates {(\x,\x) (\x+1,\x)};
            % Closed circle at start
            \addplot[only marks, mark=*, mark size=2pt] coordinates {(\x,\x)};
            % Open circle at end
            \addplot[only marks, mark=o, mark size=2pt] coordinates {(\x+1,\x)};
        }
        
        \node[black] at (axis cs:2,2.5) {$f(x)= \lfloor x \rfloor$};
    \end{axis} 
\end{tikzpicture}
\end{center}

Notiamo che per $x_0 \in \mathbb{Z}$ abbiamo che
\[
\lim_{x\to x_0^+} \lfloor x \rfloor = x_0 
\]
\[
\lim_{x\to x_0^-} \lfloor x \rfloor = x_0 -1
\]

Ma Dato che il limite destro e sinistro sono  diversi e quindi il limite in quel punto non esiste, e di conseguenza non è continua, visto che la continuità richiede che si possa calcolare il limite nel punto. 

Mentre per $x_0 \in\mathbb{R} \setminus \mathbb{Z}$, usando la definizione di limite e prendendo un valore di $\varepsilon < \min(x_0-\lfloor x_0\rfloor, \lceil x_0 \rceil - x_0 )$  la funzione $f(x) = \lfloor x \rfloor$ è costante $\forall x \in (x_0 - \varepsilon, x_0 +\varepsilon)$ è constate e, quindi esiste il limite in quel punto e fa proprio
\[
    \lim_{x\to x_0} \lfloor x \rfloor = \lfloor x_0 \rfloor
\]
E quindi per quei valori la funzione è continua. Di conseguenza possiamo scrivere che $\lfloor x \rfloor \in C^0(\mathbb{R} \setminus \mathbb{Z})$.
  
Un altro caso molto particolare è la funzione di Dirichlet, che è definita come
\[
f_D(x) = \begin{cases}
    1 & x \in \mathbb{R} \setminus \mathbb{Q} \\
    0 & x \in \mathbb{Q}
\end{cases}
\]
Notiamo che se $x_0 \in \mathbb{R} \setminus \mathbb{Q}$, per caratterizzazione sequenziale possiamo scegliere  $(a_n)_(n \in \mathbb{n})$ tale che $a_n \in \mathbb{Q}$ e $a_n \to x_0$, e questo possiamo farlo per la densità di $\mathbb{Q}$. In quei casi $\lim_{n\to +\infty} f_D(a_n) = 0$ perchè $a_n \in \mathbb{Q}$ e quindi la funzione restituisce 0, ma è diverso da $f_D(x_0) = 1$, visto che $x_0$ è irrazionale. Quindi visto che limite e valore effettivo sono diversi la funzione non è continua. Mentre per i casi $x_0 \in \mathbb{Q}$, possiamo usare sempre la caratterizzazione sequenziale con $a_n = x_0  + \frac{1}{n} \sqrt(2)$, e questa successione $a_n \in \mathbb{R} \setminus \mathbb{Q}$ dato che c'è $\sqrt{2}$, ma $a_n \to x_0$, quindi il limite $f(a_n) \to 1$ visto che $a_n\in \mathbb{R} \setminus \mathbb{Q}$ ma il valore della funzione è $f(x_0) = 0$, e quindi la funzione non è continua. In sostanza la funzione è discontinua $\forall x \in \mathbb{R}$, come era prevedibile.

\newpage

\addcontentsline{toc}{subsection}{Caratterizzazione $\varepsilon - \delta$ della Funzione Continua }
\begin{teorema}{Caratterizzazione $\varepsilon - \delta$ della Funzione Continua }{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$, se $f$ è continua in $x_0$ allora
    \[
        \forall \varepsilon > 0 \;\; \exists \delta > 0 : f(x) \in (f(x_0) - \varepsilon , f(x_0) + \varepsilon ) \;\; \forall x \in (x_0 - \delta, x_0 + \delta)
    \]
\end{teorema}
Questo viene della definizione di limite.

\addcontentsline{toc}{subsection}{Algebra delle Funzioni Continue }
\begin{teorema}{Algebra delle Funzioni Confinue}{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f,g: A \to \mathbb{R}$ e $x_0 \in A$, se $f, g$  sono continue in $x_0$ allora
    \begin{itemize}
        \item $(f\pm g)(x)$ è continua in $x_0$
        \item $(f\cdot g)(x)$ è continua in $x_0$
        \item $(\frac{f}{g})(x)$ è continua in $x_0$ se $g(x_0) \neq 0$
    \end{itemize}
\end{teorema}

Per dimostrare queste proposizioni è necessario usare l'algebra dei limiti finiti. 

\textbf{N.B.} se le funzioni sono continue perchè $x_0$ è isolato, funziona lo stesso il teorema, perchè vuol dire che anche le funzioni $(f\pm g)$, $(f\cdot g)$ e $(\frac{f}{g})$ sono isolate nel punto $x_0$, e quindi è continua per definizione.
Con questo teorema possiamo dimostrare che le funzioni $\sinh(x)$,$\cosh(x)$ e $\tan(x)$ sono continue.

\addcontentsline{toc}{subsection}{Limitatezza delle Funzioni Continue }
\begin{teorema}{Limitatezza delle funzioni continue}{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$, se $f$ è continua in $x_0$ allora è definitivamente limitata per $x\to x_0$
\end{teorema}

\begin{proof}
    Se $x_0$ è punto isolato, allora perforza la funzione è limitata visto che $\forall\varepsilon >0$ abbiamo che $|f(x)| <  |f(x_0)| + \varepsilon$, quindi non abbiamo nulla da dimostrare.
    
    Se $x_0$ è un punto di accumulazione, possiamo usare la definizione di limite e abbiamo che
    \[
    \lim_{x\to x_0} f(x) = f(x_0) \iff |f(x) - f(x_0)| < \varepsilon \;\;\; \forall x \in A \cap I 
    \]
    Ma se guardiamo il termine 
    \begin{align*}
        |f(x)| &= |f(x) - f(x_0) + f(x_0)|  \\
        &\leq |f(x) - f(x_0)| + |f(x_0)|  \\
        &\leq  \varepsilon + |f(x_0)|
    \end{align*}
    Quindi notiamo che $f(x)$ è sempre limitata.
\end{proof}

\addcontentsline{toc}{subsection}{Permanenza del Segno delle Funzioni Continue }
\begin{teorema}{Permanenza del Segno di Funzioni Continue}{}
     Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$, se $f$ è continua in $x_0$ allora 
    \begin{itemize}
        \item $f(x_0) > 0$, allora $f(x) > 0$ definitivamente per $x\to x_0$
        \item $f(x_0) < 0$, allora $f(x) < 0$ definitivamente per $x\to x_0$
    \end{itemize}
\end{teorema}
Per dimostrarlo è sufficiente fare il ragionamento analogo al teorema della permanenza del segno 
\newpage

\addcontentsline{toc}{subsection}{Continuità delle Funzioni Composte }
\begin{teorema}{Continuità di Funzioni Composte }{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f,g$ funzioni tali che $g \circ f: A \to \mathbb{R}$ sia ben definita e sono tali
    \begin{itemize}
        \item $f$ è continua in $x_0 \in \mathbb{D}(f) \cap A$
        \item $g$ è continua in $f(x_0) \in \mathbb{D}(g)$
    \end{itemize}
    Allora anche $(g \circ f)(x)$ è continua in $x_0 \in A$.
\end{teorema}
Per dimostrarlo è sufficiente usare il teorema del cambio di variabile nella definizione di continuità.

\addcontentsline{toc}{subsection}{Caratterizzazione Sequenziale di funzioni Continue}
\begin{teorema}{Caratterizzazione Sequenziale di funzioni Continue}{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$, se $f$  è continua in $x_0$ se e solo se  $\forall (a_n)_{n\in \mathbb{N}} \subseteq A$ tale che $\displaystyle \lim_{n\to +\infty} a_n = x_0$ vale $\displaystyle \lim_{n\to +\infty} f(a_n) = f(x_0)$.
\end{teorema}
Per dimostrarlo è sufficiente usare la caratterizzazione sequenziale del limite nella definizione di continuità.


\addcontentsline{toc}{subsection}{Definizione di Prolungamento per Continuità}
\begin{definizione}{Prolungamento per Continuità}{}
     Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 $ punto di accumulazione in $A$, $\displaystyle\exists \lim_{x\to x_0} f(x) = l$, se
     \[
     \begin{array}{c @{\qquad} c @{\qquad} c} 
        l \ne f(x_0) & \text{oppure} & x_0 \not \in A
     \end{array}
     \]
     Allora possiamo definire una nuova funzione $\tilde{f}: A \cup \{x_0\} \to \mathbb{R}$ che chiamiamo \textbf{prolungamento di $f$ in $x_0$} e la definiamo come
     \[
     \tilde{f}(x) = \begin{cases}
        f(x) & x \in A \setminus \{x_0\}\\
        l & x = x_0
     \end{cases}
     \]
\end{definizione}
Vediamo un esempio pratico.

\begin{esempio}{}{}
    \[
        f(x) = \frac{\log(1+x)}{x}
    \]
\end{esempio}
Notiamo che il dominio di quesa funzione è $\mathbb{D}(f) = x > -1 \land x \neq 0$, quindi possiamo vedere se possiamo prolungare in $x=0$, e notiamo che 
\[
    \lim_{x\to 0}\frac{\log(1+x)}{x} = 1
\]
Quindi visto che esiste il limite e $0 \not \in \mathbb{D}(f)$, e quindi possiamo prolungare la funzione:
\[
    \tilde{f}(x) =\begin{cases}
        \frac{\log(1+x)}{x} & x \in  (-1, 0) \cup (0, +\infty)\\
        1 & x = 0
     \end{cases}
\]
In questo modo abbiamo reso continua una funzione che prima era discontinua.

\newpage

\textbf{N.B.} con la definzione che abbiamo dato di prolungamento, potremmo estendere anche per $\pm \infty$, infatti se la funzione ha limite ad infinito e sicuramente $\pm \infty \not \in \mathbb{R}$, quindi potremmo fare anche estensioni del tipo   
\[
\lim_{x\to\pm\infty} \arctan(x) = \pm\frac{\pi}{2}
\]
Di conseguenza possiamo scrivere che 
\[
     \tilde{f}(x) = \begin{cases}
        \arctan(x) & x \in \mathbb{R} \\
        \frac{\pi}{2} & x = +\infty \\
        -\frac{\pi}{2} & x = -\infty 
     \end{cases}
\]
Pertanto possiamo calcolare la funzione nei punti $\pm \infty$, e possiamo scrivere $\tilde{f}(+\infty) = \frac{\pi}{2}$, visto che ora $+\infty$ è un punto del dominio. Questo può essere comodo per dimostrare certi problemi, però è meno intuitivo. Quindi per non rendere le definizioni ambigue, definiamo \textbf{prolungamento di $f$ nei numeri reali} il  prolungamento dove $x\neq \pm \infty$.D'ora in poi, quando scriverò prolungamento di una funzione, darò per scontato che sia un prolungamento nei reali. 


\addcontentsline{toc}{subsection}{Classificazione Punti di Discontinità}
\begin{definizione}{Punti di Discontinità}{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 $ punto di accumulazione in $A$, e diciamo che \textbf{$f$ presenta una discontinità} nel punto $x_0$, se $f$ non è continua in $x_0$. Definiamo 3 tipologie di discontinità:
    \begin{itemize}
        \item definiamo discontinità \textbf{ eliminabile} un punto $x_0$ se possiamo fare un prolungamento di $f$ in $x_0$. 
        \item definiamo discontinità \textbf{I specie} se 
        \[
        \begin{array}{c @{\qquad} c}
            \displaystyle\exists \lim_{x\to x_0^-} f(x) =l_{sx} \in \mathbb{R}& \displaystyle\exists \lim_{x\to x_0^+} f(x) =l_{dx}\in \mathbb{R}
        \end{array}
        \] 
        Ma $l_{sx} \ne l_{dx}$. Questa discontinità è detta anche \textbf{Salto}, infatti possiamo definire il salto come $S = l_{dx} - l_{sx}$.
        \item definiamo discontinità \textbf{II specie} tutte le altre casistiche che non rientrano nella discontinità eliminabile o di I specie. 
    \end{itemize}
\end{definizione}

Vediamo qualche esempio
\begin{esempio}{}{}
    Determinare le discontinuità di 
    \[
        f(x) = \frac{x^2-3x+2}{x-2}
    \]
\end{esempio}

In primis notiamo che abbiamo un denominatore, quindi necessitiamo che il denominatore sia diverso da zero: $x-2 \ne 0 \implies x \neq 2$, quindi $\mathbb{D}(f) = \mathbb{R} \setminus \{2\}$. Di conseguenza nel punto $x=2$ la funzione non sarà continua. Capiamo che specie di discontinità è
\[
\lim_{x\to 2}\frac{x^2-3x+2}{x-2} = \lim_{x\to 2}\frac{(x-2)(x-1)}{x-2} =  \lim_{x\to 2} (x-1) = 1
\] 
Quindi notiamo che ricadiamo nella discontinità eliminabile, visto che esiste il limite nel punto. E quindi è anche estendibile:
\[
    \tilde{f}(x) = \begin{cases}
        \frac{x^2-3x+2}{x-2} & x \neq 2 \\
        1 & x=2
    \end{cases}
\]
Guardiamo il grafico delle 2 funzioni

\[
\begin{array}{c @{\qquad}@{\qquad} c}

% --- Primo grafico: f(x) con buco ---
% --- Primo grafico: f(x) con buco (pallino vuoto) ---
\begin{tikzpicture}
\begin{axis}[
    xmin=-1, xmax=4,
    ymin=-3, ymax=3,
    axis equal,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=7cm
]
    % Linea x-1 escluso x=2
    \addplot[blue, ultra thick, domain=-1:1.999] ({x},{x-1});
    \addplot[blue, ultra thick, domain=2.001:4] ({x},{x-1});

    % Pallino davvero vuoto in x=2
    % Cerchio bianco sotto
    \addplot[only marks, mark=*, mark size=3pt, white] coordinates {(2,1)};
    % Cerchio blu sopra
    \addplot[only marks, mark=o, mark size=3pt, blue] coordinates {(2,1)};

    \node[blue] at (axis cs:2.3,2.2) {$f(x)$};
\end{axis}
\end{tikzpicture}
&
% --- Secondo grafico: prolungamento continuo ---
\begin{tikzpicture}
\begin{axis}[
    xmin=-1, xmax=4,
    ymin=-3, ymax=3,
    axis equal,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=7cm
]
    % Linea x-1
    \addplot[red, ultra thick, domain=-1:4] ({x},{x-1});

    % Punto chiuso in x=2
    \addplot[only marks, mark=*, mark size=3pt, red] coordinates {(2,1)};

    \node[red] at (axis cs:2.3,2.2) {$\tilde f(x)$};
\end{axis}
\end{tikzpicture}

\end{array}
\]
\begin{esempio}{}{}
     Determinare le discontinuità di 
     \[
        f(x) = \arctan\left(\frac{1}{x}\right)
     \]
\end{esempio}
Anche qua come prima dobbiamo stare attenti al denominatore: $x\ne 0$, e quindi dobbiamo controllare solamente il punto $x=0$
\[
\lim_{x\to 0}  \arctan\left(\frac{1}{x}\right)
\]
Però ricordiamo che $\lim_{x\to 0}\frac{1}{x}$ perchè dobbiamo dividere nel limite destro e limite sinistro
\[
\begin{array}{c @{\qquad} c}
    \displaystyle\lim_{x\to 0^+}  \arctan\left(\frac{1}{x}\right) = \frac{\pi}{2} & \displaystyle\lim_{x\to 0^-}  \arctan\left(\frac{1}{x}\right)= -\frac{\pi}{2}
\end{array}
\]
Quindi visto che esistono il limite destro e sinitro ma sono valori finiti ma diversi, ricadiamo nella discontinità di I specie, e possiamo calcolare il salto $S = \frac{\pi}{2} - \left(-\frac{\pi}{2}\right) = \pi$. Vediamo il grafico
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-4, xmax=4,
    ymin=-2, ymax=2,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=8cm,
    samples=400
]

    % funzione corretta: atan() in deg → convertire in rad
    \addplot[blue, ultra thick, domain=-4:-0.05]
        (x,{atan(1/x)*pi/180});
    \addplot[blue, ultra thick, domain=0.05:4]
        (x,{atan(1/x)*pi/180});

   % --- Palline vuote negli asintoti ---
    % Punto bianco sotto
    \addplot[only marks, mark=*, mark size=3pt, white]
        coordinates {(0,pi/2) (0,-pi/2)};
    % Cerchio esterno blu sopra
    \addplot[only marks, mark=o, mark size=3pt, blue]
        coordinates {(0,pi/2) (0,-pi/2)};
\begin{scope}
        \draw[
            decorate,
            decoration={brace, amplitude=8pt},
            thick,
            red
        ]
            (axis cs:0,pi/2) -- (axis cs:0,-pi/2)
            node[midway, right=5pt, yshift=-6pt, red] {$S=\pi$};
    \end{scope}
    % etichette
    \node[blue] at (axis cs:2,1.3) {$f(x)$};

\end{axis}
\end{tikzpicture}
\end{center}


\addcontentsline{toc}{subsection}{Teormea di Weierstrass}
\begin{teorema}{Teorema di Weierstrass}{}
    Siano $a,b \in \mathbb{R}$ con $a < b$, $f: [a, b] \to \mathbb{R}$, e $f \in C^0([a,b])$, allora $f$ ammette un massimo e un minimo in $[a, b]$. Cioè $\exists x_{min}, x_{max} \in [a, b]$ tali che 
    \[
    \begin{array}{c @{\qquad}@{\qquad} c}
        f(x_{min}) \leq f(x) \;\;\; \forall x \in [a,b] &f(x_{max}) \geq f(x) \;\;\; \forall x \in [a,b]
    \end{array}
    \]
\end{teorema}

\begin{proof}
    Dimostramo per il caso del massimo, il minimo è analogo.
    Dato che $f \in C^0([a,b])$ allora sappiamo che almeno esiste un estremo superiore
    \[
    M = \sup\{f(x) : x \in [a,b]\}
    \]
    E supponiamo che $M \in \mathbb{R}$, con la caratterizzazione dell'estremo superiore, sappiamo che $\forall \varepsilon > 0 $, $\exists x_\varepsilon \in [a, b]$ tale che
    \[
    f(x_\varepsilon) > M -\varepsilon \;\;\;\;\; 
    \]
    Di conseguenza possiamo scegliere $\varepsilon = \frac{1}{n}$, con $n \in \mathbb{N}$,e quindi
    \[
    f(x_n) > M -\frac{1}{n} \;\;\;\;\; \forall n \geq 1
    \]
    Ma visto che sappiamo che per ogni $n$ possiamo trovare un $x_n$ che soddisfa quella relazione, possiamo definire una successione $(x_n)_{n\in \mathbb{N}}$, che sappiamo che è limitato da
    \[  
        a \leq x_n \leq b\;\;\;\;\; \forall n \geq 1
    \]  
    Ora però per il teorema di Bolzano-Weierstrass, dato che la successione $(x_n)_{n\in \mathbb{N}}$ è limitata, sappiamo che esiste almeno una sottosuccessione, $(x_{\varphi(n)})_{n\in \mathbb{N}} \to l \in [a,b]$, per un qualsiasi valore di $l$. Ma dato che la nostra funzione è continua $\forall x \in [a,b]$, e dato che $l\in [a,b]$, allora 
    \begin{equation}\label{eq:weie1}
   \mathunderline{red}{ \lim_{n\to+\infty} f(x_{\varphi(n)}) = f(l)}
    \end{equation}
    Riusiamo la caratterizzazione degli estremi, e la definizione di estremo superiore e sappiamo che 
    \[
    M-\frac{1}{n} < f(x_n) < M \;\;\;\;\; \forall n \geq 1
    \] 
    Portando tutto al limite abbiamo che
    \[
    \lim_{n\to+\infty}M-\frac{1}{n} <\lim_{n\to+\infty} f(x_n) <\lim_{n\to+\infty} M
    \]
    \[
    M <\lim_{n\to+\infty} f(x_n) < M \implies \lim_{n\to+\infty} f(x_n) = M
    \]
    Ma dato che la serie converge ad $M$ con la successione  $(x_n)_{n\in \mathbb{N}}$, allora questo varrà per ogni sottosuccessione  $(x_{\varphi(n)})_{n\in \mathbb{N}}$, e quindi
    \begin{equation}\label{eq:weie2}
        \lim_{n\to+\infty} f(x_n) = M \implies \mathunderline{blue}{\lim_{n\to+\infty} f(x_{\varphi(n)}) = M}
    \end{equation}
    Ma combinando le informazioni (\ref{eq:weie1}) e(\ref{eq:weie2}) sappiamo che esiste almeno una sottosuccessione tale che
    \[
   \mathunderline{red}{f(l)=  \lim_{n\to+\infty}} \mathunderline{blue}{f(x_{\varphi(n)}) = M } \implies f(l) = M
    \] 
    Quindi abbiamo trovato un valore $l \in [a, b]$ tale che $f(l) = M$, ma $M$ è l'estremo superiore, di conseguenza $M$ è anche un massimo.
    \newpage
    Se invece $M = +\infty$, dobbiamo ripetere tutto il ragionamento, e quindi sappiamo che 
    \[
    \forall n \in \mathbb{N} \;\;\; \exists x_n \in [a, b] \;\;: \;\; f(x_n) > n
    \]
    Ma allora $x_n$ è limitata, e di conseguenza esiste almeno una sottosuccessione  $(x_{\varphi(n)})_{n\in \mathbb{N}} \to l \in [a,b]$, ma dato che $f$ è continua abbiamo che
    \begin{equation}\label{eq:weie3}
        \lim_{n\to+\infty} f(x_{\varphi(n)}) = f(l)
    \end{equation}
    Riutilizzando la caratterizzazione degli estremi sappiamo che
    \[
    f(x_n) > n
    \]
    ma portando al limite abbiamo che 
    \begin{equation}\label{eq:weie4}
    \lim_{n\to+\infty}  f(x_n) > \lim_{n\to+\infty}  n \implies \lim_{n\to+\infty}  f(x_n) = +\infty
    \end{equation}
    Ma questo varrà anche per ogni sottosuccessione $(x_{\varphi(n)})_{n\in \mathbb{N}}$, ma quindi campinando le informazioni (\ref{eq:weie3}) e (\ref{eq:weie3}) sappiamo che 
    \[
    \begin{aligned}
    \lim_{n\to+\infty}  f(x_{\varphi(n)}) = +\infty \\ 
    \lim_{n\to+\infty} f(x_{\varphi(n)}) = f(l)
    \end{aligned}
    \;\Rightarrow\;
    f(l) = +\infty 
    \]
    Ma così abbiamo scoperto che un punto $l \in [a,b]$ vale $+\infty$, ma ciò è impossibile visto che $f$ è continua in $[a,b]$, e quindi deve valutare solo valori finiti, quindi per assurdo, scopriamo che $M \neq +\infty$. 
\end{proof}

\addcontentsline{toc}{subsection}{Teormea di Bolzano}
\begin{teorema}{Teorema di Bolzano (o degli Zeri)}{}
     Siano $a,b \in \mathbb{R}$ con $a < b$, $f: [a, b] \to \mathbb{R}$, e $f \in C^0([a,b])$ e se $f(a) \cdot f(b) < 0$ allora 
     \[
        \exists c \in (a,b) \; : f(c) = 0 
     \]
\end{teorema}
\begin{proof}
    Dato che $f(a) \cdot f(b) < 0$, allora possono accadere due casistiche: $f(a) < 0 < f(b)$ oppure $f(b) < 0 < f(a)$, per comodità supponiamo che $f(a) < 0 < f(b)$, nell'altro caso è sufficiente porre $g(x) = -f(x)$, in questo modo $g(a) \cdot g(b) < 0$ e  anche che $g(a) < 0 < g(b)$, e così con la dimostrazione che segue abbiamo dimostrato entrambe le casistiche. 

    Definiamo \[
    \begin{array}{c @{\qquad}@{\qquad} c @{\qquad}@{\qquad} c }
        \displaystyle a_0 = a & \displaystyle c_0 =\frac{a_0+b_0}{2} & \displaystyle b_0 = b
    \end{array}\]
    E seguiamo il seguente algoritmo:
    \begin{itemize}
        \item Se $f(c_0) = 0$ abbiamo dimostrato il teorema, dato che $c_0 \in (a,b)$
        \item Se $f(c_0) < 0$, allora poniamo $a_1 = c_0$ e $b_1 = b_0$, in modo tale che $f(a_1) \cdot f(b_1) < 0$
        \item Se $f(c_0) > 0$, allora poniamo $a_1 = a_0$ e $b_1 = c_0$, in modo tale che $f(a_1) \cdot f(b_1) < 0$
    \end{itemize}

    \newpage

    Reiteriamo il procedimento, quindi definiamo 
    \[
    c_1 = \frac{a_1 + b_1}{2}
    \]
    E seguiamo l' algoritmo:
    \begin{itemize}
        \item Se $f(c_1) = 0$ abbiamo dimostrato il teorema, dato che $c_1 \in (a,b)$
        \item Se $f(c_1) < 0$, allora poniamo $a_2 = c_1$ e $b_2 = b_1$, in modo tale che $f(a_2) \cdot f(b_2) < 0$
        \item Se $f(c_1) > 0$, allora poniamo $a_2 = a_1$ e $b_2 = c_1$, in modo tale che $f(a_2) \cdot f(b_2) < 0$
    \end{itemize}
    All'n-esimo passo, qualora non fosse mai stata verificata la prima condizione dell'algoritmo, avremo che
    \[
    c_n = \frac{a_n + b_n}{2}
    \]
    \begin{itemize}
        \item Se $f(c_n) = 0$ abbiamo dimostrato il teorema, dato che $c_n \in (a,b)$
        \item Se $f(c_n) < 0$, allora poniamo $a_{n+1} = c_n$ e $b_{n+1} = b_n$
        \item Se $f(c_n) > 0$, allora poniamo $a_{n+1} = a_n$ e $b_{n+1} = c_n$
    \end{itemize}
    Con questo, possiamo definire le due successioni $(a_n)_{n\in \mathbb{N}}$ e $(b_n)_{n\in \mathbb{N}}$, e sappiamo che
    \begin{enumerate}[label=(\roman*)]
        \item $f(a_n) \le 0 $ $\forall n \in \mathbb{N}$
        \item $f(b_n) \ge 0 $ $\forall n \in \mathbb{N}$
    \end{enumerate}
    \begin{itemize}
        \item $(a_n)_{n\in \mathbb{N}}$ è crescente, dato che ad ogni passaggio o resta uguale al passaggio precedente oppure diventa $\frac{a_n + b_n}{2}$, che dato che $a_n < b_n$ sappiamo che 
        \[
           a_{n+1} = \frac{a_n + b_n}{2} \geq \frac{a_n + a_n}{2} = \frac{2a_n }{2} = a_n
        \]
        \item $(b_n)_{n\in \mathbb{N}}$ è decrescente, per lo stesso ragionamento di $(a_n)_{n\in \mathbb{N}}$
    \end{itemize}
    Dato che entrambe le successioni sono monotone e limitate sappiamo che convegono a dei valori, che chiamiamo $a_n \to A \in (a, b)$ e $b_n \to B \in (a, b)$. In più, dato che ad ogni passaggio prendiamo la metà degli estremi, sappiamo che 
    \[
    b_n - a_n = \frac{b-a}{2^n} \implies b_n = a_n +\frac{b-a}{2^n}
    \]
    Se portiamo al limite questa informazione scopriamo che
    \[
    \lim_{n\to+\infty} b_n = \lim_{n\to+\infty} \left(a_n +\frac{b-a}{2^n}\right) \implies B = A + 0 \implies B = A
    \]
    Se prendo $c = A = B$,  usando la continuità di $f$ e $(i)$, $(ii)$ 
    \[
        f(c) = f(A) = \lim_{n\to +\infty} f(a_n) \leq 0
    \]
    \[
        f(c) = f(B) = \lim_{n\to +\infty} f(b_n) \geq 0
    \]
    Da cui
    \[
    0 \leq f(c) \leq 0 \implies f(c) = 0
    \]
\end{proof}

\newpage
\textbf{N.B.} Il teorema ci dice che esiste almeno uno zero, ciò vuol dire che ne possono esistere molteplici. Per esempio se prendo $f(x) = \sin(x)$ e prendo $a=\frac{-\pi}{2}$ e $b=\frac{9\pi}{2}$, sappiamo che $\sin(x) \in C^0(\mathbb{R})$ e che $f(a) = \sin(-\frac{\pi}{2}) = -1$, $f(b) = \sin(\frac{9\pi}{2}) = 1$ e quindi $f(a)\cdot f(b) = -1\cdot 1 = -1 < 0$, quindi è applicabile il teorema di Bolzano, e quindi sappiamo che ne esiste almeno uno zero. Ma se andiamo a vedere il grafico. 

\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-1.5*pi, xmax=5.5*pi,
    ymin=-2, ymax=2,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=15cm,                  % <-- SOLO larghezza aumentata
    height=6cm,                  % <-- altezza controllata, non cresce
    samples=500,
    xtick={
        -3.1416, -1.5708, 0, 3.1416, 6.2832, 9.4248, 12.5664, 14.1372
    },
    xticklabels={
        $-\pi$, $-\frac{\pi}{2}$, $0$, $\pi$, $2\pi$, $3\pi$, $4\pi$, $\frac{9\pi}{2}$
    },
    ytick={-1,0,1}
]

    % --- Grafico sin(x) ---
    \addplot[blue, ultra thick, domain=-pi:5*pi] {sin(deg(x))};

    % --- Zeri ---
    \addplot[only marks, mark=*, blue, mark size=2pt]
        coordinates { (0,0) (pi,0) (2*pi,0) (3*pi,0) (4*pi,0)};

    % --- Punto rosso: a = -π/2 ---
    \addplot[only marks, mark=*, red, mark size=3pt]
        coordinates {(-pi/2, {-1})};
    \node[red] at (axis cs:-pi/2, -1.3) {$f(a)$};

    % --- Punto rosso: b = 9π/2 ---
    \addplot[only marks, mark=*, red, mark size=3pt]
        coordinates {(9*pi/2, {1})};
    \node[red] at (axis cs:9*pi/2, 1.3) {$f(b)$};

    % Etichetta funzione
    \node[blue] at (axis cs:4,1.3) {$\sin(x)$};

\end{axis}
\end{tikzpicture}
\end{center}

Ma notiamo subito che esistono 4 zeri. Può succedere che ci venga richiesto dell'esistenza di un unico zero, in quel caso bisogna controllare se è verificato il teorema di Bolzano (per vedere se ha degli zeri), e poi controllare se la funzione è monotona. Infatti se la funzione è monotona è verifica il teorema di Bolzano allora sappiamo che $\exists! c \in (a,b)$ tale che $f(c) =0$. Infatti con l'esempio del seno, sappiamo che il seno non è monotono, e pertanto abbiamo che ci sono più zeri. 

Invece se prendiamo la funzione $f(x)=e^x + x$ e prendiamo $a=-1$ e $b=0$, notiamo subito che $f \in C^0(\mathbb{R})$ e che $f(b)=f(0) = e^0 + 0 = 1$ e per $f(a)$ dobbiamo ricordarci che $e^{-1} < 1$, infatti $f(-1) = e^{-1} - 1 < 1 - 1 = 0$, e quindi il teorema è applicabile. 

In più notiamo che $e^x$ è monotona crescente, e anche $x$ è crescente e di conseguenza $f$ è crescente e per questo sappiamo che $\exists! c \in (-1,0)$ tale che $f(c) = 0$. Infatti con il grafico notiamo che
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-2, xmax=1.5,
    ymin=-3, ymax=6,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=400,
    xtick={-3,-2,-1,0,1,2},
    ytick={-2,0,2,4}
]

    % --- Funzione f(x) = e^x + x ---
    \addplot[blue, ultra thick, domain=-2:1.5] {exp(x) + x};

    % --- Punto a = -1 ---
    \addplot[
        only marks,
        mark=*,
        red,
        mark size=3pt
    ]
    coordinates {(-1, {exp(-1) - 1})};
    \node[red] at (axis cs:-1+0.1, {exp(-1)-1 - 0.7}) {$f(a)$};

    % --- Punto b = 0 ---
    \addplot[
        only marks,
        mark=*,
        red,
        mark size=3pt
    ]
    coordinates {(0, {exp(0) + 0})};
    \node[red] at (axis cs:0.25, {0.5}) {$f(b)$};

% --- Zero della funzione: c ≈ -0.567143 ---
    \addplot[only marks, mark=*, green!60!black, mark size=3pt]
        coordinates {(-0.567143, 0)};
    \node[green!60!black] at (axis cs:-0.567143, 0.8) {$f(c)$};

    % --- Etichetta funzione ---
    \node[blue] at (axis cs:0.6, 4.6) {$f(x)=e^x + x$};

\end{axis}
\end{tikzpicture}
\end{center}

Infatti graficamente abbiamo cioò che ci aspettavamo. Per calcolare $c$ possiamo usare il metodo usato nella dimostrazione del teorema di Bolzano, che è detto \textbf{metodo di bisezione}. Facciamo qualche passo per questo esempio:
\[
    c_0 = \frac{-1 +0}{2} = -0.5 \implies f(c_1) = e^{-0.5} + 0.5 \approx 0.1065
\]
\[
    a_1 = -1 \;\;\; b_1=-0.5 \;\;\; c_1 =-0.75 \implies f(c_1) \approx -0.2776 
\]
\[
a_2 = -0.75 \;\;\; b_2=-0.5 \;\;\; c_2 =-0.625 \implies f(c_2) \approx -0.090
\]
 E così via. Tramite una decina di iterazioni con un computer scopriamo che $c \approx -0.5671$.


\newpage

\addcontentsline{toc}{subsection}{Teormea dei Valori Intermedi}
\begin{teorema}{Teorema dei Valori Intermedi}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$, $f: I \to \mathbb{R}$ e $f \in C^0(I)$ allora 
    \[
    \left(\inf_I(f), \sup_I(f)\right) \subseteq f(I)  \subseteq \left[\inf_I(f), \sup_I(f)\right]
    \]
\end{teorema}
\begin{proof}
    In primis possiamo notare che la disequazione 
    \[
     f(I)  \subseteq \left[\inf_I(f), \sup_I(f)\right] \implies \inf_I(f) \leq f(I) \leq \sup_I(f)
    \]
    Ma questo è ovvio, per la definizione di estremo superiore e inferiore. Pertanto dobbiamo solamente dimostrare l'altra disequazione. 
    
    Per dimostrare il teorema è sufficiente che  $\forall y \in \left(\inf_I(f), \sup_I(f)\right)$ $\exists x \in I : y = f(x)$. Perchè vuol dire che ad ogni valore di $y$ riesco a trovare una $x$ in modo tale che $y=f(x)$, e se riesco a dimostrarlo per tutti i valori di $y$ abbiamo dimostrato il teorema. Ora quindi supponiamo di prendere $y \in \left(\inf_I(f), \sup_I(f)\right)$, riscrivendo scopriamo che
    \[
        \mathunderline{red}{\inf_{I}f <} y  \mathunderline{blue}{<\sup_If}
    \]
    Da cui scopriamo che
    \begin{itemize}
        \item $\mathunderline{red}{\inf_{I}f < y}$ vuol dire che $y$ NON è un minorante, dato che è più grande dell' estremo inferiore. Quindi potremmo trovare $\forall y$ $\exists a \in I$ tale che
        \begin{equation}\label{eq:int1}
            \inf_{I}f < f(a) < y \implies f(a) -  y < 0
        \end{equation}
        \item $ \mathunderline{blue}{y <\sup_If}$ vuol dire che $y$ NON è un maggiorante, dato che è più piccolo dell'estremi superiore. Quindi potremmo trovare $\forall y$ $\exists b \in I$ tale che
        \begin{equation}\label{eq:int2}
            y < f(b) <\sup_If\implies f(b) -  y > 0
        \end{equation}
    \end{itemize}
    Per comodità supponiamo che $a<b$. Da qui definiamo $g:[a,b] \to \mathbb{R}$ come
    \[
        g(x) = f(x) - y \;\;\;\;\; \forall x \in [a,b]
    \]
    Notiamo che 
    \begin{enumerate}[label=(\roman*)]
        \item dato che, per ipotesi, $f(x) \in C^0(I)$ allora anche $g(x) \in C^0(I)$
        \item  Con (\ref{eq:int1}) deduciamo che $g(a) = f(a) - y < 0$
        \item  Con (\ref{eq:int2}) deduciamo che $g(b) = f(b) - y > 0$
    \end{enumerate}
    Ma queste tre condizioni rendono possibile l'applicazione del teorema di Bolzano, dato che $g(a)\cdot g(b) <0$, e quindi sappiamo che $\exists c \in (a,b)$ tale che 
    \[
        g(c) = 0\implies f(c) - y = 0 \implies f(c) = y
    \]
    Ma quindi  $\forall y \in \left(\inf_I(f), \sup_I(f)\right)$ riesco trovare un $\exists a, b \in I$, e abbiamo scoperto che $\exists c \in (a,b)$ tale che $f(c) = y$, e quindi abbiamo verificato il teorema.
\end{proof}
\newpage

\addcontentsline{toc}{subsection}{Corollario del Teorema dei Valori Intermedi}
\begin{corollario}{del Teorema dei Valori Intermedi}{}
     Sia $a,b \in \mathbb{R}$, $f: [a,b] \to \mathbb{R}$ e $f \in C^0([a,b])$ allora
    \[
        f([a,b]) = [\min_{[a,b]}f, \max_{[a,b]}f]
    \] 
\end{corollario}
\begin{proof}
    Per il teorema dei valori intermedi sappiamo che
    \[
        \left(\inf_{[a,b]}(f), \sup_{[a,b]}(f)\right) \subseteq f([a,b])  \subseteq \left[\inf_{[a,b]}, \sup_{[a,b]}\right]
    \]
    Ma con il teorema di Weierstrass sappiamo che la funzione ammette un massimo e un minimo, per la relazione dei estrimi e massimi e minimi, allora sappiamo che se esiste un massimo o minimo allora coincide con l'estremo superiore/inferiore, quindi l'equazione sopra diventa 
     \[
        \left(\min_{[a,b]}(f), \max_{[a,b]}(f)\right) \subseteq f([a,b])  \subseteq \left[\min_{[a,b]} f, \max_{[a,b]} f\right]
    \]
    Ma dato che $\max_{[a,b]}(f) \in f([a,b])$ e $\min_{[a,b]}(f) \in f([a,b])$, allora sappiamo che
    \[
    \left[\min_{[a,b]}(f), \max_{[a,b]}(f)\right] \subseteq f([a,b])  \subseteq \left[\min_{[a,b]} f , \max_{[a,b]}f \right] \implies  f([a,b])  = \left[\min_{[a,b]}f, \max_{[a,b]}f\right] 
    \]  
\end{proof}

\addcontentsline{toc}{subsection}{Continuità delle Funzioni Inverse}
\begin{teorema}{Continuità della funzione inversa}{}
     Sia $\varnothing \ne I \subseteq \mathbb{R}$, $f: I \to \mathbb{R}$ e $f \in C^0(I)$ e invertibile in $I$ allora 
     \[
     f^{-1} \in C^0(f(I)) \iff f \text{ è strettamente monotona}
     \]  
\end{teorema}
Con questo teorema possiamo trovare altre funzioni continue, per esempio $f(x) = a^x$ con $a \in (0,1) \cup (1,+\infty)$, noi sappiamo che $f \in C^0(\mathbb{R})$ e in più sappiamo che $f$ è strettamente crescente per $x \in (1,+\infty)$ e decrescente per $x \in (0,1)$, e quindi da questo sappiamo che la funzione inversa: $f(x) = \log_a(x)$ è continua nel suo dominio: $(0, +\infty)$. 

Poi possiamo trovare anche per le funzione trigonometriche inverse, ma dobbiamo fare delle eventuali constrizioni. Prendiamo la funzione $f(x)=\sin(x)$, è vero che è continua in $\mathbb{R}$, ma non è sempre monotona, però se noi prendiamo il seno da $[-\frac{\pi}{2}, \frac{\pi}{2}]$ allora in questo intervallo è strettamente crescente, quindi possiamo dedurre che $f(x) = \arcsin(x)$ è continua in $[-1,1]$. 

Stesso ragionamento, per $f(x) =\cos(x)$ se lo prendiamo per $x\in[0, \pi]$ allora la funzione è strettamente decrescente e quindi possiamo dedurre che $f(x)= \arccos(x)$ e che $f\in C^0([-1,1])$. 

 Ragionamento analogo anche per $f(x) = \tan(x)$, infatti se prendo la funzione nell'intervallo $(-\frac{\pi}{2}, \frac{\pi}{2})$, allora so che è monotona crescente e quindi $f(x) = \arctan(x)$ è continua in $\mathbb{R}$.
 
 Tutto ciò si può dire anche per le funzioni iperboliche.

 \newpage 
 \section{Derivabilità}

\addcontentsline{toc}{subsection}{Definizione di Derivata di una Funzione}
 \begin{definizione}{Derivata di una funzione in un punto }{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, $x_0\in I$, se esiste il seguente limite finito
    \[
        \exists \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} \in \mathbb{R}
    \]
    Diciamo che \textbf{$f$ è derivabile in $x_0$} e lo indichiamo con il simbolo 
    \[
        f'(x_0) =  \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}
    \]
    Se invece non esiste il limite oppure tende a $\pm \infty$ diciamo che non è derivabile.

    Diciamo che $f$  è \textbf{derivabile } (oppure \textbf{differenziabile}) se è derivabile per ogni punto del suo dominio.
 \end{definizione}
 \textbf{N.B.} la notazione di derivata può variare, infatti ci sono varie notazioni e possono aiutare a capire il contesto, infatti potreste trovare indicata la derivata di $f(x)$ in $x_0$ come:
 \begin{itemize}
    \item $f'(x_0)$ che è la più classica ed usata
    \item $\displaystyle\frac{df}{dx}(x_0)$ oppure $\displaystyle\frac{d}{dx}(f(x_0))$ nei contesti geometrici oppure nelle equazioni differenziali
    \item $\dot{f}(x_0)$ tendenzialemente si usa in fisica
    \item $D[f(x_0)]$ quando volgiamo indicare l'operatore derivata
    \item $\displaystyle\frac{\partial f}{\partial x}(x_0)$ quando usiamo funzioni a più dimensioni
 \end{itemize}
 Quindi quando trovate uno di questi simboli indicano sempre una derivata, ma in contesti differenti. Ora proviamo a calcolare quelle fondamentali
 \addcontentsline{toc}{subsection}{Calcolo delle Derivate Elementari}
 \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=c$ in $x_0 \in \mathbb{R}$ con $c \in\mathbb{R}$ 
 \end{esercizio}
 Usiamo la definizione di derivata
 \[
 f'(x_0) =  \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} =  \lim_{x\to x_0} \frac{c-c}{x-x_0} =  \lim_{x\to x_0} \frac{0}{x-x_0} = 0
 \]
Quindi per qualsiasi funzione costante abbiamo che la sua derivata è sempre e solo 0. Questo quando vedremo gli integrali porterà ad una proprietà unica degli integrali. 
 \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=x$ in $x_0 \in \mathbb{R}$  
 \end{esercizio}
 Usiamo sempre la definizione
  \[
 f'(x_0) =  \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} =  \lim_{x\to x_0} \frac{x-x_0}{x-x_0} =  1
 \]
 \newpage
  \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=x^2$ in $x_0 \in \mathbb{R}$ 
 \end{esercizio}
 Usiamo sempre la definizione
  \[
 f'(x_0) =  \lim_{x\to x_0} \frac{x^2-x^2_0}{x-x_0} =  \lim_{x\to x_0} \frac{(x-x_0)(x+x_0)}{x-x_0} = \lim_{x\to x_0}(x+x_0) = 2x_0
 \]
 Per ora non abbiamo ancora trovato un pattern comodo per le potenze, provamo a calcolare la potenza n-esima.
 \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=x^n$ in $x_0 \in \mathbb{R}$ con $n \in \mathbb{N}$ 
 \end{esercizio}
 Usiamo sempre la definizione
  \[
 f'(x_0) =  \lim_{x\to x_0} \frac{x^n-x^n_0}{x-x_0} 
 \]
 Per calcolarlo dobbiamo riutilizzare la formula che abbiamo usato anche per il limite $x^n\to x^n_0$ e quindi
 \begin{align*}
    \lim_{x\to x_0} \frac{x^n-x^n_0}{x-x_0}  &= \lim_{x\to x_0} \frac{\displaystyle(x-x_0)\left(\sum_{k=0}^{n-1}x^{n-1-k}x^k_0\right)}{x-x_0} = \lim_{x\to x_0} \left(\sum_{k=0}^{n-1}x^{n-1-k}x^k_0\right) \\
    &= \sum_{k=0}^{n-1}x^{n-1-k}_0x^k_0 = \sum_{k=0}^{n-1}x^{n-1-k + k}_0 = \sum_{k=0}^{n-1}x^{n-1}_0 = nx_0^{n-1}
\end{align*}
Per le prossime derivate dobbiamo un attimino modificare la definizione di derivata, infatti se alla definizione applico un cambio di variabole con $h = x-x_0$, e quindi $x = h+x_0$, abbiamo che
\[
f'(x_0) = \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} =  \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)}{h}
\]
Con questo possiamo sostituire $x_0$ con $x$ per calcolare la funzione derivata su qualsiasi punto anzichè su un punto solo. Quindi la nuova definizione di derivata (che è uguale a quella di prima ma può essere più comoda per certi conti) è
\[
f'(x) = \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}
\]
 \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=x^\alpha$ in $x> 0$ con $\alpha \in \mathbb{R}$ 
 \end{esercizio}
Usiamo sempre la definizione
 \begin{align*}
    f'(x_0) &=  \lim_{h\to 0} \frac{(x+h)^\alpha-x^\alpha}{h} = \lim_{h\to 0} \frac{\left(x\left(1+\frac{h}{x}\right)\right)^\alpha-x^\alpha}{h} \\
 &= \lim_{h\to 0} \frac{x^\alpha\cdot\left(1+\frac{h}{x}\right)^\alpha-x^\alpha}{h} = \lim_{h\to 0} \frac{x^\alpha\cdot\left(\left(1+\frac{h}{x}\right)^\alpha-1\right)}{h}
 \end{align*}
Possiamo applicare un cambio di variabile con $t = \frac{h}{x}$, e dato che $x>0$ non ci reca alcun danno.
\[
\lim_{h\to 0} \frac{x^\alpha\cdot\left(\left(1+\frac{h}{x}\right)^\alpha-1\right)}{h} = \lim_{t\to 0} x^\alpha\cdot \frac{(1+t)^\alpha-1}{tx} = \lim_{t\to 0} x^{\alpha-1}\cdot \frac{(1+t)^\alpha-1}{t} 
\]
Ora dobbiamo usare un limite notevole al contrario. Infatti noi sappiamo che 
\[
 e^t \sim 1+t \;\;\; \text{per } t\to0 
\]
E quindi nella stramagioranza degli esercizi sostituavamo $1+t$ al posto di $e^t$, ma possiamo fare anche il contrario. In effetti in questo caso ci conviene sostituire $1+t$ con $e^t$. Quindi
\[
\lim_{t\to 0} x^{\alpha-1}\cdot \frac{(1+t)^\alpha-1}{t} \sim \lim_{t\to 0} x^{\alpha-1}\cdot \frac{(e^t)^\alpha-1}{t} = \lim_{t\to 0} x^{\alpha-1}\cdot \frac{e^{\alpha t}-1}{t} = \alpha \cdot x^{\alpha-1}
\]
  



\begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=\sin(x)$  
 \end{esercizio}
  Usiamo sempre la definizione
\begin{align*}
     f'(x) &=  \lim_{h\to 0} \frac{f(x+h)-f(x)}{h} = \lim_{h\to 0} \frac{\sin(x+h)-\sin(x)}{h} \\
     &= \lim_{h\to 0} \frac{\mathunderline{red}{\sin(x+h)}-\sin(x)}{h} = \lim_{h\to 0} \frac{\mathunderline{red}{\sin(x)\cos(h)+\sin(h)\cos(x)}-\sin(x)}{h} \\
&= \lim_{h\to 0} \left(\frac{\sin(x)\cos(h)-\sin(x)}{h} + \frac{\sin(h)\cos(x)}{h} \right)
    \end{align*}
    Sistemando i limiti notevoli abbiamo
\[
=\lim_{h\to 0} \left(\frac{\sin(x)\cos(h)-\sin(x)}{h} + \frac{\sin(h)\cos(x)}{h} \right) = \lim_{h\to 0} \left(\frac{\sin(x)(\cos(h)-1)}{h} + \frac{\sin(h)}{h} \cos(x)\right)  
\]
\[
= \lim_{h\to 0} \left(\sin(x)\frac{(\cos(h)-1)}{h^2} \cdot h + \frac{\sin(h)}{h} \cos(x)\right)  = \sin(x)\cdot \left(\frac{-1}{2}\right) \cdot 0 + 1\cdot \cos(x) = \cos(x)       
\]
Ragionamento analogo per il coseno
  \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=\cos(x)$  
 \end{esercizio}
\begin{align*}
     f'(x) &=  \lim_{h\to 0} \frac{f(x+h)-f(x)}{h} = \lim_{h\to 0} \frac{\cos(x+h)-\cos(x)}{h} \\
     &= \lim_{h\to 0} \frac{\mathunderline{red}{\cos(x+h)}-\cos(x)}{h} = \lim_{h\to 0} \frac{\mathunderline{red}{\cos(x)\cos(h)-\sin(h)\sin(x)}-\cos(x)}{h} \\
&= \lim_{h\to 0} \left(\frac{\cos(x)\cos(h)-\cos(x)}{h} - \frac{\sin(h)\sin(x)}{h} \right)
    \end{align*}
    Per lo stesso ragionamento di prima
    \[
=\lim_{h\to 0} \left(\cos(x)\frac{\cos(h)-1}{h} - \frac{\sin(h)\sin(x)}{h} \right) = \cos(x)\cdot \left(\frac{-1}{2}\right) \cdot 0 - 1\cdot \sin(x) = -\sin(x)  
\]

Attenzione quindi che la derivata del coseno è meno seno, cosa che può portare a confunsione anche perchè con gli integrali sarà il contrario, quindi attenzione
\[
\begin{array}{c @{\qquad}@{\qquad} c}
    \displaystyle \frac{d}{dx}(\sin(x)) = \cos(x) &\displaystyle \frac{d}{dx}(\cos(x)) = -\sin(x)
\end{array}
\]
  \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=e^x$  
 \end{esercizio}
 \[
    f'(x) =  \lim_{h\to 0} \frac{e^{x+h} - e^x}{h} = \frac{e^{x}\cdot e^h - e^x}{h} = \frac{e^{x}(e^h - 1)}{h} =e^x
 \]
 Una caratteristica che ci tornerà utile con le equazioni differenziali è che l'esponenziale è l'unica funzione la cui derivatà è uguale alla funzione di partenza, e vedremo come ci aiuterà a risolvere le equazioni differenziali.
   \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=\log(x)$  
 \end{esercizio}
 \begin{align*}
    f'(x) &=  \lim_{h\to 0} \frac{\log(x+h) - \log(x)}{h} =  \lim_{h\to 0} \frac{1}{h}\cdot \log\left(\frac{x+h}{x}\right) \\
    &= \lim_{h\to 0} \frac{1}{h}\cdot \log\left(1 + \frac{h}{x}\right) =\lim_{h\to 0} \log\left(\left(1 + \frac{h}{x}\right) ^{\frac{1}{h}}\right) 
 \end{align*}
 Ora è sufficiente fare un cambio di variabile con $t = \frac{h}{x}$, questo è possibile dato che $x>0$ per il dominio di $f(x)$.
\begin{align*}
 \lim_{h\to 0} \log\left(\left(1 + \frac{h}{x}\right) ^{\frac{1}{h}}\right)  &= \lim_{t\to 0} \log\left(\left(1 + t\right) ^{\frac{1}{tx}}\right) =  \lim_{t\to 0} \log\left(\left(\left(1 + t\right) ^{\frac{1}{t}}\right)^{\frac{1}{x}}\right)\\
    &= \lim_{t\to 0} \frac{1}{x}\log\left(\left(1 + t\right) ^{\frac{1}{t}}\right) = \frac{1}{x} \log(e) = \frac{1}{x}
\end{align*}
In seguito una tabella che racchiude le principali derivate

\begin{center}
\begin{tabular}{c @{\quad} c}

% prima tabella (solo tabular)
\begin{tabular}{c|c}
Funzione Base & Derivata              \\ \hline
$c$           & $0$                   \\
$\sin(x)$     & $\cos(x)$             \\
$e^x$         & $e^x$
\end{tabular}
&
% seconda tabella (solo tabular)
\begin{tabular}{c|c}
Funzione Base & Derivata              \\ \hline
$x^\alpha$    & $\alpha x^{\alpha-1}$ \\
$\cos(x)$     & $-\sin(x)$            \\
$\log(x)$     & $\frac{1}{x}$
\end{tabular}

\end{tabular}
\end{center}

 \newpage

 \addcontentsline{toc}{subsection}{Interpretazione Geometrica di Derivata}
Ora vediamo da dove viene fuori la derivata, infatti se prendiamo una qualsiasi funzione e scegliamo un punto $x_0$ e prendiamo anche un altro punto $x_0+h$, con $h>0$, e tracciamo la retta passante per $(x_0, f(x_0))$ e $(x_0+h, f(x_0+h))$ notiamo che
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=0, xmax=4,
    ymin=-1, ymax=2.5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=400,
    xtick=\empty,
    ytick=\empty
]

    % --- Funzione f(x) = ln(x) ---
    \addplot[blue, ultra thick, domain=0.3:4] {ln(x)};
\node[blue] at (axis cs:3.65,1.75) {$y=f(x)$};
    % --- Punto x0 = 2 ---
    \addplot[
        only marks,
        mark=*,
        red,
        mark size=2pt
    ]
    coordinates {(2, {ln(2)})};

 --- Linea tratteggiata in x0 = 2 ---
    \addplot[dashed, gray, thick] coordinates {(2,-0.5) (2,2.5)};
    \node at (axis cs:2,-0.5) [below] {$x_0$};

    % --- Linea tratteggiata in x0+h = 3 ---
    \addplot[dashed, gray, thick] coordinates {(3,-0.5) (3,2.5)};
    \node at (axis cs:3,-0.5) [below] {$x_0 + h$};

    % --- Linea tratteggiata orizzontale per f(x0) = ln(2) ---
\addplot[dashed, gray, thick] coordinates {(4, {ln(2)}) (0, {ln(2)})};
\node at (axis cs:0, {ln(2)}) [left] {$f(x_0)$};

% --- Linea tratteggiata orizzontale per f(x0+h) = ln(3) ---
\addplot[dashed, gray, thick] coordinates {(4, {ln(3)}) (0, {ln(3)})};
\node at (axis cs:0, {ln(3)}) [left] {$f(x_0+h)$};



    % --- Punto x0 + h = 3 ---
    \addplot[
        only marks,
        mark=*,
        red,
        mark size=2pt
    ]
    coordinates {(3, {ln(3)})};

    % --- Retta secante tra 2 e 3 ---
    % Pendenze: (ln(3)-ln(2)) / (3-2) = ln(3)-ln(2)
    \addplot[red, ultra thick, domain=0:4]
        {ln(2) + (ln(3)-ln(2))*(x-2)};

\end{axis}
\end{tikzpicture}
\end{center}

Notiamo che il limite che abbiamo calcolato fino ad ora non è altro che il coefficente angolare della retta disegnata in rosso. Infatti
\[
m = \frac{\Delta y}{\Delta x} = \frac{f(x_0+h) - f(x_0)}{x_0+h - x_0} = \frac{f(x_0+h) - f(x_0)}{h}
\]
Questo rapposto è definito anche come \textbf{rapporto incrementale}. Ma la derivata è il limite di questo rapporto, quindi con la derivata stiamo cercando di avvicinarci il più possibile alla retta tangente nel punto $x_0$, infatti più che $h\to 0$ più che la retta rossa tende alla retta tangente effettiva (quella in verde)
\begin{center}
\begin{tabular}{c@{\qquad}c@{\qquad}c}

% ================= PRIMO GRAFICO: h = 1.5 =================
\begin{tikzpicture}
\begin{axis}[
    xmin=0, xmax=4,
    ymin=-1, ymax=2.5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=6cm,
    height=6cm,
    samples=400,
    xtick=\empty,
    ytick=\empty
]
    \addplot[blue, ultra thick, domain=0.3:4] {ln(x)};

    % x0 = 2
    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(2, {ln(2)})};

    % h = 1.5 → x0+h = 3.5
    \addplot[dashed, gray, thick] coordinates {(2,-0.5) (2,2.5)};
    \addplot[dashed, gray, thick] coordinates {(3.5,-0.5) (3.5,2.5)};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(2)}) (0, {ln(2)})};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(3.5)}) (0, {ln(3.5)})};

    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(3.5, {ln(3.5)})};

% tangente in x=2
    \addplot[green!60!black, ultra thick, domain=0:4]
        {ln(2) + (1/2)*(x - 2)};

    % secante
    \addplot[red, ultra thick, domain=0:4]
        {ln(2) + (ln(3.5)-ln(2))/1.5*(x-2)};

    

\end{axis}
\end{tikzpicture}
&
% ================= SECONDO GRAFICO: h = 1 =================
\begin{tikzpicture}
\begin{axis}[
    xmin=0, xmax=4,
    ymin=-1, ymax=2.5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=6cm,
    height=6cm,
    samples=400,
    xtick=\empty,
    ytick=\empty
]
    \addplot[blue, ultra thick, domain=0.3:4] {ln(x)};

    % x0 = 2
    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(2, {ln(2)})};

    % h = 1 → x0+h = 3
    \addplot[dashed, gray, thick] coordinates {(2,-0.5) (2,2.5)};
    \addplot[dashed, gray, thick] coordinates {(3,-0.5) (3,2.5)};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(2)}) (0, {ln(2)})};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(3)}) (0, {ln(3)})};

    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(3, {ln(3)})};
 %tangente in x=2
    \addplot[green!60!black, ultra thick, domain=0:4]
        {ln(2) + (1/2)*(x - 2)};
    % secante
    \addplot[red, ultra thick, domain=0:4]
        {ln(2) + (ln(3)-ln(2))*(x-2)};

    

\end{axis}
\end{tikzpicture}
&
% ================= TERZO GRAFICO: h = 0.5 =================
\begin{tikzpicture}
\begin{axis}[
    xmin=0, xmax=4,
    ymin=-1, ymax=2.5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=6cm,
    height=6cm,
    samples=400,
    xtick=\empty,
    ytick=\empty
]
    \addplot[blue, ultra thick, domain=0.3:4] {ln(x)};

    % x0 = 2
    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(2, {ln(2)})};

    % h = 0.5 → x0+h = 2.5
    \addplot[dashed, gray, thick] coordinates {(2,-0.5) (2,2.5)};
    \addplot[dashed, gray, thick] coordinates {(2.5,-0.5) (2.5,2.5)};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(2)}) (0, {ln(2)})};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(2.5)}) (0, {ln(2.5)})};

    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(2.5, {ln(2.5)})};
\addplot[green!60!black, ultra thick, domain=0:4]
        {ln(2) + (1/2)*(x - 2)};
    % secante
    \addplot[red, ultra thick, domain=0:4]
        {ln(2) + (ln(2.5)-ln(2))/0.5*(x-2)};

    % tangente
    

\end{axis}
\end{tikzpicture}

\end{tabular}
\end{center}

Di conseguenza abbiamo capito che la derivata è il limite del rapporto incrementale, e che quindi è il coefficente angolare della retta tangete in un punto ad una qualsiasi funzione $y=f(x)$. Quindi in generale la retta tangente ad una funzione in un punto è
\[
y = f(x_0) + f'(x_0) (x-x_0)
\]

Possiamo notare che se calcoliamo questo limite della differenza tra la funzione $f(x)$ e la sua retta tangente e lo dividiamo per $x-x_0$ notiamo che
\[
\lim_{x\to x_0} \frac{f(x) - (f(x_0) + f'(x_0)(x-x_0))}{x-x_0} = \lim_{x\to x_0} \left(\frac{f(x) - f(x_0) }{x-x_0} - f'(x_0)\right) 
\]
Ma dato che sappiamo che la funzione è derivabile (visto che esiste la retta tangente) in $x_0$ allora il limite tende a $f'(x_0)$
\[
\lim_{x\to x_0} \left(\frac{f(x) - f(x_0) }{x-x_0} - f'(x_0)\right)= f'(x_0) - f'(x_0) = 0
\]
Dato che è venuto fuori 0, allora se definiamo $a=f'(x_0)$ e $b=f(x_0) - f'(x_0)x_0$ possiamo dire
\[
f(x) - (ax+b) = o(x-x_0) \;\;\;\;\; \text{per } x\to x_0
\]

\addcontentsline{toc}{subsection}{Definizione di Retta Tangente}
\begin{definizione}{Retta Tangente}{}
    In maniera più formale, definiamo \textbf{retta tangente} a $f$ nel punto $x_0$ la retta di equazione
    \[
        y=ax+b \;\;\;\;\; a,b \in \mathbb{R}
    \]
    tale che
    \[
        f(x) - (ax+b) = o(x-x_0) \;\;\;\;\; \text{per } x\to x_0
    \]
\end{definizione}
Con questa formula si possono ricavare, ormai già ben noti, i prodotti notevoli, infatti se prendiamo $f(x) = e^x$, allora calcoliamo la retta tangente in $x_0 = 0$
\[
\begin{array}{c @{\qquad}@{\qquad} c}
    f(x_0) = e^0 = 1 & f'(x_0) = e^0 = 1
\end{array}
\] 
Quindi la retta tangente ha l'equazione
\[
    y=1 + 1(x-0) \implies y =x+1
\]
E quindi dato che è la retta tangente sappiamo che
\[
e^x - (x+1) = o(x-x_0) \;\;\;\;\; \text{per } x\to 0
\]
Da cui ricaviamo
\[
e^x  = x+1 + o(x-x_0) \;\;\;\;\; \text{per } x\to 0
\] 
Che conoscevamo già. Poi si possono dimostrare anche tutti gli altri limiti notevoli.

\begin{esercizio}{}{}
    Determinare l'equazione delle due rette passanti per $(1, -3)$ e tangenti al grafico di $f(x) = x^2$
\end{esercizio}
Possiamo trovare la retta generale ad $f(x)=x^2$ per un punto generale $x_0$, che poi decideremo con l'altra condizione. Quindi in primis dobbiamo alcolare la derivata di $f(x)$, che con le formule che abbiamo scoperto prima abbiamo che
\[
    \frac{d}{dx}(x^2) = 2x
\]
Quindi la retta tangente sarà
\[
\begin{array}{c @{\qquad}@{\qquad} c}
    f(x_0) = x^2_0 & f'(x_0) = 2x_0
\end{array}
\]
\[
y=  x^2_0 +2x_0 (x-x_0) \implies y= x^2_0 + 2x_0 x  +2x_0^2  \implies y=  2x_0 x -x^2_0 
\]
Ora questa è la retta generica, noi dobbiamo trovare quelle che passano per $(1,-3)$ quindi sostituiamo le coordinate
\[
-3 = 2x_0 \cdot  1 - x^2_0 \implies    -x^2_0 + 2x_0 +3 = 0 \implies x_{0} = -1  \lor x_{0} = 3
\]
Quindi le due rette sono
\[
\begin{array}{c @{\qquad} c}
    y=  2(-1) x -(-1)^2 \implies y=-2x -2 & y=  2(3) x -(3)^2 \implies y=6x -9
\end{array}
\]
\newpage
\addcontentsline{toc}{subsection}{Relazione tra Continuità e Derivabilità}
\begin{teorema}{Relazione tra Continuità e Derivabilità}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$. Se $f$ è derivabile in $x_0 \in I$ allora è anche continua.
    \[
    \text{Derivabilità } \implies \text{ Continuità}
    \]
\end{teorema}
\begin{proof}
    Dato che $f$ è derivabile in $x_0$ allora esiste la retta tangente nel punto $x_0$. Quindi se calcoliamo il limite di $f$ possiamo sostituirlo con la formula della retta tangente
    \begin{align*}
        \lim_{x\to x_0} f(x) &= \lim_{x\to x_0} \left(f(x_0) + f'(x_0) (x-x_0) + o(x-x_0)\right) \\
        &= f(x_0) + f'(x_0) \cdot\lim_{x\to x_0} (x-x_0) +\lim_{x\to x_0}  \left(\frac{o(x-x_0)}{x-x_0} \cdot (x-x_0)\right) \\
        &=  f(x_0) + f'(x_0) \cdot 0 + 0\cdot 0 = f(x_0)
    \end{align*}
\end{proof}
Questa informazione può essere molto utile negli esercizi perchè se vediamo che un punto non è continuo, allora non sarà nemmeno derivabile. 
 
\textbf{N.B.} Non è vero il contrario, infatti se prendiamo $f(x) = |x|$, per i limiti sappiamo che è continua in $\mathbb{R}$, però se proviamo a calcolare la derivata in $x=0$ notiamo che 
\[
    \lim_{x\to x_0} \frac{|x| -|x_0|}{x - x_0} =\lim_{x\to 0} \frac{|x| -|0|}{x - 0}=  \lim_{x\to 0} \frac{|x|}{x}
\]
Se proviamo a distinguere il il caso del limite destro e sinistro, usando anche la proprietà del modulo tale che se $x > 0$ allora $|x| = x$, mentre se $x < 0$ allora $|x| = -x$, quindi
\[
\begin{array}{c @{\qquad}@{\qquad} c}
    \displaystyle\lim_{x\to 0^+} \frac{|x|}{x} = \lim_{x\to 0^+} \frac{x}{x} = 1 &  \displaystyle\lim_{x\to 0^-} \frac{|x|}{x} = \lim_{x\to 0^+} \frac{-x}{x} = -1
\end{array}
\]
Ma quindi limite destro e sinistro sono diversi e quindi il limite non esiste e questo vuol dire che la funzione non  è derivabile in quel punto. Con questo ragionamento possiamo dedurre che
\[
\frac{d}{dx}(|x|)= \begin{cases}
    1 & x>0 \\
    -1 & x <0
\end{cases}
\]

\addcontentsline{toc}{subsection}{Definizione di Derivata Destra e Sinistra}
\begin{definizione}{Derivata Destra e Sinistra}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, $x_0 \in I$ allora
    \begin{itemize}
        \item Se
    \[
        \exists \lim_{x\to x_0^+} \frac{f(x)-f(x_0)}{x-x_0} = \lim_{h\to 0^+} \frac{f(x+h)-f(x)}{h} \in \mathbb{R}
    \]
     diciamo che $f$ \textbf{è derivabile da destra in $x_0$} e lo indichiamo come $f'_+(x_0)$. 
    \item Se
    \[
        \exists \lim_{x\to x_0^-} \frac{f(x)-f(x_0)}{x-x_0} = \lim_{h\to 0^-} \frac{f(x+h)-f(x)}{h} \in \mathbb{R}
    \]
     diciamo che $f$ \textbf{è derivabile da sinistra in $x_0$} e lo indichiamo come $f'_-(x_0)$. 
    \end{itemize}
\end{definizione}


\addcontentsline{toc}{subsection}{Classificazione dei punti di non Derivabilità}
\begin{definizione}{Classificazione dei punti di non Derivabilità}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, $x_0 \in I$ punto di non derivabilità, allora 
    \begin{itemize}
        \item Se $f'_-(x_0) = f'_+(x_0) = +\infty$ oppure $f'_-(x_0) = f'_+(x_0) = -\infty$ allora diciamo che $f$ ha un \textbf{Flesso a tangente verticale} in $x_0$
        \item Se $f'_-(x_0) \in \mathbb{R} \lor f'_+(x_0) \in \mathbb{R}$ (chiaramente $f'_-(x_0) \neq f'_+(x_0) $, altrimenti sarebbe derivabile), allora diciamo che $f$ ha un \textbf{punto angoloso} in $x_0$
        \item Se $f'_-(x_0) -\infty \land f'_+(x_0) = +\infty$ oppure $f'_-(x_0) +\infty \land f'_+(x_0) = -\infty$ allora diciamo che $f$ ha una \textbf{cuspide} in $x_0$
    \end{itemize}
\end{definizione}
Vediamo degli esempi, per esempio la funzione $f(x) =\sqrt[3]{x}$, possiamo calcolare la derivata con le regole di prima
\[
\frac{d}{dx}(\sqrt[3]{x}) = \frac{d}{dx}(x^\frac{1}{3}) = \frac{1}{3} x^{\frac{1}{3}-1} = \frac{1}{3} x^{-\frac{2}{3}} = \frac{1}{3\sqrt[3]{x^2}}
\]
Ora possiamo calcolare la derivata destra e sinistra in $x=0$
\[
\begin{array}{c@{\qquad}@{\qquad}c}
    \displaystyle\lim_{x\to 0^+} \frac{1}{3\sqrt[3]{x^2}} = +\infty & \displaystyle\lim_{x\to 0^-} \frac{1}{3\sqrt[3]{x^2}} = +\infty 
\end{array}
\]
Quindi notiamo che è un flesso a tangente verticale, e vediamo graficamente che nel punto $x=0$ la funzione "impenna" drasticamente per poi riassestarsi.
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-8, xmax=8,
    ymin=-4, ymax=4,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=500
]

    % --- f(x) = cube root(x) (forma corretta!) ---
    \addplot[blue, ultra thick, domain=-8:8]
        {sign(x)*abs(x)^(1/3)};

    % Etichetta funzione
    \node[blue] at (axis cs:6,2.5) {$f(x)=\sqrt[3]{x}$};

\end{axis}
\end{tikzpicture}
\end{center}

Per il punto angoloso abbiamo già visto un esempio: il valore assoluto. Infatti avevamo visto che 
\[
\begin{array}{c @{\qquad} c}
    f'_-(0) = -1 & f'_+(0) = 1
\end{array}
\]
E graficamente notiamo che si forma un angolo in $x=0$
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-5, xmax=5,
    ymin=-0.5, ymax=5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=500
]

    % --- f(x) = |x| ---
    \addplot[blue, ultra thick, domain=-5:5]
        {abs(x)};

    % Etichetta funzione
    \node[blue] at (axis cs:4.5,3) {$f(x)=|x|$};

\end{axis}
\end{tikzpicture}
\end{center}

\newpage

Invece se prendiamo la funzione $f(x)=\sqrt{|x|}$ dobbiamo usare la definizione di derivata destra e sinistra
\[
\lim_{x\to 0^+} \frac{\sqrt{|x|} - \sqrt{|0|}}{x-0} = \lim_{x\to 0^+} \frac{\sqrt{x}}{x} = \lim_{x\to 0^+} \frac{1}{\sqrt{x}} = +\infty
\]
\[
\lim_{x\to 0^-} \frac{\sqrt{|x|} - \sqrt{|0|}}{x-0} = \lim_{x\to 0^-} \frac{\sqrt{-x}}{-(\sqrt{-x})^2} = \lim_{x\to 0^-} \frac{1}{-\sqrt{-x}} = -\infty
\]
Quindi ricadiamo nella casistica della cuspide, che graficamente assomiglia ad una V (o una V rovesciata se il segno segni infiniti è invertito)
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-5, xmax=5,
    ymin=-0.5, ymax=3,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=500
]

    % --- f(x) = sqrt(abs(x)) ---
    \addplot[blue, ultra thick, domain=-5:5]
        {sqrt(abs(x))};

    % Etichetta funzione
    \node[blue] at (axis cs:3.2,2.4) {$f(x)=\sqrt{|x|}$};

\end{axis}
\end{tikzpicture}
\end{center}

\addcontentsline{toc}{subsection}{Algebra delle Derivate}
\begin{teorema}{Algebra delle Derivate}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f,g:I \to \mathbb{R}$. Se $f$ e $g$ sono derivabili in $I$ allora
    \begin{enumerate}[label=(\roman*)]
        \item $(\alpha f\pm \beta g)(x)$ è continua in $I$ (con $\alpha, \beta \in \mathbb{R}$) e vale 
        \[
            \frac{d}{dx}((\alpha f\pm \beta g)(x)) = \alpha f'(x) \pm \beta g'(x)
        \]
        \item $(f \cdot g)(x)$ è continua in $I$ e vale 
        \[
            \frac{d}{dx}((f \cdot g)(x)) = f'(x)g(x) + f(x)g'(x)
        \]
        \item $\left(\dfrac{f}{g}\right)(x)$ è continua in $I$ se $g(x)\ne 0$ e vale 
        \[
            \frac{d}{dx}\left(\left(\dfrac{f}{g}\right)(x)\right) = \dfrac{f'(x)g(x)-f(x)g'(x)}{g^2 (x)}
        \]
    \end{enumerate}
\end{teorema}
\begin{proof}
    $(i)$ quindi data la funzione $(\alpha f\pm \beta g)(x)$ usiamo la definizione di derivata
    \begin{align*}
        \frac{d}{dx}((\alpha f\pm \beta g)(x)) &= \lim_{x\to x_0}\dfrac{(\alpha f(x) \pm \beta g(x)) - (\alpha f(x_0) \pm \beta g(x_0))}{x-x_0} \\
        &= \lim_{x\to x_0}\dfrac{(\alpha f(x) - \alpha f(x_0)) \pm (\beta g(x) - \beta g(x_0))}{x-x_0} \\
    &= \lim_{x\to x_0}\alpha\cdot\dfrac{f(x) - f(x_0) }{x-x_0} \pm \beta \cdot\frac{ g(x) - g(x_0)}{x-x_0} 
    \end{align*} 
    \newpage 
    Per ipotesi sappiamo che $f$ e $g$ sono derivabili e quindi vale 
    \[
    \begin{array}{c @{\qquad} @{\qquad} c}
        \displaystyle f'(x_0) = \lim_{x\to x_0}\dfrac{ f(x) - f(x_0) }{x-x_0} & \displaystyle g'(x_0) = \lim_{x\to x_0}\dfrac{ g(x) - g(x_0) }{x-x_0}
    \end{array}\]
    Quindi usando l'algebra dei limiti scopriamo che
    \[
    \lim_{x\to x_0}\alpha\cdot\dfrac{ f(x) - f(x_0) }{x-x_0} \pm \beta \cdot\frac{ g(x) - g(x_0)}{x-x_0}  = \alpha f'(x_0) \pm \beta g'(x_0)
    \]
    Ma dato che $f$ e $g$ sono continue $\forall x \in I$, possiamo più semplicemente scrivere
    \[
\frac{d}{dx}((\alpha f\pm \beta g)(x))=    \alpha f'(x) \pm \beta g'(x)
    \] 

    $(ii)$ Usando la definizione di derivata abbiamo che
    \[
    \frac{d}{dx}((f \cdot g)(x)) = \lim_{x\to x_0} \frac{f(x)g(x) - f(x_0)g(x_0)}{x-x_0}
    \]
    Per dimostrare questo è sufficiente aggiungere e togliere il termine $f(x_0)g(x)$ (oppure anche $f(x)g(x_0)$) e notiamo che
    \begin{align*}
        \lim_{x\to x_0} \frac{f(x)g(x) - f(x_0)g(x_0)}{x-x_0} &= \lim_{x\to x_0} \frac{f(x)g(x) \color{red}{-f(x_0)g(x) +f(x_0)g(x)} \color{black}{- f(x_0)g(x_0)}}{x-x_0} \\
        &= \lim_{x\to x_0} \frac{(f(x) -f(x_0))g(x) +f(x_0)(g(x) - g(x_0)}{x-x_0} \\
        &=   \lim_{x\to x_0} g(x)\cdot\frac{f(x) -f(x_0)}{x-x_0} + \lim_{x\to x_0} f(x_0)\cdot\frac{g(x) -g(x_0)}{x-x_0}
    \end{align*}
    Per le frazioni usiamo lo stesso ragionamento di prima, per il termine $f(x_0)$ lo possiamo portare fuori dato che è una costante, invece il termine $g(x)$ dobbiamo ragionare. Infatti noi sappiamo solamente che $g$ è derivabile, però non sappiamo nulla su $\displaystyle\lim_{x\to x_0} g(x)$. Però possiamo ricordarci che se una funzione è derivabile allora è anche continua. Pertanto scopriamo che $g$ è anche continua, e che quindi vale 
    \[
     \lim_{x\to x_0} g(x) = g(x_0)
    \] 
    Quindi il limite diventa
    \[
     \lim_{x\to x_0} g(x)\cdot\frac{f(x) -f(x_0)}{x-x_0} + \lim_{x\to x_0} f(x_0)\cdot\frac{g(x) -g(x_0)}{x-x_0} = g(x_0)f'(x_0) + f(x_0)g'(x_0)
    \]
    Come prima, dato che $f$ e $g$ sono derivabili (e continue) $\forall x \in I$, possiamo riscriverlo come
    \[
g(x_0)f'(x_0) + f(x_0)g'(x_0) = f'(x)g(x) + f(x)g'(x)
    \]

    $(iii)$ Usiamo la definizione di derivata
    \[
    \frac{d}{dx}\left(\left(\dfrac{f}{g}\right)(x)\right) = \lim_{x\to x_0} \dfrac{\dfrac{f(x)}{g(x)} - \dfrac{f(x_0)}{g(x_0)}}{x-x_0}
    \]
    \newpage
    Facciamo il denominatore comune
    \[
    \lim_{x\to x_0} \dfrac{\dfrac{f(x)}{g(x)} - \dfrac{f(x_0)}{g(x_0)}}{x-x_0} = \lim_{x\to x_0} \dfrac{\dfrac{f(x)g(x_0) - f(x_0)g(x)}{g(x)g(x_0)}}{x-x_0} = \lim_{x\to x_0} \dfrac{f(x)g(x_0) - f(x_0)g(x)}{g(x)g(x_0)(x-x_0)}  
    \]
    Ora possiamo fare lo stesso trucchetto di prima aggiungendo e sottraendo $f(x)g(x)$
    \begin{align*}
        \lim_{x\to x_0} \dfrac{f(x)g(x_0) - f(x_0)g(x)}{g(x)g(x_0)(x-x_0)} &= \lim_{x\to x_0} \dfrac{f(x)g(x_0) \color{red}{-f(x)g(x) +f(x)g(x)}\color{black}- f(x_0)g(x)}{g(x)g(x_0)(x-x_0)} \\
        &= \lim_{x\to x_0} \dfrac{-f(x)(-g(x_0) +g(x)) +(f(x)- f(x_0))g(x)}{g(x)g(x_0)(x-x_0)} \\
        &= \lim_{x\to x_0} -f(x)\cdot \dfrac{-g(x_0) +g(x)}{g(x)g(x_0)(x-x_0)} +\dfrac{f(x)- f(x_0)}{g(x)g(x_0)(x-x_0)}\cdot g(x)
    \end{align*}
    Ora possiamo usare come prima la definizione di defivata per $f$ e $g$, e anche per la continuità di $f$ e $g$ sappiamo che $f(x)\to f(x_0)$ e $g(x)\to g(x_0)$
    \begin{align*}
        \lim_{x\to x_0} -f(x)\cdot \dfrac{-g(x_0) +g(x)}{g(x)g(x_0)(x-x_0)} +\dfrac{f(x)- f(x_0)}{g(x)g(x_0)(x-x_0)}\cdot g(x)  &= \dfrac{-f(x_0)g'(x_0)}{g(x_0)g(x_0)} + \dfrac{f'(x_0)g(x_0)}{g(x_0)g(x_0)} \\
&= \dfrac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}
    \end{align*}
\end{proof}


\addcontentsline{toc}{subsection}{Derivata di Funzioni Composte}
\begin{teorema}{Derivata di Funzioni Composte}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f$, $g$ due funzioni tali che $g \circ f : I \to \mathbb{R}$, è ben definito. Se
    \begin{itemize}
        \item $f$ è derivabile in $x_0 \in I$
        \item $g$ è derivabile in $f(x_0)$ 
    \end{itemize}
    Allora 
    \[
    \dfrac{d}{dx}((g\circ f)(x)) = g'(f(x))\cdot f'(x)
    \]
\end{teorema}
\begin{proof}
    Usando la definione di retta tangente, e il cambio di variabile sappiamo che 
    \begin{enumerate}[label=(\arabic*)]
        \item $f(x) = f(x_0) + f'(x_0)(x-x_0) + o(x-x_0)$ per $x\to x_0$
        \item $g(y) = g(y_0) + g'(y_0)(y-y_0) + o(y-y_0)$ per $y\to y_0$
    \end{enumerate}
    Quindi se scegliamo $y_0 = f(x_0)$, avremo che
    \[
    g(\mathunderline{red}{y}) = g(f(x_0)) + g'(f(x_0))(\mathunderline{red}{y}-f(x_0)) + o(\mathunderline{red}{y}-f(x_0)) \text{ per } \mathunderline{red}{y}\to f(x_0)
    \]
    Quindi ora calcolando $g(f(x))$ avremo che
    \[
    g(\mathunderline{red}{f(x)}) = g(f(x_0)) + g'(f(x_0))(\mathunderline{red}{f(x)}-f(x_0)) + \mathunderline{blue}{o(\mathunderline{red}{f(x)}-f(x_0))} \text{ per } \mathunderline{red}{f(x)}\to f(x_0)
    \]
    \newpage
    Ora dato usando $(1)$ possiamo calcolare meglio
    \begin{align*}
        \mathunderline{green}{f(x)}-f(x_0) &= \mathunderline{green}{f(x_0) + f'(x_0)(x-x_0) + o(x-x_0)} - f(x_0) \\
        &= f'(x_0)(x-x_0) + o(x-x_0) 
    \end{align*}
    Ora riscriviamo meglio il termine con l'o-piccolo, usando la proprietà $o(f+o(f)) =o(f)$ e quella che perde tutte le costanti moltiplicative
    \begin{align*}
        \mathunderline{blue}{o(f(x)-f(x_0))} &= o(f'(x_0)(x-x_0) + o(x-x_0)) \\
        &= o\left(f'(x_0)\left((x-x_0) + \frac{o(x-x_0)}{f'(x_0)}\right)\right) \\
        &= o(x-x_0 + o(x-x_0)) = o(x-x_0)
    \end{align*}
    Quindi riprendendo l'espressione con $g(f(x))$ possiamo dire che
    \begin{align*}
    g(f(x)) &= g(f(x_0)) + g'(f(x_0))(\mathunderline{red}{f(x)-f(x_0)}) + \mathunderline{blue}{o(f(x)-f(x_0))} \\
     &= g(f(x_0)) + g'(f(x_0))(\mathunderline{red}{f'(x_0)(x-x_0) + o(x-x_0)}) + \mathunderline{blue}{o(x-x_0)} \\
    &= g(f(x_0)) + g'(f(x_0))(f'(x_0)(x-x_0)) + g'(f(x_0))o(x-x_0) + o(x-x_0) \\
    &= g(f(x_0)) + g'(f(x_0))(f'(x_0)(x-x_0)) + o(x-x_0)
    \end{align*}
    Portanto il termine $g(f(x_0))$ alla sinistra scopriamo che
    \[
    g(f(x)) - g(f(x_0)) = g'(f(x_0))(f'(x_0)(x-x_0)) + o(x-x_0) \text{ per } x\to x_0
    \]
    Con questa informazione possiamo usare la definizione di derivata 
    \begin{align*}
       \lim_{x\to x_0} \dfrac{g(f(x)) - g(f(x_0))}{x-x_0} &= \lim_{x\to x_0} \dfrac{g'(f(x_0))(f'(x_0)(x-x_0)) + o(x-x_0)}{x-x_0} \\
     &= \lim_{x\to x_0} \left(\dfrac{g'(f(x_0))(f'(x_0)(x-x_0))}{x-x_0} + \dfrac{o(x-x_0)}{x-x_0} \right) \\
    &= g'(f(x_0))f'(x_0) + 0\\
    &= g'(f(x))f'(x)
    \end{align*}
\end{proof}


\addcontentsline{toc}{subsection}{Esercizi sulla Derivata di Funzioni Composte}
Vediamo qualche esercizio per impraticarci con i teoremi visti fino ad ora.
\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
        f(x) = \frac{1+x^2}{1+x}
    \]
\end{esercizio}
Notiamo subito che è una frazione, quindi usiamo la regola del quoziente:
\begin{align*}
    \frac{d}{dx} \left(\frac{1+x^2}{1+x}\right) &= \dfrac{(1+x^2)'(1+x) - (1+x^2)(1+x)'}{(1+x)^2} \\
&= \dfrac{(2x)(1+x) - (1+x^2)(1)}{(x+1)^2} \\
&= \dfrac{2x + 2x^2 -1-x^2}{(x+1)^2} \\
&= \dfrac{x^2+2x  -1}{(x+1)^2}
\end{align*}

\newpage
\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
        f(x) = \left(\frac{1+x^2}{1+x}\right)^5
    \]
\end{esercizio}
Notiamo che questo esercizio è molto simile a quello precedente, però c'è la potenza 5 che rovina i piani. Quindi possiamo fare il teorema delle funzioni composte, infatti se scelgo $y= \left(\frac{1+x^2}{1+x}\right)$, in questo modo abbiamo 
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle g(y) = y^5 &\displaystyle h(x) = \frac{1+x^2}{1+x}
\end{array}
\]
Dato che la funzione originale non è altro che $f(x)=g(h(x))$, la derivata di $h(x)$ la possiamo utilizzare quella dell'esercizio scorso. Quindi calcoliamo le singole derivate
\[\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle g'(y) = 5y^4 &\displaystyle h'(x) = \dfrac{x^2+2x  -1}{(x+1)^2}
\end{array}
\]
Di conseguenza:
\[
\frac{d}{dx}(f(x)) = \frac{d}{dx}(g(h(x))) = g'(h(x))\cdot h'(x) = 5\left(\frac{1+x^2}{1+x}\right)^4\cdot \dfrac{x^2+2x  -1}{(x+1)^2}
\]
Scrivendo meglio 
\[
\frac{d}{dx}(f(x)) = \dfrac{5(1+x^2)^4(x^2+2x  -1)}{(x+1)^6}
\]
\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
        f(x) = \cot(x) = \dfrac{\cos(x)}{\sin(x)}
    \]
\end{esercizio}
Questo esercizio è semplicemente una frazione
\begin{align*}
    \frac{d}{dx} \left(\dfrac{\cos(x)}{\sin(x)}\right) &= \dfrac{(\cos(x))'(\sin(x)) - (\cos(x))(\sin(x))'}{(\sin(x))^2} \\
    &= \dfrac{(-\sin(x))(\sin(x)) - (\cos(x))(\cos(x))}{\sin^2(x)} \\
    &= \dfrac{-\sin^2(x) - \cos^2(x)}{\sin^2(x)} = \dfrac{-1}{\sin^2(x)}
\end{align*}

\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
        f(x) = (x\cot(x))^2 
    \]
\end{esercizio}
Notiamo che sembra di essere nella situazione simile a prima, quindi posso scegliere
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle g(y) = y^2 &\displaystyle h(x) = x\cot(x)
\end{array}
\]
Però notiamo che la derivata di $g(y)$ è molto semplice, infatti $g'(y) = 2y$, però la derivata di $h(x)$ dobbiamo fare qualche conto in più, infatti dobbiamo usare la regola del prodotto 
\[
h'(x) = (x)'\cot(x) + x(\cot(x))' = 1\cdot \frac{\cos(x)}{\sin(x)} + x \cdot \left(\frac{-1}{\sin^2(x)}\right) = \frac{\cos(x)\sin(x) - x}{\sin^2(x)}
\]
Ora possiamo usare la regola della composizione
\[
\frac{d}{dx}(f(x)) = \frac{d}{dx}(g(h(x))) = g'(h(x))h'(x) = 2(x\cot(x)) \cdot \frac{\cos(x)\sin(x) - x}{\sin^2(x)}
\]
Che scrivendo meglio
\[
\frac{d}{dx}(f(x))=  \dfrac{2x\cos(x)(\cos(x)\sin(x) - x)}{\sin^3(x)}  
\]

\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
    f(x) = \sqrt{x+\sqrt{x+\sqrt{x}}}
    \]
\end{esercizio}
Rimandendo con la stessa logica potremmo scegliere
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle g(y) = \sqrt{y} &\displaystyle h(x) = x+\sqrt{x+\sqrt{x}}
\end{array}
\]
La derivata di $g(y)$ sappiamo che è $g'(y) = \frac{1}{2\sqrt{x}}$, ma la dobbiamo invece calcolare la derivata di $h(x)$.
\[
h'(x) = \frac{d}{dx}\left(x+\sqrt{x+\sqrt{x}}\right) = 1 + \left(\sqrt{x+\sqrt{x}}\right)'
\]
Però noi non sappiamo quale è la derivata di quella radice, quindi dobbiamo riapplicare il criterio della composta, e scegliamo
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle j(y) = \sqrt{y} &\displaystyle k(x) = x+\sqrt{x}
\end{array}
\] 
Di questo possiamo calcolare la derivata 
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle j'(y) = \frac{1}{2\sqrt{y}} &\displaystyle k'(x) = 1+\frac{1}{2\sqrt{x}}
\end{array}
\] 
Di conseguenza
\[
\frac{d}{dx}(j(k(x))) = j'(k(x))k'(x) =   \frac{1}{2\sqrt{x+\sqrt{x}}}\cdot \left(1+\frac{1}{2\sqrt{x}}\right) =  \frac{2\sqrt{x}+1}{4\sqrt{x}\sqrt{x+\sqrt{x}}} 
\]
Quindi ora possiamo calcolare $h'(x)$
\[
h'(x) = 1 + \left(\sqrt{x+\sqrt{x}}\right)' = 1 +\frac{2\sqrt{x}+1}{4\sqrt{x}\sqrt{x+\sqrt{x}}} 
\]
Ma quindi ora possiamo calcolare $f'(x)$
\[
\frac{d}{dx}(f(x)) = \frac{d}{dx}(g(h(x))) = g'(h(x)) h'(x) = \frac{1}{2\sqrt{x+\sqrt{x+\sqrt{x}}}} \cdot \left(1 +\frac{2\sqrt{x}+1}{4\sqrt{x}\sqrt{x+\sqrt{x}}} \right) 
\]

\newpage
\addcontentsline{toc}{subsection}{Derivata di Potenze di Funzioni}
\begin{teorema}{Derivata di Potenze di Funzioni}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f,g: I \to \mathbb{R}$ con $f$ e $g$ derivabili in $I$, se $f(x) > 0$ $\forall x \in I$, allora
    \[
        \dfrac{d}{dx}\left(f(x)^{g(x)}\right) = f(x)^{g(x)}\left(g'(x)\log(f(x)) + g(x)\cdot \dfrac{f'(x)}{f(x)}\right)
    \]
\end{teorema}
\begin{proof}
    Per risolvere questi esercizi è necessario usare la così detta \textbf{differenziazione logaritmica}, infatti se definiamo 
    \[
    f(x)^{g(x)} = L(x)
    \] 
    Allora la derivata che noi cerchiamo è $L'(x)$. Per calcolarla possiamo applicare il logaritmo ad entrambi i membri e poi calcolare la derivata. 
    \begin{align*}
        \log\left(f(x)^{g(x)}\right) &= \log(L(x)) \\ 
        g(x) \cdot \log(f(x)) &= \log(L(x))
    \end{align*}
    Ora possiamo applicare la derivata a destra e a sinistra. Ricordandoci che la derivata di $\log(f) = \dfrac{f'}{f}$, e questo lo si evince dalla regola della catena. 
    \begin{align*}
        \dfrac{d}{dx}\left(g\cdot \log(f)\right) &= \dfrac{d}{dx}\log(L) \\
        g'\cdot \log(f) +  g \cdot (\log(f))'  &= \dfrac{L'}{L} \\
        g'\cdot \log(f) +  g \cdot \dfrac{f'}{f}  &= \dfrac{L'}{L} \\
        L\left(g'\cdot \log(f) +  g \cdot \dfrac{f'}{f}\right) &= L' \\
        f^g\left(g'\cdot \log(f) +  g \cdot \dfrac{f'}{f}\right) &= L'
    \end{align*}
    Ma all'inizio avevamo detto che $L'$ è la derivata della funzione originale, e quindi abbiamo trovato la derivata di $f(x)^{g(x)}$.
\end{proof}

\begin{esercizio}{}{}
    Calcolare la derivata di 
    \[
    h(x) = x^x \;\;\;\;\; \text{ per } x>0
    \]
\end{esercizio}
Dato che $h(x) = x^x$, possiamo definire (rimanendo con la notazione del teorema) $f(x) = x$ e $g(x)=x$, quindi possiamo calcolare le derivate 
\[
\begin{array}{c @{\qquad}@{\qquad} c}
    f'(x) = 1 & g'(x) = 1
\end{array}
\]

E quindi usando la formula del teorema della potenza di funzioni
\begin{align*}
    \dfrac{d}{dx}(x^x) = f(x)^{g(x)}\left(g'(x)\log(f(x)) + g(x)\cdot \dfrac{f'(x)}{f(x)}\right) &= x^{x}\left(1\log(x) + x\cdot \dfrac{1}{x}\right) \\
    &= x^{x}\left(\log(x) +1\right)
\end{align*}

\newpage
\addcontentsline{toc}{subsection}{Derivabilità della funzione Inversa}
\begin{teorema}{Derivabilità della funzione Inversa}{}
     Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, se $f$ è derivabile e invertibile, sia $x_0 \in I$, se $f(x_0) \neq 0$ allora definiamo $y_0 = f(x_0)$ e vale
     \[
     \frac{d}{dx}(f^{-1}(y_0)) = \frac{1}{f'(x_0)}
     \]
\end{teorema}
\begin{proof}
    Usiamo la definizione di derivata
    \[
    \frac{d}{dy}(f^{-1}(y_0)) = \lim_{y\to y_0} \frac{f^{-1}(y) - f^{-1}(y_0) }{y-y_0}
    \]
    Ora possiamo applicare un cambio di variabile con $y=f(x)$, allora sappiamo che $y_0=f(x_0)$, e di conseguenza $x=f^{-1}(y)$ e $x_0=f^{-1}(y_0)$ allora 
    \[
    \lim_{y\to y_0} \frac{f^{-1}(y) - f^{-1}(y_0) }{y-y_0} = \lim_{x\to x_0} \frac{x - x_0}{f(x) - f(x_0)} = \lim_{x\to x_0} \frac{1}{\frac{f(x) - f(x_0)}{x - x_0} } = \frac{1}{f'(x_0)} = \frac{1}{f'(f^{-1}(y_0))}
    \] 
\end{proof}

Proviamo a derivare la derivata della funzione inversa in maniera geometrica. Infatti noi sappiamo che la derivata è il limite del coefficiente angolare della retta tangente, e quindi se il coefficiente della retta lo calcoliamo con $\dfrac{\Delta y}{\Delta x}$ se lo portiamo il limite lo scriviamo come $\dfrac{dy}{dx}$, è per questo motivo che si scrive anche in questo modo la derivata. Quindi se prendiamo la funzione $f(x) = x^2$ e la sua inversa $f^{-1}(y)=\sqrt{y}$ (Consideriamo solo valori positivi di $x$).

\begin{figure}[h!]
\centering

% --- Primo grafico ---
\begin{minipage}{0.48\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=6.2cm,
    height=5.3cm,
    xmin=0, xmax=2.5,
    ymin=0, ymax=4.5,
    axis lines=middle,
    axis line style={-stealth, thick},
    samples=300,
    xlabel={$x$}, ylabel={$y$},
    xtick={1,2}, ytick={1,2,3,4},
    clip=false
]

\addplot[blue, ultra thick, domain=0:2.2] {x^2};
\node[blue] at (axis cs:1.7,4.0) {$f(x)$};

\addplot[only marks, mark=*] coordinates {(1,1) (2,4)};

\addplot[red, thick, domain=0.67:2.2] {3*x - 2};

\draw[green!60!black, thick]
    (1,1) -- (2,1) -- (2,4);

\node[green!60!black] at (1.5,0.7) {$dx$};
\node[green!60!black] at (2.3,2.5) {$dy$};

\end{axis}
\end{tikzpicture}
\end{minipage}
%
\begin{minipage}{0.48\textwidth}
\centering
% --- Secondo grafico: f^{-1}(y)=sqrt(y) ---
\begin{tikzpicture}
\begin{axis}[
    width=6.2cm,  
    height=5.3cm,      % <-- più stretto
    xmin=0, xmax=4.5,
    ymin=0, ymax=2.5,     % <-- più basso = radice meno allungata   
    axis lines=middle,
    axis line style={-stealth, thick},
    samples=300,
    xlabel={$y$}, ylabel={$x$},
    xtick={1,2,3,4}, ytick={1,2},
    clip=false
]

\addplot[blue, ultra thick, domain=0:4.5] ({x}, {sqrt(x)});
\node[blue] at (axis cs:3.0,2.1) {$f^{-1}(y)$};

\addplot[only marks, mark=*] coordinates {(1,1) (4,2)};

% retta secante dell'inverso estesa
\addplot[red, thick, domain=0:4.5] ({x}, {1/3*x + 2/3});

\draw[green!60!black, thick]
    (1,1) -- (4,1) -- (4,2);

\node[green!60!black] at (2.5,0.7) {$dy$};
\node[green!60!black] at (4.3,1.5) {$dx$};

\end{axis}
\end{tikzpicture}

\end{minipage}
\end{figure}

Possiamo notare che il coefficiente angolare della retta di $f^{-1}(y)$ non è altro che $\displaystyle\frac{\Delta x}{\Delta y}$, dato che gli assi sono invertiri rispetto alla funzione originale. Quindi se potiamo al limite questa informazione sappiamo che la derivata di $f^{-1}(y)$ la calcoliamo come $\displaystyle\frac{dx}{dy}$. Noi però conosciamo solamente la derivata di $f(x)$ che è $\displaystyle\frac{dy}{dx}$, quindi dato che è una frazione possiamo calcolare la derivata di $f^{-1}(y)$ come
\[
 (f^{-1}(y_0))' = \dfrac{dx}{dy} = \dfrac{1}{\dfrac{dy}{dx}} = \frac{1}{f'(x_0)} 
\]
\newpage
Esercitiamoci con qualche funzione inversa molto importante, partiamo con
\begin{esercizio}{}{}
    Calcolare la derivata della funzione inversa di 
    \[
    f(x) = \sin(x)
    \]
\end{esercizio}
In sostanza l'esercizio ci sta chiedendo di calcolare la derivata di $f^{-1}(y)=\arcsin(y)$, quindi in primis dobbiamo calcolare la derivata di $f(x)$
\[
f'(x) = \cos(x)
\] 
Quindi 
\[
\frac{d}{dy}(\arcsin(y)) = \frac{1}{\cos(x)}
\]
Però ora dobbiamo tornare alla variabile y, e quindi dovremmo sostituire al posto di $x=\arcsin(y)$, ma dopo verrebbe un denominatore troppo complesso. Quindi per semplificarci la vita, cerchiamo di riscrivere $\cos(x)$ in funzione di $\sin(x)$, in modo tale che dopo possiamo sostituire $\sin(x)=y$ (dato che prima dovevamo sostituire $x=\arcsin(y))$. Quindi usando le formule fondamentali della trigonometria abbiamo che
\[
\frac{d}{dy}(\arcsin(y)) = \dfrac{1}{\cos(x)} = \dfrac{1}{\sqrt{1-\sin^2(x)}} = \dfrac{1}{\sqrt{1-y^2}} 
\]
\begin{esercizio}{}{}
    Calcolare la derivata della funzione inversa di 
    \[
    f(x) = \tan(x)
    \]
\end{esercizio}
Quindi ci stanno chiedendo di calcolare la derivata di $f^{-1}(y)=\arctan(y)$, quindi in primi sobbiamo calcolare la derivata di $f(x)$
\begin{align*}
f'(x) = \left(\dfrac{\sin(x)}{\cos(x)}\right)' &= \dfrac{\cos(x)\cos(x) - \sin(x)(-\sin(x))}{\cos^2(x)}\\
&= \dfrac{\cos^2(x)}{\cos^2(x)} + \dfrac{\sin^2(x)}{\cos^2(x)} \\
&= 1+\tan^2(x)
\end{align*}
Ho scritto la derivata così, e non $\frac{1}{\cos^2(x)}$, per evitare di avere lo stesso problema dell'esercizio di prima. Infatti prima abbiamo dovuto riscrivere il $\cos(x)$ in funzione del $\sin(x)$ in modo dale da sostituire $y=\sin(x)$. Quindi ho già scritto la derivata di $f(x)$ in funzione di $\tan(x)$, così da evitare il passaggio che abbiamo fatto sopra (e anche perchè non era così semplice per questo esercizio) e poter sostituire subito $y=\tan(x)$. Quindi la derivata di $\arctan(y)$ è
\[
\dfrac{d}{dy}(\arctan(y)) = \dfrac{1}{f'(x)} = \dfrac{1}{1+\tan^2(x)} = \dfrac{1}{1+y^2}
\]


\addcontentsline{toc}{subsection}{Definizione di Derivata Seconda}
\begin{definizione}{Derivata Seconda}{}
     Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, se $f$ è derivabile, sia $x_0 \in I$, se
     \[
     \exists \lim_{x\to x_0} \dfrac{f'(x)-f'(x_0)}{x-x_0} = l \in \mathbb{R}
     \] 
     Allora tale limite si dice \textbf{Derivata Seconda di $f$ in $x_0$} e si indica come
     \[
     f''(x_0) = \lim_{x\to x_0} \dfrac{f'(x)-f'(x_0)}{x-x_0}
     \]
\end{definizione}
Come con le derivate prime, esistono varie notazioni analoghe a quelle già viste
\[
\begin{array}{c@{\qquad} c @{\qquad} c}
    f''(x_0) & \dfrac{d^2f}{dx^2} & \ddot{f}(x_0) 
\end{array}
\]
\[
\begin{array}{c@{\qquad} @{\qquad} c}
    D^2[f(x_0)] & \dfrac{\partial^2f}{\partial x^2}
\end{array}
\]

\addcontentsline{toc}{subsection}{Definizione di Derivata n-esima}
\begin{definizione}{Derivata n-esima}{}
     Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, se $f$ è derivabile (n-1) volte, sia $x_0 \in I$, se
     \[
     \exists \lim_{x\to x_0} \dfrac{f^{(n-1)}(x)-f^{(n-1)}(x_0)}{x-x_0} = l \in \mathbb{R}
     \] 
     Allora tale limite si dice \textbf{Derivata n-esima di $f$ in $x_0$} e si indica come
     \[
     f^{(n)}(x_0) = \lim_{x\to x_0} \dfrac{f'(x)-f'(x_0)}{x-x_0}
     \]
\end{definizione}
Come con le derivate prime e seconde, esistono varie notazioni 
\[
\begin{array}{c@{\qquad} c @{\qquad} c @{\qquad} c}
    f^{(n)}(x_0) & \dfrac{d^nf}{dx^n}  &D^n[f(x_0)] & \dfrac{\partial^nf}{\partial x^n}
\end{array}
\]

\textbf{N.B.} nella notazione standard, il grado della derivata va indicato tra parentesi tonde per non scambiarlo con l'elevazione alla n
\[
\dfrac{d^nf}{dx^n} = f^{(n)}(x_0) \neq f^{n}(x_0) = \left(f(x_0)\right)^n
\]
Per convenzione si è deciso che la \textbf{Derivata Zeresima} fosse la funzione originale, quindi 
\[
f^{(0)}(x) = f(x)
\]

\addcontentsline{toc}{subsection}{Definizione di $C^n$}
\begin{definizione}{$C^n$}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $\forall n \in \mathbb{N}_0$
    \[
    C^n (I) = \{f:I\to \mathbb{R} :  f^{(n)} \in C^0(I)\}
    \]
\end{definizione}
\textbf{N.B.}  $f$ deve essere derivabile $n-1$ volte affinchè abbia senso.

\newpage

\addcontentsline{toc}{subsection}{Definizione di Massimi e Minimi Globali e Locali}
\begin{definizione}{Massimi e Minimi Globali e Locali}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$ allora diciamo che
    \begin{itemize}
        \item $x_0$ è \textbf{massimo globale} (o assoluto) se vale
        \[
        f(x_0) \geq f(x) \;\;\; \forall x \in I
        \]
         \item $x_0$ è \textbf{minimo globale} (o assoluto) se vale
        \[
        f(x_0) \leq f(x) \;\;\; \forall x \in I
        \]
        \item $x_0$ è \textbf{massimo locale} (o relativo) se $\exists \delta >0$ tale che
        \[
        f(x_0) \geq f(x) \;\;\; \forall x \in I \cap \{x_0-\delta, x_0 +\delta\}
        \]
         \item $x_0$ è \textbf{minimo locale} (o relativo) se $\exists \delta >0$ tale che
        \[
        f(x_0) \leq f(x) \;\;\; \forall x \in I \cap \{x_0-\delta, x_0 +\delta\}
        \]
        \item $x_0$ è \textbf{massimo locale stretto} se $\exists \delta >0$ tale che
        \[
        f(x_0) > f(x) \;\;\; \forall x \in I \cap \{x_0-\delta, x_0 +\delta\} \setminus \{0\}
        \]
         \item $x_0$ è \textbf{minimo locale stretto} se $\exists \delta >0$ tale che
        \[
        f(x_0) < f(x) \;\;\; \forall x \in I \cap \{x_0-\delta, x_0 +\delta\} \setminus \{0\}
        \]
         \item $x_0$ è \textbf{estremo relativo} se è punto di massimo relativo oppure minimo relativo
         \item $x_0$ è punto \textbf{critico} (o \textbf{stazionario}) se $\exists f'(x_0) = 0$
        
    \end{itemize}
\end{definizione}
Dopo questo elenco enorme di definizioni, capiamo come sono legate tra di loro


\addcontentsline{toc}{subsection}{Teormea di Fermat}
\begin{teorema}{Teorema di Fermat}{}
    Siano $a,b \in \mathbb{R}$, $a<b$, $f:(a,b) \to \mathbb{R}$, $x_0 \in (a,b)$. Se $x_0$ è un punto di estremo relativo per $f$, se $f$ è derivabile in $x_0$ allora $f'(x_0) = 0$.
\end{teorema}
\begin{proof}
    Dato che per ipotesi $f$ è derivabile in $x_0$, allora sappiamo che
    \begin{equation}\label{eq:fermat}
    f'(x_0) = f'_+(x_0)= f'_-(x_0)        
    \end{equation}

    Dato che $x_0$ è un estremo relativo supponiamo che sia un massimo (per il minimo la dimostrazione è analoga), quindi per la definizione di massimo relativo sappiamo che $\exists \delta >0$ tale che
    \[
    f(x_0) \geq f(x) \;\;\; \forall x \in (x_0-\delta, x_0+\delta) 
    \]
    Quindi deduciamo che
    \[
    f(x)-f(x_0) \leq 0  \;\;\; \forall x \in (x_0-\delta, x_0+\delta) 
    \]
    Ora analiziamo il rapporto incrementale, e dato che $f(x)-f(x_0) \leq 0$, il segno del rapporto incrementale è dato solo ed unicamente dal denominatore ($x-x_0$). Quindi se $x>x_0$ allora il rapporto incrementale avrà segno negativo, mentre per $x<x_0$ il rapporto incrementale avrà segno positivo. Però dato che noi sappiamo che $ f(x)-f(x_0) \leq 0$ solamente per $x \in (x_0-\delta, x_0+\delta) $, allora il rapporto sarà negativo per  $x \in x>x_0 \land(x_0-\delta, x_0+\delta)$ cioè $x \in (x_0, x_0+\delta)$, ragionamento analogo per l'altra casistica. 
    \[
    \dfrac{f(x)-f(x_0)}{x-x_0} = \begin{cases}
        \geq 0 & \text{per } x \in (x_0-\delta,x_0) \\
        \leq 0 & \text{per } x \in (x_0,x_0+\delta) \\
    \end{cases}
    \]
    Ma quindi per il teorema della permanenza del segno deduciamo che
    \[
    \begin{array}{c@{\qquad}c}
        \displaystyle f'_-(x_0) = \lim_{x\to x_0^-} \dfrac{f(x)-f(x_0)}{x-x_0} \geq 0 &\displaystyle f'_+(x_0) = \lim_{x\to x_0^+} \dfrac{f(x)-f(x_0)}{x-x_0} \leq 0
    \end{array}
    \]
    Ma per (\ref{eq:fermat}) possiamo dedurre che
    \[
        0 \leq f'_-(x_0) = f'(x_0) = f'_+(x_0) \leq 0
    \]
    \[
    0 \leq  f'(x_0)  \leq 0 
    \]
    \[
    f'(x_0)  = 0 
    \]
\end{proof}

\addcontentsline{toc}{subsection}{Teormea di Rolle}
\begin{teorema}{Teorema di Rolle}{}
    Siano $a,b \in \mathbb{R}$, $a<b$, $f:[a,b] \to \mathbb{R}$, $f \in C^0([a,b])$ e derivabile su $(a,b)$. Se $f(a)=f(b)$ allora $\exists x_0 \in (a,b)$ tale che $f'(x_0)=0$
\end{teorema}

\begin{proof}
    Dato che per ipotesi $f \in C^0([a,b])$, allora per il teorema di Weierstrass sappiamo che $\exists x_{min}, x_{max} \in [a,b]$ punti di minimo/massimo. Dividiamo in due casistiche
    \begin{itemize}
        \item se $x_{min} \in (a, b)$ oppure $x_{max} \in (a, b)$, allora sono punti di estremo di $f$, ma dato che $f$ è derivabile in $(a,b)$ (per ipotesi), allora per il teorema di Fermat sappiamo che 
        \[
        \begin{array}{c@{\qquad}c @{\qquad} c}
            f'(x_{min}) = 0 & \text{oppure} & f'(x_{max}) = 0
        \end{array}
        \]
        E quindi abbiamo verificato il teorema
        \item se $x_{min}, x_{max} \in \{a,b\}$ allora può solamente succedere che $x_{min} = a \land x_{max} = b $ oppure $x_{min} = b \land x_{max} = a $, ma in entrambi i casi, dato che $f(a)=f(b)$ scopriamo che
        \[
        f(x_{min}) = f(x_{max})
        \]
        Ma dato che $f(x_{min})$ e $f(x_{max})$ sono gli estremi sappiamo che
        \[
        f(x_{min}) \leq f(x) \leq f(x_{max}) \;\;\; \forall x \in [a,b]
        \]
        Ma questo vuol dire che 
        \[
        f(x)=f(x_{min})= f(x_{max}) \;\;\; \forall x \in [a,b]
        \]
        Questo può accadere soltanto se $f(x)$ è costante in $[a,b]$. Ma dato che $f(x)=c \in \mathbb{R}$ allora $f'(x) = 0$ $\forall x \in [a,b]$, e quindi abbiamo verificato il teorema anche in questo caso.
    \end{itemize}
\end{proof}


\addcontentsline{toc}{subsection}{Teormea di Lagrange}
\begin{teorema}{Teorema di Lagrange}{}
    Siano $a,b \in \mathbb{R}$, $a<b$, $f:[a,b] \to \mathbb{R}$, $f \in C^0([a,b])$ e derivabile su $(a,b)$. Allora $\exists x_0 \in (a,b)$ tale che 
    \[
    f'(x_0) = \frac{f(b)-f(a)}{b-a}
    \]
\end{teorema}
\begin{proof}
    Definiamo una nuova funzione $g:[a,b] \to \mathbb{R}$ definita come
    \[
    g(x) = f(x) - \left(f(a) + (x-a)\dfrac{f(b)-f(a)}{b-a}\right)
    \]
    Notiamo che $g\in C^0([a,b])$ dato che lo è anche $f(x)$ e la nuova parte è continua in $\mathbb{R}$. Per lo stesso ragionamento è anche derivabile in $(a,b)$, infatti la derivata è
    \begin{align*}
    g'(x) &= \left( f(x) - \left(f(a) + (x-a)\dfrac{f(b)-f(a)}{b-a}\right)\right)'\\
    &=\left( f(x) - \left(f(a) + x\dfrac{f(b)-f(a)}{b-a} - a\dfrac{f(b)-f(a)}{b-a}\right)\right)' \\
     &= f'(x) -\left(0 + \dfrac{f(b)-f(a)}{b-a} - 0\right) \\
     &= f'(x) - \dfrac{f(b)-f(a)}{b-a} 
    \end{align*}
    Poi scopriamo anche che
    \[
    g(a) = f(a) - \left(f(a) + (a-a)\dfrac{f(b)-f(a)}{b-a}\right) = f(a) - f(a) + 0 = 0
    \]
    \[
     g(b) = f(b) - \left(f(a) + (b-a)\dfrac{f(b)-f(a)}{b-a}\right) = f(b) - (f(a) + f(b)-f(a)) = 0
    \]
    Ma quindi possiamo applicare il teorema di Rolle, e quindi $\exists x_0 \in (a,b)$ tale che
    \[
     g'(x_0) = 0
    \]
    \[
f'(x_0) - \dfrac{f(b)-f(a)}{b-a} = 0
    \]
\[
    f'(x_0) = \dfrac{f(b)-f(a)}{b-a}
    \]
    
\end{proof}

\addcontentsline{toc}{subsection}{Corollario del T. Lagrange: Carattterizzazione delle Costanti}
\begin{corollario}{Carattterizzazione delle Costanti}{}
    Siano $a,b \in \mathbb{R}$, $a<b$, $f:[a,b] \to \mathbb{R}$, $f \in C^0([a,b])$ e derivabile su $(a,b)$. Allora 
    \[
    f'(x) = 0 \iff f(x)=c \in \mathbb{R} \;\;\; \forall x \in [a,b]
    \]
\end{corollario}
\begin{proof}
    L'implicazione $(\impliedby)$ è ovvia per l'algebra delle derivate. Mentre per l'implicazione $(\implies)$ dobbiamo usare il teorema di Lagrange. 
    
    Se prendiamo $x_1, x_2 \in [a, b]$, con $x_1 < x_2$, notiamo che $f \in C^0([x_1,x_2])$ e che $f$ è derivabile in $(x_1,x_2)$. Quindi possiamo applicare il teorema di Lagrange, pertanto $\exists x_0 \in (x_1, x_2)$ tale che
    \[
    f'(x_0) = \dfrac{f(x_2)-f(x_1)}{x_2-x_1}
    \] 
    Ma per ipotesi sappiamo che $f'(x) = 0$ $\forall x \in [a,b]$, quindi 
    \[
    \dfrac{f(x_2)-f(x_1)}{x_2-x_1} = 0
    \]
    \[
    f(x_2)-f(x_1) = 0 \implies f(x_2) = f(x_1)
    \]
    Dato che vale $\forall x_1, x_2 \in [a,b]$ allora $f(x_2) = f(x_1)$ vale sempre, e questo può accadere solamente se $f(x) = c$, per un qualche $c\in \mathbb{R}$. 
\end{proof}

Questa caratterizzazione può aiutarci per scoprire se una funzione è costante. Infatti vediamo 
\begin{esercizio}{}{}
    Dimostrare che per $x>0$ vale
    \[
    \arctan(x) + \arctan\left(\dfrac{1}{x}\right) = \dfrac{\pi}{2}
    \]
\end{esercizio}

Notiamo che abbiamo una funzione uguale ad una costante quindi, se fosse vero, dovremmo avere che la derivata della funzione sia 0 per ogni punto dell'intervallo. Però per ora non possiamo ancora applicare il teorema, infatti l'intervallo che abbiamo $(0, +\infty)$ è aperto, mentre per applicare il teorema è necessario che sia un intervallo chiuso. Quindi possiamo prendere $\varepsilon$ e $M$ con $0< \varepsilon < M$, con questo prendiamo l'intervallo $[\varepsilon, M]$, e dato che $f\in C^0(\mathbb{R}\setminus\{0\})$ allora sarà anche in $[\varepsilon, M]$, ragionamento analogo per la derivabilità. Ora possiamo applicare  il corollario di Lagrange.
\begin{align*}
    \dfrac{d}{dx}\left(\arctan(x) + \arctan\left(\dfrac{1}{x}\right)\right) &= \dfrac{1}{1+x^2} + \dfrac{1}{1+\left(\dfrac{1}{x}\right)^2} \cdot \frac{-1}{x^2} \\
    &= \dfrac{1}{1+x^2} + \dfrac{x^2}{x^2+1} \cdot \frac{-1}{x^2} \\
     &= \dfrac{1}{1+x^2} + \dfrac{-1}{x^2+1}  = 0
\end{align*}
Quindi per il corollario sappiamo che la nostra funzione è costante nell'intervallo $[\varepsilon, M]$, però dato che posso scegliere un qualsiasi $\varepsilon>0$ e $M>0$, allora la funzione è costante nell'intervallo $(0,+\infty)$. Per trovare la costante basta calcolare la funzione in un qualsiasi punto, per esempio in 1.
\[
\arctan(1) + \arctan\left(\dfrac{1}{1}\right) = \frac{\pi}{4} + \frac{\pi}{4} = \frac{\pi}{2}
\]


\addcontentsline{toc}{subsection}{Corollario del T. Lagrange: Relazione Derivata e Limite di Derivata}
\begin{corollario}{Relazione Derivata e Limite di Derivata}{}
    Siano $a,b \in \mathbb{R}$, $a<b$, $f:[a,b] \to \mathbb{R}$, $f \in C^0([a,b])$ e derivabile su $(a,b)$. Allora se
    \[
    \exists \lim_{x\to a^+} f'(x) = l \in \mathbb{R} \implies \exists f'_+(a)=l
    \]
\end{corollario}
\begin{proof}
    Notiamo che la funzione rispetta tutti le ipotesi del teorema di Lagrange, e quindi possiamo applicarlo, però anzichè applicarlo in $(a,b)$, scegliamo un $x$ variabile in $(a, b)$, e prendere l'intervallo $(a,x)$ che chiaramente anche su questo intervallo si può applicare il teorema di Lagrange dato che $(a,x) \subset (a,b)$. Quindi notiamo che $\exists z(x) \in (a,x)$ tale che 
    \begin{equation}\label{eq:lag1}
    f'(z(x)) = \dfrac{f(x)-f(a)}{x-a}    
    \end{equation}
    Notiamo che 
    \[
    a < z(x) < x
    \]
    Quindi se portiamo al limite per $x\to a^+$, per il teorema dei carabinieri sappiamo che
    \[
    \lim_{x\to a^+}z(x) = a
    \]
   Con  (\ref{eq:lag1}) possiamo notare che
    \[
         \lim_{x\to a^+} f(x) = l \implies \lim_{x\to a^+} f(z(x)) = l \implies \lim_{x\to a^+} \dfrac{f(x)-f(a)}{x-a}  = l \implies f'_+(a) = l    
     \]
\end{proof}


\textbf{N.B.} se vi sembra ovvio questo risultato beh non lo è affatto, infatti vediamo un esempio che mostra che non è ovvio affatto.

Prendiamo 
\[
f(x) = \begin{cases}
    x^2\sin\left(\dfrac{1}{x}\right) & x \in (0,1] \\
    0 & x = 0
\end{cases}
\]
Possiamo notare che la funzione è continua in $[0,1]$ ed è derivabile in $(0, 1)$, infatti la sua derivata è 
\[
    f'(x) =2x\sin\left(\frac{1}{x}\right) -\cos\left(\frac{1}{x}\right) \;\;\; \;\; \forall x \in (0,1) 
\]
\[
f'(x) = \lim_{x\to 0^+} \frac{f(x)-f(0)}{x-0} = \lim_{x\to 0^+} \frac{x^2\sin\left(\dfrac{1}{x}\right)-0}{x} = \lim_{x\to 0^+} x\sin\left(\dfrac{1}{x}\right) = 0 \;\; \text{per } x = 0
\]
Notiamo quindi che la derivata in $x=0$ esiste e vale proprio $f'(0)=0$. Però se facciamo il limite della derivata abbiamo che
\[
\lim_{x\to 0^+} f'(x) = \lim_{x\to 0^+} \left(2x\sin\left(\frac{1}{x}\right) -\cos\left(\frac{1}{x}\right)\right) = 0 -\lim_{x\to0^+}\cos\left(\frac{1}{x}\right) = \lim_{t\to+\infty} \cos(t)
\]
Però questo limite non esiste, dato che il coseno oscilla all'infinito. Quindi in questo caso, la funzione ha derivata finita in $x=0$, però non esiste il limite della funzione derivata per $x\to 0^+$. Difatto, esiste la derivata destra, ma non esiste il limite destro della derivata.

\addcontentsline{toc}{subsection}{Teorema di Cauchy}
\begin{teorema}{Teorema di Cauchy}{}
    Siano $a,b \in \mathbb{R}$, $a<b$, $f,g:[a,b] \to \mathbb{R}$, $f,g \in C^0([a,b])$ e derivabili su $(a,b)$. Allora $\exists x_0 \in (a,b)$ tale che 
    \[
    f'(x_0)(g(b)-g(a)) = g'(x_0)(f(b)-f(a))
    \]
    E se $g(x_0)\neq 0$ e $g(b)\neq g(a)$ possiamo scrivere anche
    \[
    \dfrac{f'(x_0)}{g'(x_0)} = \dfrac{f(b)-f(a)}{g(b)-g(a)}
    \]
\end{teorema}
Ora facciamo qualche esercizio sui teoremi delle derivate visti finora

\begin{esercizio}{}{}
    Siano $f,g\in C^0([0,+\infty))$ derivabili in $(0,+\infty)$. Dimostrare che, se $f(0)\geq g(0)$ e $f'(x) \geq g'(x)$ per ogni $x>0$, allora $f(x)\geq g(x)$ per ogni $x\geq 0$.
\end{esercizio}

Iniziamo definendo $h(x) = f(x)-g(x)$, che quindi $h \in C^0([0,+\infty))$ e derivabile in $(0, +\infty)$. L'idea sarebbe di applicare il teorema di Lagrange, ma l'intervallo definito è illimitato, quindi dobbiamo definire $M >0$ e usare l'intervallo $[0,M]$.  Quindi applichiamo il teorema di Lagrange, e scopriamo che $\exists c \in (0,M)$ tale che
\begin{align*}
    h'(c) &= \dfrac{h(M) - h(0)}{M-0} \\
    f'(c) - g'(c) &=  \dfrac{\big[f(M)-g(M)\big] - \big[f(0)-g(0)\big]}{M} 
\end{align*}
Riarrangiando i termini scopriamo che
\[
f(M)-g(M) =  \left(f'(c) + g'(c)\right)M + \left(f(0)-g(0)\right)
\]
Dalle ipotesi sappiamo che $f'(x) \geq g'(x)$ per ogni $x>0$, ma dato che $c \in (0, M)$ possiamo dire anche che $f'(c) \geq g'(c)$ da cui $f'(c) - g'(c) \geq 0$. Poi sappiamo anche che $f(0)\geq g(0)$ da cui $f(0)- g(0)\geq 0$, e dato anche che $M>0$ possiamo dedurre che
\[
f(M)-g(M) =  \left(f'(c) - g'(c)\right)M + \left(f(0)-g(0)\right) \geq 0
\]
\[
f(M)-g(M)  \geq 0
\]
\[
f(M) \geq g(M)
\]
Dato che possiamo fare questo ragionamento per $\forall M > 0$, allora possiamo dire che
\[
f(x) \geq g(x) \;\;\;\;\; \forall x \in (0,+\infty)
\]
Ma dato che per ipotesi sappiamo anche che $f(0)\geq g(0)$ allora
\[
f(x) \geq g(x) \;\;\;\;\; \forall x \in [0,+\infty)
\]

\newpage 
\begin{esercizio}{}{}
    Siano $a,b \in \mathbb{R}$, con $a<b$, e sia $f \in C^0([a,b])$ derivabile in $(a,b)$. Dimostrare che $f$ è affine  se e solo se $f'$ è costante su $(a,b)$.
\end{esercizio}

Quindi riscrivendo la domanda, ci stanno chiedendo di verificare che
\[
f(x) = mx+q \iff f'(x) = c \;\;\;\;\; m,q,c \in \mathbb{R}
\]

l'implicazione $(\implies)$ è semplice e basta usare l'algebra delle derivate. Mentre per l'implicazione $(\impliedby)$ dobbiamo usare il teorema di Lagrange. Se scegliamo $x, x_0 \in (a,b)$ con $x<x_0$ possiamo applicare il teorema di Fermat nell'intervallo $[x,x_0]$, e che quindi $\exists k \in (x, x_0)$ tale che
\[
f'(k) = \dfrac{f(x) - f(x_0)}{x-x_0}
\]
Ma dato che $f'(x) = c$ per ogni $x \in (a,b)$ e $k \in (x, x_0) \subset (a,b)$ allora $f'(k) = c$
\begin{align*}
    c &= \dfrac{f(x) - f(x_0)}{x-x_0} \\
    c(x-x_0) &=  f(x) - f(x_0) \\
    cx-cx_0 + f(x_0) &=  f(x) 
\end{align*}
Quindi se scegliamo $a = c$ e $b = -cx_0 + f(x_0)$ abbiamo che
\[
f(x) = ax+b
\]
Quindi $f$ è una funzione affine

\begin{esercizio}{}{}
    Siano $a,b \in \mathbb{R}$, con $a<b$, e sia $f \in C^0([a,b])$ derivabile in $(a,b)$. Dimostrare che $f$ è quadratica  se e solo se $f'$ è affine su $(a,b)$.
\end{esercizio}
Come l'esercizio precedente riscriviamo la richiesta
\[
f(x) = ax^2+bx+c \iff f'(x) = mx+q \;\;\;\;\; a,b,c,m,q \in \mathbb{R}
\]
Come prima l'implicazione $(\implies)$ la si evince dall'algebra delle derivate. Per l'implicazione $(\impliedby)$ si usa il teorema di Lagrange. Quindi come prima scegliamo $x, x_0 \in (a,b)$ e applicando il teorema all'intervallo $[x,x_0]$ sappiamo che $\exists k \in (x,x_0)$ tale che
\[
f'(k) = \dfrac{f(x)-f(x_0)}{x-x_0}
\]
Dato che $f'(x)=mx+q$ per ogni $x \in (a,b)$, allora sappiamo che
\begin{align*}
    mx+q &= \dfrac{f(x)-f(x_0)}{x-x_0} \\
    (mx+q)(x-x_0) &= f(x)-f(x_0) \\
     mx^2+qx -x_0mx -qx_0 &= f(x)-f(x_0) \\
     mx^2+qx -x_0mx -qx_0 + f(x_0) &= f(x)
\end{align*}
Ora se scegliamo $a=m$, $b=q-x_0m$,$c=-qx_0 + f(x_0)$ abbiamo che
\[
f(x) = ax^2+bx+c
\]

\newpage

\addcontentsline{toc}{subsection}{Relazione tra Derivata e Monotonia}
\begin{teorema}{Relazione tra Derivata e Monotonia}{}
    Siano $a,b \in \mathbb{R}$, $a<b$, $f,g:[a,b] \to \mathbb{R}$, $f,g \in C^0([a,b])$ e derivabili su $(a,b)$. Allora
    \begin{enumerate}[label=(\roman*)]
        \item $f$ è crescente in $[a,b] \iff f'(x) \geq 0 \;\; \forall x \in (a,b)$
        \item $f$ è strettamente crescente in $[a,b] \iff f'(x) \geq 0 \;\; \forall x \in (a,b)$ e non esiste un intervallo $(c,d) \subset (a,b)$ tale che  $f'(x) = 0\;\; \forall x \in (c,d)$
        \item $f$ è decrescente in $[a,b] \iff f'(x) \leq 0 \;\; \forall x \in (a,b)$ 
        \item $f$ è strettamente decrescente in $[a,b] \iff f'(x) \leq 0 \;\; \forall x \in (a,b)$ e non esiste un intervallo $(c,d) \subset (a,b)$ tale che  $f'(x) = 0\;\; \forall x \in (c,d)$
    \end{enumerate}
\end{teorema}

\begin{proof}
    $(i)$, in primis dimostriamo $(\implies)$. Scelgo $x_0 \in (a,b)$, notiamo che se $x > x_0$ allora $x-x_0>0$. E dato che la funzione è monotona crescente, se $x>x_0$ allora $f(x)\geq f(x_0)$ e quindi $f(x)-f(x_0)\geq 0$. Discorso analogo per $x<x_0$, e notiamo che il rapporto incrementale
    \[
    \dfrac{f(x)-f(x_0)}{x-x_0} = \begin{cases}
        \geq 0 & x>x_0 \\
        \geq 0 & x<x_0 
    \end{cases}
    \]
    Da cui se portiamo al limite, per il teorema della permanenza del segno abbiamo che
    \[
    f'(x_0) = \lim_{x\to x_0}\dfrac{f(x)-f(x_0)}{x-x_0} \geq 0
    \]

    $(\impliedby)$ Prendiamo $x_1,x_2$ tali che $a < x_1 < x_2 < b$. Applichiamo il teoreama di Lagrange nell'intervallo $[x_1,x_2]$. Ora per il teorema della permanenza del segno e dato che $f'(x_0) \geq 0$ per ipotesi, allora anche il rapporto incrementale sarà positivo. In più dato che $x_1 < x_2$ allora $x_2-x_1 > 0$ e quindi
    \[
    \dfrac{f(x_2)-f(x_1)}{x_2-x_1} \geq 0 
    \]
    \[
    f(x_2)-f(x_1) \geq 0  \iff f(x_2)\geq f(x_1)
    \]
    Ma dato che vale $\forall x_1,x_2 \in (a,b)$ allora la funzione è monotona crescente in $(a,b)$.

    $(ii)$ Una funzione è strettamente crescente  se e solo se è monotona crescente e non è mai costante nell'intervallo. Ma come abbiamo visto nel primo corollario del teorema di Fermat, sappiamo che una funzione è costante se e solo se $f'(x) = 0$ per ogni valore nell'intervallo. Quindi per controllare se una funzione è strettamente crescente basta controllare se è crescente, e quindi $f'(x) \ge 0$, e che non ci sia mai un intervallo su cui vale $f(x) = 0$ per ogni valore del sotto intervallo.
    \[
    f(x) \text{ strettamente crescente } = \begin{cases}
        f(x) \text{ crescente } \\
        f(x) \text{ non è costante}
    \end{cases} =
    \begin{cases}
        f'(x)\ge 0 \\
        \nexists\text{un intervallo} : f'(x)=0 
    \end{cases}
    \] 
    $(iii)$ e $(iv)$ sono analoghi ad $(i)$ e $(ii)$
\end{proof}

\newpage 
\begin{esercizio}{}{}
    Discutere l'esistenza di valori di $x \in \mathbb{R}$ che risolvono la seguente disequazione
    \[
    e^x = x+2
    \]
\end{esercizio}
In primi definiamo 
\[
f(x) = e^x - x -2
\]
Con il teorema di Bolzano notiamo che $f(1) = e - 1 -2 < 0$ e che $f(2)=e^2-2-2>0$, e che quindi c'è almeno uno zero nell'intervallo $(1,2)$. Però è necessario studiare la monotonia della funzione, quindi possiamo studiare il segno della derivata
\begin{align*}
f'(x) &\geq 0 \\
e^x - 1 &\geq 0 \\
e^x   &\geq 1 \\
x &\geq 0
\end{align*}
Quindi scopriamo che la funzione è monotona crescente per $x>0$ e decrescente per $x<0$. Quindi lo zero è unico nell'intervallo $(1,2)$. Però abbiamo detto che la funzione è decrescente per $x<0$, quindi dobbiamo controllare se c'è qualche zero anche per $x<0$. Infatti si può notare che $f(-2) = e^{-2} -2 +2 >0$ e che quindi esiste uno zero anche in $(-2,1)$ (e questo lo deduciamo dal teorema di Bolzano) e dato che in quell'intervallo è monotona allora sappiamo che quello zero è unico.

\begin{esercizio}{}{}
    Trovare il più grande $a \in \mathbb{R}$ tale che $f(x)=-2x^3+3x^2+12x+100$ sia crescente su $[0,a]$
\end{esercizio}
Se ci chiedono di studiare quando è crescente, allora dovremo studiare quando la derivata prima della funzione è positiva.
\begin{align*}
\dfrac{d}{dx}(f(x)) = -6x^2+6x+12 &\geq 0 \\
x^2-x-2 &\leq 0 \\
(x+1)(x-2) &\leq 0 \\
-1\leq x &\leq 2
\end{align*}

Quindi vuol dire che la nostra è crescente in $[-1,2]$ e decrescente in $\mathbb{R} \setminus [-1,2]$. 


    \begin{center}
        
\begin{tikzpicture}[scale=1.5]

    % 1. Disegna la retta dei numeri con la freccia e l'etichetta R
    \draw[->, very thick, >=Stealth] (-3, 0) -- (4, 0) node[right, font=\Huge] {$\mathbb{R}$};

    % 2. Definisci i punti e le etichette
    \def\ptA{-1} % Punto A
    \def\ptB{2}  % Punto B
    \def\tickheight{0.2} % Altezza dei trattini

    % Trattino verticale per -1
    \draw[very thick] (\ptA, -\tickheight) -- (\ptA, \tickheight);
    % Etichetta per -1 (sopra il trattino)
    \node[above, font=\Large] at (\ptA, \tickheight) {$-1$};

    % Trattino verticale per 2
    \draw[very thick] (\ptB, -\tickheight) -- (\ptB, \tickheight);
    % Etichetta per 2 (sopra il trattino)
    \node[above, font=\Large] at (\ptB, \tickheight) {$2$};
    
    % 3. Disegna le frecce direzionali (come nel tuo disegno)

    % Freccia Blu Sinistra (Intervallo x < -1)
    \draw[->, blue, very thick, line cap=round]   (-3, -0.5) -- (-1.5, -1.5); 
    
    % Freccia Rossa Centrale (Intervallo -1 <= x <= 2 o qualcosa che va verso il 2)
    % Usiamo un percorso che sale verso il punto 2
    \draw[->, red, very thick, line cap=round] (-0.3, -1.5) -- (\ptB - 0.5, -0.4); 
    
    % Freccia Blu Destra (Intervallo x > 2)
    \draw[->, blue, very thick, line cap=round] (2.5, -0.5) -- (3.5, -1.5); 

\end{tikzpicture}
    \end{center}
Quindi $a=2$, dato che per $x>a$ la funzione è decrescente.  

\newpage
\begin{esercizio}{}{}
    Trovare il più piccolo $a \in \mathbb{R}$ tale che $f(x) = x^{\sqrt{x}}$ sia invertibile su $[a, +\infty)$
\end{esercizio}

Una funzione è invertibile solo se è monotona (crescente o decrescente non importa) in modo tale che sia iniettiva. Quindi studiamo il segno della derivata. Prima però notiamo che il dominio della funzione è $x>0$ dato che la base di un esponenziale non può essere negativa e l'argomento della radice deve essere positivo.
\begin{align*}
    x^{\sqrt{x}} &= L \\
    \log\left(x^{\sqrt{x}}\right) &= \log(L) \\
    \sqrt{x}\log(x) &= \log(L) \\
    \dfrac{d}{dx}\left(\sqrt{x}\log(x)\right) &= \dfrac{d}{dx}\left(\log(L)\right) \\
    \dfrac{1}{2\sqrt{x}}\log(x) + \sqrt{x}\cdot \dfrac{1}{x}&= \dfrac{1}{L} \cdot L'\\
    L\left(\dfrac{1}{2\sqrt{x}}\log(x) + \dfrac{1}{\sqrt{x}}\right) &= L' \\
    x^{\sqrt{x}}\left[\dfrac{1}{2\sqrt{x}}\left(\log(x)+2\right)\right]&= L'
\end{align*}

Ora studiamo quando è positiva
\[
x^{\sqrt{x}}\left[\dfrac{1}{2\sqrt{x}}\left(2-\log(x)\right)\right] \geq 0
\]
Ora possiamo notare che $x^{\sqrt{x}}$ è positiva per ogni punto del suo dominio (dato che è un esponenziale). In più anche $\sqrt{x}$ è sempre positiva nel suo domionio, e quindi possiamo toglierle dato che non influenzano nello studio del segno della derivata
\begin{align*}
\log(x)+2 &\geq 0 \\
\log(x) &\geq -2 \\
x &\geq e^{-2} 
\end{align*}
Quindi notiamo che la nostra funzione è crescente per $x> e^{-2}$, quindi 
\begin{center}
    
\begin{tikzpicture}[scale=1.5] % Uso una scala moderata per un buon centraggio

    % Definiamo coordinate arbitrarie per i punti (non in scala reale)
    \def\StartLine{0}  % La linea inizia in 0
    \def\EndLine{5}    % La linea finisce in 5
    
    \def\ptA{0}        % Punto A: 0
    \def\ptB{2.5}      % Punto B: e^-2 (posizionato a metà della linea per centrarlo)
    \def\tickheight{0.2} % Altezza dei trattini

    % 1. Disegna la retta dei numeri partendo da 0 fino a EndLine
    \draw[->, very thick, >=Stealth] (\StartLine, 0) -- (\EndLine, 0) node[right, font=\Huge] {$\mathbb{R}$};

    % 2. Punti e Etichette
    
    % Trattino verticale per 0 (inizio linea)
    \draw[very thick] (\ptA, -\tickheight) -- (\ptA, \tickheight);
    % Etichetta per 0 
    \node[above, font=\Large] at (\ptA, \tickheight) {$0$};

    % Trattino verticale per e^(-2) (centrato)
    \draw[very thick] (\ptB, -\tickheight) -- (\ptB, \tickheight);
    % Etichetta per e^(-2) 
    \node[above, font=\Large] at (\ptB, \tickheight) {$e^{-2}$};
    
    % 3. Frecce direzionali (adattate alle nuove coordinate)

    % Freccia Blu (parte da 0 e va in basso)
    \draw[->, blue, very thick, line cap=round] (\ptA+0.8, -0.3) -- (\ptA+1.8, -0.7); 
    
    % Freccia Rossa Destra (parte tra e^-2 e la fine e va a destra)
    % Inizia circa a metà strada tra e^-2 (2.5) e la fine (5)
    \draw[->, red, very thick, line cap=round] (3.3, -0.7) -- (4.3, -0.3); 

\end{tikzpicture}
\end{center}

Quindi $f(x)$ è invertibile negli intervalli $(0,e^{-2}]$ e $[e^{-2},+\infty)$, e quindi stando alla consegna $a=e^{-2}$

\newpage 

\addcontentsline{toc}{subsection}{Classificazione dei Punti Stazionari}
\begin{definizione}{Classificazione dei Punti Stazionari}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo $f:I\to \mathbb{R}$, $f\in C^0(I)$, $x_0 \in I$, se $f$ è derivabile in $x_0$ e vale $f'(x_0) = 0$ allora diciamo 
    \begin{itemize}
        \item $x_0$ è un punto di \textbf{Flesso a Tangente Orizzontale} se $\exists U$ intorno di $x_0$ tale che
        \[
            f'(x)\geq 0 \;\;\; \forall x \in U \;\;\; \lor \;\;\; f'(x)\leq 0 \;\;\; \forall x \in U 
        \]
         \item $x_0$ è un punto di \textbf{Massimo Locale} se $\exists U_{sx}$ intorno sinistro di $x_0$, e $\exists U_{dx}$ intorno destro di $x_0$ tale  che
        \[
            f'(x)\geq 0 \;\;\; \forall x \in U_{sx} \;\;\; \land \;\;\; f'(x)\leq 0 \;\;\; \forall x \in U_{dx}
        \]
        \item $x_0$ è un punto di \textbf{Minimo Locale} se $\exists U_{sx}$ intorno sinistro di $x_0$, e $\exists U_{dx}$ intorno destro di $x_0$ tale  che
        \[
            f'(x)\leq 0 \;\;\; \forall x \in U_{sx} \;\;\; \land \;\;\; f'(x)\geq 0 \;\;\; \forall x \in U_{dx}
        \]
    \end{itemize}
\end{definizione}

Con dei grafici notiamo che il flesso a tangente verticale lo abbiamo quando le linee dei segno sono di questo tipo
\begin{center}
\begin{tikzpicture}[scale=1, line cap=round]

%%%%%%%%%%%%%%%%%%%%%%
% SINISTRA (frecce rosse)
%%%%%%%%%%%%%%%%%%%%%%

% Retta
\draw[->, very thick] (0,0) -- (5,0);

% Tacca e w0
\draw[very thick] (2.5,-0.25) -- (2.5,0.25);
\node[above] at (2.5,0.25) {$x_0$};

% Freccia rossa a sinistra
\draw[->, very thick, red] (0.8,-0.8) -- (1.8,-0.2);

% Freccia rossa a destra
\draw[->, very thick, red] (3.2,-0.8) -- (4.2,-0.2);


%%%%%%%%%%%%%%%%%%%%%%
% DESTRA (frecce blu)
%%%%%%%%%%%%%%%%%%%%%%

\begin{scope}[xshift=6cm] % sposto il secondo schema

% Retta
\draw[->, very thick] (0,0) -- (5,0);

% Tacca e w0
\draw[very thick] (2.5,-0.25) -- (2.5,0.25);
\node[above] at (2.5,0.25) {$x_0$};

% Freccia blu a sinistra
\draw[->, very thick, blue] (0.8,-0.2) -- (1.8,-0.8);

% Freccia blu a destra
\draw[->, very thick, blue] (3.2,-0.2) -- (4.2,-0.8);

\end{scope}

\end{tikzpicture}
\end{center}

Dei grafici con un flesso a tangente orizzontale sono: $f(x)=x^3$ e $g(x)=-x^3$ in $x_0=0$

\begin{figure}[h!]
\centering

% --- Primo grafico ---
\begin{minipage}{0.48\textwidth}
\centering


\begin{tikzpicture}

% -----------------------------------------------------------
% FIRST GRAPH: f(x) = x^3
% -----------------------------------------------------------
\begin{axis}[
    name=leftplot,
    xmin=-2, xmax=2,
    ymin=-1.2, ymax=1.2,
    width=6cm,
    height=4cm,
    axis lines=middle,
    axis line style={-stealth, thick},
    xtick=\empty,
    ytick=\empty,
    samples=300,
    clip=false
]

    % f(x)
    \addplot[red!60!black, ultra thick, domain=-2:2] {0.125*x^3};

    % inflection point
    \addplot[only marks, mark=*, green, mark size=2pt] coordinates {(0,0)};

    % vertical line x=0
    \addplot[green, very thin] coordinates {(-2,0) (2,0)};

    % label
    \node at (axis cs:0,-1.2) [below] {$f(x)=x^3$};

\end{axis}


\end{tikzpicture}

\end{minipage}
%
\begin{minipage}{0.48\textwidth}
\centering
% --- Secondo grafico: f^{-1}(y)=sqrt(y) ---


\begin{tikzpicture}
    
% -----------------------------------------------------------
% SECOND GRAPH: g(x) = -x^3
% -----------------------------------------------------------
\begin{axis}[
    at={(leftplot.outer east)},  % <-- PERFECT ALIGNMENT
    xshift=3cm,                  % spacing between the plots
    xmin=-2, xmax=2,
    ymin=-1.2, ymax=1.2,
    width=6cm,
    height=4cm,
    axis lines=middle,
    axis line style={-stealth, thick},
    xtick=\empty,
    ytick=\empty,
    samples=300,
    clip=false
]

    % g(x)
    \addplot[blue!60!black, ultra thick, domain=-2:2] {-0.125*x^3};

      % inflection point
    \addplot[only marks, mark=*, green, mark size=2pt] coordinates {(0,0)};

    % vertical line x=0
    \addplot[green, very thin] coordinates {(-2,0) (2,0)};


    % label
    \node at (axis cs:0,-1.2) [below] {$g(x)=-x^3$};

\end{axis}

\end{tikzpicture}


\end{minipage}
\end{figure}


Invece per un massimo locale abbiamo la seguente linea dei segni

\begin{center}
\begin{tikzpicture}[scale=1, line cap=round]

%%%%%%%%%%%%%%%%%%%%%%
% SINISTRA (frecce rosse)
%%%%%%%%%%%%%%%%%%%%%%

% Retta
\draw[->, very thick] (0,0) -- (5,0);

% Tacca e w0
\draw[very thick] (2.5,-0.25) -- (2.5,0.25);
\node[above] at (2.5,0.25) {$x_0$};

% Freccia rossa a sinistra
\draw[->, very thick, red] (0.8,-0.8) -- (1.8,-0.2);

% Freccia blu a destra
\draw[->, very thick, blue] (3.2,-0.2) -- (4.2,-0.8);


\end{tikzpicture}
\end{center}

Un grafico con un massimo è: $f(x)=1-x^2$  in $x_0=0$
\begin{center}
    
\begin{tikzpicture}
    
% -----------------------------------------------------------
% SECOND GRAPH: g(x) = -x^3
% -----------------------------------------------------------
\begin{axis}[
    xmin=-2, xmax=2,
    ymin=-1.5, ymax=1.5,
    width=6cm,
    height=4cm,
    axis lines=middle,
    axis line style={-stealth, thick},
    xtick=\empty,
    ytick=\empty,
    samples=300,
    clip=false
]

    % g(x)
    \addplot[red!60!black, ultra thick, domain=-1.5:0] {1-x^2};
    % g(x)
    \addplot[blue!60!black, ultra thick, domain=0:1.5] {1-x^2};

      % inflection point
    \addplot[only marks, mark=*, green, mark size=2pt] coordinates {(0,1)};

    % vertical line x=


    % label
    \node at (axis cs:0,-1.2) [below] {$f(x)=1-x^2$};

\end{axis}

\end{tikzpicture}

\end{center}


Invece per un minimo locale abbiamo la seguente linea dei segni

\begin{center}
\begin{tikzpicture}[scale=1, line cap=round]

%%%%%%%%%%%%%%%%%%%%%%
% SINISTRA (frecce rosse)
%%%%%%%%%%%%%%%%%%%%%%

% Retta
\draw[->, very thick] (0,0) -- (5,0);

% Tacca e w0
\draw[very thick] (2.5,-0.25) -- (2.5,0.25);
\node[above] at (2.5,0.25) {$x_0$};

% Freccia rossa a sinistra
\draw[->, very thick, blue] (0.8,-0.2) -- (1.8,-0.8);

% Freccia blu a destra
\draw[->, very thick, red] (3.2,-0.8) -- (4.2,-0.2);


\end{tikzpicture}
\end{center}

Un grafico con un massimo è: $f(x)=x^2-1$  in $x_0=0$
\begin{center}
    
\begin{tikzpicture}
    
% -----------------------------------------------------------
% SECOND GRAPH: g(x) = -x^3
% -----------------------------------------------------------
\begin{axis}[
    xmin=-2, xmax=2,
    ymin=-1.5, ymax=1.5,
    width=6cm,
    height=4cm,
    axis lines=middle,
    axis line style={-stealth, thick},
    xtick=\empty,
    ytick=\empty,
    samples=300,
    clip=false
]

    % g(x)
    \addplot[blue!60!black, ultra thick, domain=-1.5:0] {x^2-1};
    % g(x)
    \addplot[red!60!black, ultra thick, domain=0:1.5] {x^2-1};

      % inflection point
    \addplot[only marks, mark=*, green, mark size=2pt] coordinates {(0,-1)};

    

    % label
    \node at (axis cs:0,-1.2) [below] {$f(x)=x^2-1$};

\end{axis}

\end{tikzpicture}

\end{center}


\addcontentsline{toc}{subsection}{Teorema di De l'Hôpital}
\begin{teorema}{Teorema di De l'Hôpital}{}
    Siano $a,b \in \mathbb{R}\cup\{\pm \infty\}$, $a<b$. Siano $f,g:[a,b]\to \mathbb{R}$ e $f,g \in C^1((a,b))$. Supponiamo che $f$ e $g$ siano entrambe infinitesime (cioè tendono a 0) oppure entrambe infinite (cioè che tendono a $\infty$) per $x\to a^+$. Se vale
    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $g'(x) \ne 0\;\;\forall x \in (a,b)$
        \item $\displaystyle\exists \lim_{x\to a^+}\dfrac{f'(x)}{g'(x)} = l \in \mathbb{R} \cup \{\pm \infty\} $
    \end{enumerate}
    Allora valgono 
    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $g(x) \ne 0\;\;\forall x \in (a,b)$
        \item $\displaystyle\exists \lim_{x\to a^+}\dfrac{f(x)}{g(x)} = l$
    \end{enumerate}
    Discorso analogo per $x\to b^-$
\end{teorema}
Vediamo un esempio di applicazione
\begin{esercizio}{}{}
    Calcolare il seguente limite
    \[
    \lim_{x\to 0^+} x\log(x)
    \]
\end{esercizio}
Si vede subito che questo limite è della forma indeterminata di $\bigr[0\cdot \infty\bigl]$, quindi per ora non sono rispettate le ipotesi del teorema di De l'Hôpital. Però possiamo fare un trick per far sbucare fuori una forma $\bigr[\frac{\infty}{\infty}\bigl]$ (che è richiesta dal teorema)
\[
    \lim_{x\to 0^+} x\log(x) = \lim_{x\to0^+}\dfrac{\log(x)}{\frac{1}{x}} = \bigr[\frac{\infty}{\infty}\bigl]
    \]
Ora rientramo nelle ipotesi del teorema. Ora dobbiamo controllare se $g'(x)\ne 0$ e notiamo che $g'(x) =\dfrac{-1}{x^2}$ il che è diverso da zero in $(0,+\infty)$, e quindi va bene per ogni $b>0$. 

Ora dobbiamo controllare se esiste il limite del rapporto delle derivate 
\[
\lim_{x\to 0^+} \dfrac{\dfrac{1}{x}}{\dfrac{-1}{x^2}} = \lim_{x\to 0^+} -x = 0
\]
Ora tutte le ipotesi del teorema sono soddisfatte e quindi scopriamo che
\[
\lim_{x\to 0^+} x\log(x) = 0
\]

\begin{esercizio}{}{}
    Calcolare il seguente limite
    \[
    \lim_{x\to0^+} \dfrac{\log(1+x) - x}{x^2}
    \]
\end{esercizio}
Ad primo impatto potrebbe venirvi in mente di calcolare tramite gli o-piccoli, però notiamo che non funzionano in questo esercizio
\[
\lim_{x\to0^+} \dfrac{\log(1+x) - x}{x^2} = \lim_{x\to0^+} \dfrac{x + o(x) - x}{x^2} = \lim_{x\to0^+} \dfrac{o(x)}{x^2} = \lim_{x\to0^+} \dfrac{o(x)}{x} \cdot \dfrac{1}{x} = \bigl[0\cdot \infty\bigr] 
\] 
Infatti ci viene fuori una forma indeterminata. Tra poco vedremo anche che si può risolvere con gli sviluppi di Taylor. Ora però proviamo ad applicare il teorema di De L'Hôpital. Quindi sappiamo che $f(x)=\log(1+x) - x \in C^1((0,+\infty))$ e anche che $g(x)=x^2\in C^1(\mathbb{R})$. Possiamo vedere che $g'(x)=2x$ è diverso da zero nell'intervallo $(0,+\infty)$, quindi come prima possiamo scegliere un qualsiasi $b >0$. Ora proviamo a calcolare il limite del rapporto.
\begin{equation}\label{eq:hop}
\lim_{x\to0^+} \dfrac{\dfrac{1}{1+x} \cdot 1 -1}{2x} = \bigl[\dfrac{0}{0}\bigr]    
\end{equation}

Però è venuta fuori un'altra forma indeterminata della forma $\bigl[\dfrac{0}{0}\bigr]$. Però non sappiamo se esiste o è un valore, quindi possiamo riapplicare il teorema di De l'Hôpital ma all limite (\ref{eq:hop}). Così se scopriamo che tende ad un valore, allora tenderà anche il limite (\ref{eq:hop}), che a sua volta verifica il limite dell'esercizio. Si può notare che $f'(x)=\dfrac{1}{1+x} -1$ e $g'(x)=2x$ rispettano il teorema e quindi possiamo applicarlo
\[
\lim_{x\to0^+} \dfrac{f''(x)}{g''(x)} = \lim_{x\to0^+} \dfrac{\dfrac{-1}{(1+x)^2} \cdot 1}{2} =  \dfrac{\dfrac{-1}{(1+0)^2} }{2} = -\dfrac{1}{2}
\]
Quindi con questo abbiamo scoperto che anche 
\[
\lim_{x\to0^+} \dfrac{\dfrac{1}{1+x} -1}{2x} = -\dfrac{1}{2} 
\]
Ma a sua volta questo ci permette di scoprire che
\[
    \lim_{x\to0^+} \dfrac{\log(1+x) - x}{x^2} = -\dfrac{1}{2} 
    \]

    \newpage

\addcontentsline{toc}{subsection}{Definizione di Concavità e Convessità}
    \begin{definizione}{Concavità e Convessità}{}
        Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo $f:I\to \mathbb{R}$ diciamo che
        \begin{itemize}
            \item $f$ è \textbf{Convessa} se $\forall t \in [0,1] \; \forall x_1, x_2 \in I$ vale
            \[
            f((1-t)x_1+tx_2) \leq (1-t)f(x_1) + tf(x_2)
            \]
            \item $f$ è \textbf{Concava} se $\forall t \in [0,1] \; \forall x_1, x_2 \in I$ vale
            \[
            f((1-t)x_1+tx_2) \geq (1-t)f(x_1) + tf(x_2)
            \]
        \end{itemize}
    \end{definizione}
    Cerchiamo di capire geometricamente cosa vuol dire questo. Prendiamo una qualsiasi funzione e fissiamo due punti $x_1,x_2$ tracciamo la retta passante per quei punti.
\begin{center}
    \begin{tikzpicture}
\begin{axis}[
    width=8cm,
    height=6cm,
    xmin=0, xmax=3.0,
    ymin=0, ymax=4.0,
    axis lines=middle,
    axis line style={-stealth, thick},
    samples=300,
    xlabel={$x$}, ylabel={$y$},
    xtick={0.5, 2}, xticklabels={$x_1$, $x_2$}, % Etichette personalizzate sugli assi
    ytick=\empty, % Asse Y pulito
    clip=false
]

% 1. La funzione f(x) = (x-1)^2 + 1
\addplot[blue, ultra thick, domain=0:2.8] {(x-1)^2 + 1};
\node[blue] at (axis cs:2.3, 3.5) {$f(x)$};

% 2. Calcolo della retta passante per i punti
% P1(0.5, 1.25) e P2(2, 2)
% m = (2 - 1.25) / (2 - 0.5) = 0.75 / 1.5 = 0.5
% Eq: y - 2 = 0.5(x - 2) => y = 0.5x + 1
\addplot[green!60!black, thick, domain=0:2.5] {0.5*x + 1};

% 3. I "pallini verdi" sui punti richiesti
\addplot[only marks, mark=*, green!60!black, mark size=2.5pt] coordinates {(0.5, 1.25) (2, 2)};

% 4. Linee tratteggiate di proiezione sull'asse x (opzionale, per chiarezza)
\draw[dashed, gray!50] (axis cs:0.5, 0) -- (axis cs:0.5, 1.25);
\draw[dashed, gray!50] (axis cs:2, 0) -- (axis cs:2, 2);

\end{axis}
\end{tikzpicture}
\end{center}

La retta tangente ha equazione 
\[
y=f(x_1) + (\mathunderline{red}{x}-x_1)\dfrac{f(x_2)-f(x_1)}{x_2-x_1}
\]
Mentre il termine $(1-t)x_1+tx_2$ con $\forall t \in [0,1]$ permette di prendere tutti i valori (al variare di $t$) tra $x_1$ e $x_2$. Quindi potete vederlo come uno slider che parte da $x_1$ e $x_2$. Quindi $f((1-t)x_1+tx_2)$ non è altro che un punto di $f(x)$ con un valore compreso tra $x_1$ e $x_2$. Mentre per capire cos'è $(1-t)f(x_1) + tf(x_2)$ dobbiamo fare qualche conto. Infatti proviamo a mettere $(1-t)x_1+tx_2$ nella retta secante. 
\begin{align*}
    f(x_1) + (\mathunderline{red}{(1-t)x_1+tx_2}-x_1)\dfrac{f(x_2)-f(x_1)}{x_2-x_1} &= f(x_1) + (x_1-tx_1+tx_2-x_1)\dfrac{f(x_2)-f(x_1)}{x_2-x_1} \\
    &= f(x_1) + t(x_2-x_1)\dfrac{f(x_2)-f(x_1)}{x_2-x_1} \\
    &= f(x_1) + t(f(x_2)-f(x_1)) \\
    &=  (1-t)f(x_1) + tf(x_2)
\end{align*}

Quindi abbiamo scoperto che il termine $(1-t)f(x_1) + tf(x_2)$ non è altro che il valore che assume la retta nel punto generico $(1-t)x_1+tx_2$. Quindi una funzione convessa (come quella in figura sopra) è inferiore alla retta passante per due punti della funzione. Se una funzione è convessa dal punto di vista grafico assomiglia ad una U (come nel grafico sopra), mentre se una funzione è convessa (e quindi la funzione sarà sempre sopra alla retta secante) avrà una forma simile ad una U rovesciata.

\newpage
Il concetto di Concavità e Convessità si riferisce a tutto il dominio della funzione, ma funzioni come il seno non sono perennemente concave (o perennemente convesse), ma lo sono a tratti. Infatti se prendiamo l'intervallo $[0,\pi]$ la funzione $f(x) = \sin(x)$ è concava (ed infatti assomiglia ad una parabola triste), mentre  nell'intervallo $[\pi, 2\pi]$  è convessa (e quindi assomiglia ad una parabola felice).
\begin{center}
    
\begin{tikzpicture}
\begin{axis}[
    width=10cm,
    height=6cm,
    % Impostiamo il formato in radianti per facilitare i calcoli
    trig format plots=rad,
    xmin=0, xmax=6.5, % Un po' più di 2pi (6.28)
    ymin=-1.5, ymax=1.5,
    axis lines=middle,
    axis line style={-stealth, thick},
    xlabel={$x$}, ylabel={$y$},
    % Imposto i tick esattamente sui valori di pi greco
    xtick={1.5708, 3.14159, 4.71239, 6.28318},
    xticklabels={$\frac{\pi}{2}$, $\pi$, $\frac{3\pi}{2}$, $2\pi$},
    ytick={-1, 1},
    samples=300,
    clip=false
]

% 1. La funzione seno (Blu)
\addplot[blue, ultra thick, domain=0:6.28] {sin(x)};

% 2. Parabola tangente in pi/2 (Massimo)
% Taylor grado 2: f(x) ~ 1 - 1/2(x - pi/2)^2
% Dominio limitato per estetica
\addplot[red, thick, domain=0:3.5] {1 - 0.5*(x - 1.5708)^2};

% 3. Parabola tangente in 3pi/2 (Minimo)
% Taylor grado 2: f(x) ~ -1 + 1/2(x - 3pi/2)^2
\addplot[green!60!black, thick, domain=2.71:6.71] {-1 + 0.5*(x - 4.7124)^2};

% 4. Pallini sui punti di tangenza
\addplot[red, only marks, mark=*, mark size=2pt] coordinates {
    (1.5708, 1) 
    
};
\addplot[green, only marks, mark=*, mark size=2pt] coordinates {
    (4.7124, -1)
    
};

% Linee tratteggiate di riferimento (opzionali, per pulizia le ho omesse o fatte leggerissime)
\draw[dotted, gray] (axis cs:1.5708, 0) -- (axis cs:1.5708, 1);
\draw[dotted, gray] (axis cs:4.7124, 0) -- (axis cs:4.7124, -1);

\end{axis}
\end{tikzpicture}
\end{center}

Per questo spesso si indica la concavità e convessità ad intervalli, anche perchè sono poche le funzioni completamente concave o completamente convesse.

\addcontentsline{toc}{subsection}{Caratterizzazione della Convessità e Concavità }
\begin{teorema}{Caratterizzazione della Convessità e Concavità }{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo $f:I\to \mathbb{R}$, se $f$ è derivabile  allora
    \begin{enumerate}[label=(\roman*)]
        \item $f \text{ è convessa su } I \iff f' \text{ è crescente su } I$
        \item $f \text{ è concava su } I \iff f' \text{ è decrescente su } I$ 
         \item $f \text{ è convessa su } I \iff f(x) \geq f(x_0) + f'(x_0)(x-x_0) \;\;\; \forall x \in I$
         \item $f \text{ è concava su } I \iff f(x) \leq f(x_0) + f'(x_0)(x-x_0) \;\;\; \forall x \in I$ 
    \end{enumerate}
\end{teorema}

Dal punto $(i)$ e $(ii)$ se ne deduce un corollario molto utile per lo studio di funzione
\addcontentsline{toc}{subsection}{Relazione Derivata Seconda e Concavità e Convessità }
\begin{corollario}{Relazione Derivata Seconda e Concavità e Convessità }{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo $f:I\to \mathbb{R}$, se $f$ è derivabile  allora
    \begin{enumerate}[label=(\roman*)]
        \item $f \text{ è convessa su } I \iff f''(x) \geq 0 \;\;\; \forall x \in I$
        \item $f \text{ è concava su } I \iff f''(x) \leq 0 \;\;\; \forall x \in I$
    \end{enumerate}
\end{corollario}
Rimanendo quindi con l'esempio del seno, se proviamo a fare la derivata seconda
\[
\dfrac{d^2}{dx^2}(\sin(x)) = \dfrac{d}{dx}\left(\dfrac{d}{dx}(\sin(x))\right) =  \dfrac{d}{dx}(\cos(x)) = -\sin(x)
\]
Quindi se proviamo a vedere quando $f''(x)\geq 0$ notiamo che
\[
f''(x) \geq 0 \implies -\sin(x) \geq 0 \implies \sin(x) \leq 0 \implies \pi + 2k\pi \leq x \leq 2\pi + 2k\pi \;\;\; \forall k \in \mathbb{Z}
\]
Di conseguenza con questo scopriamo che il seno è convesso per $x \in [\pi,2\pi]$ (più le ripetizioni),  proprio come avevamo individuato con il grafico. Chiaramente $f''\leq 0$ per $x\in [0, \pi]$, e quindi il seno è concavo in questo intervallo, come avevamo già individuato. 
\addcontentsline{toc}{subsection}{Definizione di punti di Flesso}
\begin{definizione}{Punti di Flesso}{}
Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo $f:I\to \mathbb{R}$, $x_0 \in I$. Diciamo che $x_0$ è un punto di \textbf{Flesso} se $f''(x_0)=0$ e se è concava in un intordo sinitro e convessa in un intorno destro, oppure se è convessa in un intorno sinistro e concava in un intorno destro.
\end{definizione}

Quindi come abbiamo visto prima con il seno, scopriamo che $x=\pi$ è un punto di flesso, dato che prima di esso la funzione è concava, mentre dopo la funzione è convessa. Un altro esempio  è la gaussiana, ovvero la funzione $f(x)=e^{-x^2}$. Infatti se calcoliamo la derivata seconda abbiamo che
\begin{align*}
   \dfrac{d}{dx}\left(\dfrac{d}{dx}(e^{-x^2})\right) = \dfrac{d}{dx}\left(e^{-x^2}\cdot (-2x)\right) &= (e^{-x^2})'\cdot (-2x) + (e^{-x^2})\cdot (-2x)' \\
   &=(e^{-x^2}\cdot (-2x))\cdot (-2x) + (e^{-x^2})\cdot (-2) \\
   &= e^{-x^2}(4x^2-2)
\end{align*}

Vediamo quando è positiva 
\[
f''(x) \geq 0 \implies e^{-x^2} (4x^2-2) \geq 0
\]
Però $e^{-x^2}$ è sempre positiva, quindi non va a influire sul segno
\[
(4x^2-2) \geq 0 \implies x^2 \geq \dfrac{1}{2} \implies x\leq -\dfrac{1}{\sqrt{2}} \lor x \geq \dfrac{1}{\sqrt{2}}
\]
Quindi notiamo che $f(x)$ è concava per $x \in (-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$ e convessa nel resto del dominio. Di conseguenza i punti $x=\pm\frac{1}{\sqrt{2}}$ sono punti di flesso. Quindi quando $x\in(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$ la funzione assomiglierà ad una parabola triste (quella in rosso), mentre per il restante assomiglierà ad una parabola felice (quelle in verde).  
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        width=12cm, height=8cm,
        axis lines=middle,
        xlabel={$x$}, ylabel={$y$},
        xmin=-2.5, xmax=2.5,
        ymin=-0.5, ymax=1.3,
        grid=none, 
        clip=false
        % Legenda rimossa completamente
    ]

    % --- 1. La Gaussiana principale ---
    \addplot[
        blue, ultra thick,
        domain=-2.5:2.5,
        samples=200,
        smooth
    ] {exp(-x^2)};

    % --- 2. Parabola tangente in x=0 (Rossa) ---
    \addplot[
        red, thick,
        domain=-1.2:1.2,
        samples=100
    ] {1 - x^2};
    
    % Mark x=0
    \addplot[only marks, mark=*, red, mark size=2pt] coordinates {(0,1)};


    % --- 3. Parabola tangente in x = +1.2 (Verde) ---
    \pgfmathsetmacro{\a}{1.2} 
    \pgfmathsetmacro{\fa}{exp(-\a*\a)} 
    \pgfmathsetmacro{\fpa}{-2*\a*exp(-\a*\a)} 
    \pgfmathsetmacro{\fppa}{2*exp(-\a*\a)*(2*\a*\a - 1)} 
    
    \addplot[
        green!60!black, thick,
        domain=0.3:2.7, 
        samples=100
    ] {\fa + \fpa*(x-\a) + 0.5*\fppa*(x-\a)^2};

    % Mark x=1.2
    \addplot[only marks, mark=*, green!60!black, mark size=2pt] coordinates {(1.2, \fa)};


    % --- 4. Parabola tangente in x = -1.2 (Verde) ---
    \pgfmathsetmacro{\b}{-1.2} 
    \pgfmathsetmacro{\fb}{exp(-\b*\b)} 
    \pgfmathsetmacro{\fpb}{-2*\b*exp(-\b*\b)} 
    \pgfmathsetmacro{\fppb}{2*exp(-\b*\b)*(2*\b*\b - 1)} 
    
    \addplot[
        green!60!black, thick,
        domain=-2.7:-0.3, 
        samples=100
    ] {\fb + \fpb*(x-\b) + 0.5*\fppb*(x-\b)^2};

    % Mark x=-1.2
    \addplot[only marks, mark=*, green!60!black, mark size=2pt] coordinates {(-1.2, \fb)};


    % --- 5. Punti di flesso (Neri) ---
    \pgfmathsetmacro{\xinf}{1/sqrt(2)}
    \pgfmathsetmacro{\yinf}{exp(-0.5)}

    \addplot[
        only marks,
        mark=*,
        black,
        mark size=2.5pt
    ] coordinates {(-\xinf, \yinf) (\xinf, \yinf)};

    % Etichette flessi
    \node[black, above] at (axis cs:-\xinf-0.3, \yinf) {$Flesso$};
    \node[black, above] at (axis cs:\xinf+0.3, \yinf) {$Flesso$};

    \end{axis}
\end{tikzpicture}    

\end{center}


\newpage
\addcontentsline{toc}{subsection}{Definizione di Asintoti}
\begin{definizione}{Asintoti}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo $f:I\to \mathbb{R}$. $x_0$ punto di accumulazione di $I$ 
    \begin{itemize}
        \item Se $\displaystyle\lim_{x\to x_0^-} f(x) \in \{\pm\infty\}$ allora diciamo che $x=x_0$ è \textbf{asintoto verticale sinistro}, se invece  $\displaystyle\lim_{x\to x_0^+} f(x) \in \{\pm\infty\}$ diciamo che $x=x_0$ è \textbf{asintoto verticale destro}, mentre se $x=x_0$ è sia asintoto verticale destro che sinistro di dice \textbf{bilatero}.
        \item Se $\displaystyle\lim_{x\to \pm \infty} f(x) = l \in \mathbb{R}$ allora diciamo che $y=l$ è \textbf{asintoto orizzontale}.
        \item Se $\displaystyle\lim_{x\to \pm \infty} (f(x)-(ax+b)) = 0$ allora diciamo che $y=ax+b$ è \textbf{asintoto obliquo}, per qualche $a \in \mathbb{R}\setminus \{0\}$, $b \in \mathbb{R}$.
    \end{itemize}
\end{definizione}

Per capire cosa sono dal punto di vista grafico gli asintoti. Vediamo che per $f(x)=e^{\frac{1}{x-1}}$ abbiamo un asintoto verticale destro per $x=1$. Mentre la funzione $g(x) = \frac{1}{x-2}$ ha un asintoto verticale bilatero per $x=2$. Vediamo il grafico

\begin{figure}[h]
    \centering
    
    % --- GRAFICO 1: e^(1/(x-1)) ---
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=6.5cm, height=4.5cm,
                axis lines=middle,
                xlabel={$x$}, ylabel={$y$},
                xmin=-2, xmax=4,
                ymin=-1, ymax=6,
                samples=200,
                grid=none,
                clip=true,
                % FONDAMENTALE: Impedisce l'errore di overflow limitando il calcolo
                restrict y to domain=-5:20 
            ]
            
            % Asintoto verticale x=1 in ROSSO
            \draw[red, thick,dashed] (axis cs:1, -1) -- (axis cs:1, 6);

            % Ramo Sinistro (x < 1) -> tende a 0
            \addplot[blue, thick, domain=-2:0.95] {exp(1/(x-1))};

            % Ramo Destro (x > 1) -> tende a infinito
            \addplot[blue, thick, domain=1.01:4] {exp(1/(x-1))};

            % Pallino vuoto in (1,0)
            \addplot[only marks, mark=o, thick, mark size=2.5pt, black, fill=white] coordinates {(1,0)};
            
            % Etichetta funzione
            \node[blue, anchor=west] at (axis cs:1.9, 3) {$f(x)$};
            \node[red, anchor=west] at (axis cs:1.1, 1.3) {$x=1$};

            \end{axis}
        \end{tikzpicture}
    \end{minipage}
    \hfill
    % --- GRAFICO 2: 1/(x-2) ---
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=6.5cm, height=5cm,
                axis lines=middle,
                xlabel={$x$}, ylabel={$y$},
                xmin=-1, xmax=5,
                ymin=-5, ymax=5,
                samples=200,
                grid=none,
                clip=true,
                restrict y to domain=-10:10
            ]
            
            % Asintoto verticale x=2 in ROSSO tratteggiato
            \draw[red, thick, dashed] (axis cs:2, -10) -- (axis cs:2, 10);

            % Ramo Sinistro (x < 2)
            \addplot[blue, thick, domain=-1:1.9] {1/(x-2)};

            % Ramo Destro (x > 2)
            \addplot[blue, thick, domain=2.1:5] {1/(x-2)};
            
            % Etichetta funzione
            \node[blue] at (axis cs:3.2, 2) {$g(x)$};
            \node[red, anchor=west] at (axis cs:2.2, -2.2) {$x=2$};

            \end{axis}
        \end{tikzpicture}
    \end{minipage}

\end{figure}

Invece notiamo che la funzione $f(x)=|\arctan(x)|$ ha un asintoto orizzontale per $y=\frac{\pi}{2}$
\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
            width=12cm,        % Diagramma allargato
            height=5cm,
            axis lines=middle,
            xlabel={$x$}, ylabel={$y$},
            xmin=-10, xmax=10, % Dominio x esteso
            ymin=-0.5, ymax=2,
            samples=300,       % Campionamento alto per rendere bene la punta in 0
            grid=none,
            clip=false,
            trig format plots=rad % Calcoli in radianti
        ]
        
        % 1. Asintoto ORIZZONTALE y = pi/2 (Rosso tratteggiato)
        \addplot[red, thick, dashed, domain=-10:10] {pi/2};
        
        % Etichetta asintoto
        \node[red, anchor=south east] at (axis cs:10, 1.57) {$y=\frac{\pi}{2}$};

        % 2. Funzione f(x) = |arctan(x)|
        % abs() è la funzione valore assoluto
        \addplot[blue, ultra thick, domain=-10:10] {abs(atan(x))};
        
        % Etichetta funzione
        \node[blue] at (axis cs:4, 0.8) {$f(x)$};

        \end{axis}
    \end{tikzpicture}
\end{center}



In fine, si può osservare che la funzione $f(x)=\sqrt{x^2+1}$ ha due asintoti obliqui: $y=x$ per $x\to +\infty$, e $y=-x$ per $x\to -\infty$



\begin{center}
    
    \begin{tikzpicture}
        \begin{axis}[
            width=10cm, height=5cm,
            axis lines=middle,
            xlabel={$x$}, ylabel={$y$},
            xmin=-5, xmax=5,
            ymin=-0.5, ymax=5.5,
            samples=200,
            grid=none,
            clip=false,
            % Opzionale: assicura che x e y abbiano la stessa scala visiva
            % per far sembrare gli asintoti davvero a 45 gradi
            axis equal image 
        ]
        
        % 1. Asintoto Obliquo Destro y=x (Rosso tratteggiato)
        \addplot[red, thick, dashed, domain=-1:5] {x};
        \node[red, anchor=south east] at (axis cs:4.5, 4.5) {$y=x$};

        % 2. Asintoto Obliquo Sinistro y=-x (Rosso tratteggiato)
        \addplot[red, thick, dashed, domain=-5:1] {-x};
        \node[red, anchor=south west] at (axis cs:-4.5, 4.5) {$y=-x$};

        % 3. Funzione f(x) = sqrt(x^2 + 1)
        \addplot[blue, ultra thick, domain=-5:5] {sqrt(x^2+1)};
        
        % Etichetta funzione (posizionata al vertice per chiarezza)
        \node[blue, anchor=south] at (axis cs:2.2, 3) {$f(x)$};

        % (Opzionale) Evidenziare il minimo in (0,1)
        %\addplot[only marks, mark=*, mark size=1.5pt] coordinates {(0,1)};

        \end{axis}
    \end{tikzpicture}
\end{center}

\newpage

\addcontentsline{toc}{subsection}{Definizione di Polinomio di Taylor}
\begin{definizione}{Polinomio di Taylor}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo $f:I\to \mathbb{R}$. $x_0\in I$, se $f$ è derivabile n volte allora si dice \textbf{polinomio di Taylor di grado n centrato in $x_0$} (e si indica con $T_{n,x_0}$) il seguente polinomio
    \[
        T_{n,x_0}[f](x) = \sum_{k=0}^{n} \dfrac{f^{(k)}(x_0)}{k!}(x-x_0)^k    
    \] 
\end{definizione}
Vediamo un paio di esempi. Prendiamo $f(x) = e^x$ con $x_0=0$. Proviamo a calcolare le prime derivate per vedere se esiste un qualche pattern
\[
\dfrac{d^0}{dx^0}(f(x)) = e^x \implies e^0 = 1
\]\[
\dfrac{d^1}{dx^1}(f(x)) = e^x \implies e^0 = 1
\]\[
\dfrac{d^2}{dx^2}(f(x)) = e^x \implies e^0 = 1
\]\[
\dfrac{d^3}{dx^3}(f(x)) = e^x \implies e^0 = 1
\]
Notiamo quindi che la derivata di $e^x$ è sempre $1$, quindi scopriamo che il polinomio di taylor per $e^x$ centrato in $x=0$ di grado n è
\[
T_{n,0}[e^x](x) = \sum_{k=0}^{n} \dfrac{x^k}{k!} 
\]
Quindi facendo qualche esempio abbiamo che
\[
\begin{array}{c@{\qquad}c}
    T_{0,0}[e^x](x) = 1  & T_{2,0}[e^x](x) = 1 + x + \frac{1}{2}x^2 \\
 T_{1,0}[e^x](x) = 1 + x & T_{3,0}[e^x](x) = 1 + x + \frac{1}{2}x^2+ \frac{1}{6}x^3 
\end{array}
\]
E così via. Vediamo altri sviluppi di Taylor interessanti. Vediamo $f(x)=\sin(x)$ in $x=0$
\[
\renewcommand{\arraystretch}{2.5} % <--- QUESTO è il comando magico che distanzia le righe
\begin{array}{l @{\hspace{1cm}} l} 
\dfrac{d^0}{dx^0}(f(0)) = \sin(0) = 0  & \dfrac{d^4}{dx^4}(f(0)) = \sin(0) = 0 \\
\dfrac{d^1}{dx^1}(f(0)) = \cos(0) = 1 & \dfrac{d^5}{dx^5}(f(0)) = \cos(0) = 1 \\
\dfrac{d^2}{dx^2}(f(0)) = -\sin(0) = 0 & \dfrac{d^6}{dx^6}(f(0)) = -\sin(0) = 0  \\
\dfrac{d^3}{dx^3}(f(0)) = -\cos(0) = -1 & \dfrac{d^7}{dx^7}(f(0)) = -\cos(0) = -1\\
\end{array}
\]
Notiamo che il seno segue un andamento "ciclico", infatti ad ogni derivata pari vale 0, e quindi nel polinomio di taylor non ci sarà. E poi quelle dispari hanno un segno alterno. Quindi il polinomio di taylor sarà
\[
T_{2n+1,0}[\sin(x)](x) = \sum_{k=0}^{n} (-1)^k\dfrac{x^{2k+1}}{(2k+1)!} 
\]
\addcontentsline{toc}{subsection}{Proprietà del Polinomio di Taylor}
\begin{teorema}{Proprietà del Polinomio di Taylor}{}
     Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo $f,g:I\to \mathbb{R}$, $x_0\in I$, se $f$ e $g$ sono derivabili n volte  allora 
     \begin{enumerate}[label=(\roman*)]
        \item $T_{n,x_0}[\alpha f + \beta g](x) = \alpha T_{n,x_0}[f](x) + \beta T_{n,x_0}[g](x) \;\;\; \forall \alpha, \beta \in \mathbb{R}$
        \item $T_{n-1,x_0}[f'](x) = (T_{n,x_0}[f](x))'$
        \item $\dfrac{d^k}{dx^k}\left(T_{k,x_0}[f](x)\right) =  f^{(k)}(x_0) \;\;\; \forall k \leq n$
     \end{enumerate}
\end{teorema}
\begin{proof}
    $(i)$ Usando la definizione di polinomio di taylor e la linearlità di derivate e sommatorie abbiamo 
    \begin{align*}
        T_{n,x_0}[\alpha f + \beta g](x) &=  \sum_{k=0}^{n} \dfrac{(\alpha f(x_0) + \beta g(x_0))^{(k)}}{k!}(x-x_0)^k\\
        &=  \sum_{k=0}^{n} \dfrac{\alpha f^{(k)}(x_0) + \beta g^{(k)}(x_0)}{k!}(x-x_0)^k  \\
          &=  \sum_{k=0}^{n} \dfrac{\alpha f^{(k)}(x_0)}{(x_0)}{k!}(x-x_0)^k + \sum_{k=0}^{n} \dfrac{\beta g^{(k)}(x_0)}{k!}(x-x_0)^k  \\ 
          &= \alpha\sum_{k=0}^{n} \dfrac{ f^{(k)}(x_0)}{(x_0)}{k!}(x-x_0)^k + \beta \sum_{k=0}^{n} \dfrac{g^{(k)}(x_0)}{k!}(x-x_0)^k  \\ 
    &= \alpha T_{n,x_0}[f](x) + \beta T_{n,x_0}[g](x)
        \end{align*}

    $(ii)$ Proviamo a calcolare l'espressione alla destra 
    \begin{align*} 
        (T_{n,x_0}[f](x))' &= \left(\sum_{k=0}^{n} \dfrac{f^{(k)}(x_0)}{k!}(x-x_0)^k \right)'  \\
    &= \left(f(x_0)+f'(x_0)(x-x_0) + \dfrac{f''(x_0)}{2!} (x-x_0)^2  + ... +  \dfrac{f^{(n)}(x_0)}{n!} (x-x_0)^n   \right)'\\
    &=     \hspace{0.7cm}0 \hspace{0.21cm}+\hspace{0.21cm}f'(x_0)\hspace{0.475cm} + \hspace{0.475cm}\dfrac{f''(x_0)}{2!} (x-x_0)^1 \cdot 2  + ... +  \dfrac{f^{(n)}(x_0)}{n!} (x-x_0)^{n-1} \cdot n   \\
    &=     f'(x_0) + \dfrac{f''(x_0)}{1!} (x-x_0)^1   + ... +  \dfrac{f^{(n)}(x_0)}{(n-1)!} (x-x_0)^{n-1}  
\end{align*}
    Se ora proviamo a sostituire $g(x) = f'(x)$ avremmo che
    \[
    (T_{n,x_0}[f](x))' = g(x_0)+ \dfrac{g'(x_0)}{1!} (x-x_0)^1   + ... +  \dfrac{g^{(n-1)}(x_0)}{(n-1)!} (x-x_0)^{n-1} 
    \]
    Ora si può notare che questo è il polinomio di Taylor di $g(x)$ di grado $n-1$ in $x_0$, quindi 
    \[
    (T_{n,x_0}[f](x))' = T_{n-1,x_0}[g](x)
    \]
    Ma dato che $g(x)=f'(x)$ abbiamo che
    \[
    (T_{n,x_0}[f](x))' = T_{n-1,x_0}[f'](x)
    \]

    \newpage 
    $(iii)$ Dimostriamo questo punto con l'induzione. Quindi controlliamo per $k=0$, ricordandoci che la derivata zeresima è la funzione stessa.
    \begin{align*}
        \dfrac{d^0}{dx^0}\left(T_{0,x_0}[f](x)\right) &=  f^{(0)}(x_0) \\
        T_{0,x_0}[f](x)&=f(x_0) \\
        \dfrac{f^{(0)}(x_0)}{0!}(x-x_0)^0 &=f(x_0) \\
f(x_0)&=f(x_0)
    \end{align*}
    Quindi è verificato il caso base. Ora diamo per vera la proposiazione $P(k)$, cioè
    \[
    P(k) : \dfrac{d^k}{dx^k}\left(T_{k,x_0}[f](x)\right) =  f^{(k)}(x_0)
    \]
    Controlliamo se è vera $P(k+1)$
    \[
        \dfrac{d^{k+1}}{dx^{k+1}}\left(T_{k+1,x_0}[f](x)\right) =  f^{(k+1)}(x_0) 
    \] \[    \dfrac{d^{k+1}}{dx^{k+1}}\left(T_{k,x_0}[f](x) + \dfrac{f^{(k+1)}(x_0)}{(k+1)!}(x-x_0)^{k+1}\right) =  f^{(k+1)}(x_0) 
     \] \[    \dfrac{d^{k+1}}{dx^{k+1}}\left(T_{k,x_0}[f](x)\right) + \dfrac{d^{k+1}}{dx^{k+1}}\left(\dfrac{f^{(k+1)}(x_0)}{(k+1)!}(x-x_0)^{k+1}\right) = f^{(k+1)}(x_0)
     \] \[    \dfrac{d}{dx}\left(\dfrac{d^{k}}{dx^{k}}\left(T_{k,x_0}[f](x)\right)\right)  +\dfrac{f^{(k+1)}(x_0) }{(k+1)!}\cdot\dfrac{d^{k+1}}{dx^{k+1}}\left((x-x_0)^{k+1}\right) =  f^{(k+1)}(x_0)
    \]
    Notiamo che $\dfrac{d^{k}}{dx^{k}}\left(T_{k,x_0}[f](x)\right)=f^{(k)}(x_0) \in \mathbb{R}$ dato che $P(k)$ è vero. Poi notiamo che 
    \[
    \dfrac{d}{dx}\left((x-x_0)^{k+1}\right)  = (k+1)\cdot (x-x_0)^{k}
    \]
    \[
    \dfrac{d^2}{dx^2}\left((x-x_0)^{k+1}\right)  = \dfrac{d}{dx}\left((k+1)\cdot (x-x_0)^{k}\right) = (k+1)\cdot k \cdot (x-x_0)^{k-1}
    \]
    \[
    \dfrac{d^{n+1}}{dx^{n+1}}\left((x-x_0)^{k+1}\right) = (k+1)\cdot k \cdot (k-1)\cdot ... \cdot 2\cdot 1 \cdot (x-x_0)^0 = (k+1)!
    \]
    Quindi 
    \[    \dfrac{d}{dx}\left(\mathunderline{red}{\dfrac{d^{k}}{dx^{k}}\left(T_{k,x_0}[f](x)\right)}\right)  +\dfrac{f^{(k+1)}(x_0) }{(k+1)!}\cdot\mathunderline{blue}{\dfrac{d^{k+1}}{dx^{k+1}}\left((x-x_0)^{k+1}\right)} =  f^{(k+1)}(x_0)
    \]
    \[    \dfrac{d}{dx}\left(\mathunderline{red}{f^{(k)}(x_0)}\right)  +\dfrac{f^{(k+1)}(x_0) }{(k+1)!}\cdot \mathunderline{blue}{(k+1)!} =  f^{(k+1)}(x_0) 
    \]
    \[
    0 + f^{(k+1)}(x_0) = f^{(k+1)}(x_0)
    \]
    \[
    f^{(k+1)}(x_0) = f^{(k+1)}(x_0)
    \]
\end{proof}

\addcontentsline{toc}{subsection}{Formula di Taylor con Resto di Peano}
\begin{teorema}{Formula di Taylor con Resto di Peano}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo $f:I\to \mathbb{R}$, $x_0\in I$, se $f$ è derivabile n-1 volte su $I$ e derivabile n volte in $x_0$ allora 
    \begin{enumerate}[label=(\roman*)]
        \item $\displaystyle f(x)= T_{n,x_0}[f](x) + o(|x-x_0|^n)$ per $x\to x_0$
        \item $T_{n,x_0}[f](x)$ è l'unico polinomio di grado minore o uguale a $n$ che soddisfa $(i)$.
    \end{enumerate} 
\end{teorema}

\begin{proof}
    Segue solo la dimostrazione di $(i)$.

    Procediamo per induzione, controlliamo per $n=0$
   \begin{align*}
    f(x)&= T_{0,x_0}[f](x) + o(|x-x_0|^0) \text{ per  } x\to x_0\\
    f(x)&= f(x_0) + o(1)  \text{ per  } x\to x_0
   \end{align*}
   Se portiamo al limite notiamo che
   \begin{align*}
    \lim_{x\to x_0} f(x) &= \lim_{x\to x_0} \left(f(x_0) + o(1)\right)\\
    \lim_{x\to x_0} f(x) &= f(x_0) + 0\\
    \lim_{x\to x_0} f(x) &= f(x_0)
   \end{align*}
   Ma questo vuol dire che $f$ è continua in $x_0$, ma dato che è derivabile per ipotesi, allora è anche continua. Quindi il passo baso è verificato, ora data la proposizione $P(n)$ definita come
   \[
   P(n) : f(x)= T_{n,x_0}[f](x) + o(|x-x_0|^n) \text{ per  } x\to x_0 
   \]
   Da cui deduciamo per la definizione di o-piccolo
   \[
   f(x)= T_{n,x_0}[f](x) + o(|x-x_0|^n) \text{ per  } x\to x_0  \iff  \lim_{x\to x_0} \dfrac{f(x)-T_{n,x_0}[f](x)}{|x-x_0|^n} = 0
   \]
   Ora dobbiamo controllare che $P(n+1)$ sia vera, quindi 
   \[
   f(x)= T_{n+1,x_0}[f](x) + o(|x-x_0|^{n+1}) \text{ per  } x\to x_0  \iff  \lim_{x\to x_0} \dfrac{f(x)-T_{n+1,x_0}[f](x)}{|x-x_0|^{n+1}} = 0
   \]
   Quindi basta che controlliamo se il limite tende a 0. Si può notare che il limite in questione soddisfa i requisiti del teorema di  De l'Hôpital, quindi vediamo se esiste il limite del rapporto delle derivate
   \[
   \lim_{x\to x_0} \dfrac{(f(x)-T_{n+1,x_0}[f](x))'}{(|x-x_0|^{n+1})'} = \lim_{x\to x_0} \dfrac{f'(x)-(T_{n+1,x_0}[f](x))'}{(n+1)|x-x_0|^{n}} =\lim_{x\to x_0} \dfrac{f'(x)-(T_{n,x_0}[f'](x))}{(n+1)|x-x_0|^{n}}
   \]
   Ma dato che $P(n)$ è vera scopriamo che
   \[
   \lim_{x\to x_0} \dfrac{f'(x)-(T_{n,x_0}[f'](x))}{(n+1)|x-x_0|^{n}} = \dfrac{1}{n+1}\cdot \lim_{x\to x_0} \dfrac{f'(x)-(T_{n,x_0}[f'](x))}{|x-x_0|^{n}} = \dfrac{1}{n+1}\cdot 0 = 0
   \]
   Quindi il limite tende a 0, come volevamo, e quindi anche il passo induttivo è verificato e quindi è verificato il teorema.
\end{proof}
\newpage
Segue una tabella con i principali sviluppi di Taylor 

\textbf{N.B.} tutti gli sviluppi sono per $x\to 0$

\addcontentsline{toc}{subsection}{Tabella principali Sviluppi di Taylor}
\begin{table}[!h]
    \centering
\begin{tabular}{c|c|c}
Funzione       & Formula di Taylor     & Espansione ai primi termini \\ \hline
$e^x$          & $\displaystyle\sum_{k=0}^{n}\dfrac{x^k}{k!} + o(x^n)$  &   $1+x+\dfrac{x^2}{2} + \dfrac{x^3}{6} + o(x^3) $                          \\
$\log(1+x)$    & $\displaystyle\sum_{k=1}^{n}(-1)^{k+1}\dfrac{x^k}{k} + o((x^n))$ &            $x-\dfrac{x^2}{2} + \dfrac{x^3}{3} -\dfrac{x^4}{4} + o(x^4)$                  \\
$(1+x)^\alpha$ & $\displaystyle\sum_{k=1}^{n}\binom{\alpha}{k}x^k + o(x^n)$             &     $1+\alpha x + \dfrac{\alpha(\alpha-1)}{2}x^2 + o(x^3)$                       \\
$\sin(x)$      & $\displaystyle\sum_{k=0}^{n}(-1)^{k}\dfrac{x^{2k+1}}{(2k+1)!} + o(x^{2n+1})$            &   $x - \dfrac{x^3}{6} +  \dfrac{x^5}{5!} - \dfrac{x^7}{7!} + o(x^7) $                        \\
$\cos(x)$      & $\displaystyle\sum_{k=0}^{n}(-1)^{k}\dfrac{x^{2k}}{(2k)!} + o(x^{2n})$                 &  $1- \dfrac{x^2}{2} +  \dfrac{x^4}{24} - \dfrac{x^6}{6!} + o(x^6)$                            \\
$\sinh(x)$     & $\displaystyle\sum_{k=0}^{n}\dfrac{x^{2k+1}}{(2k+1)!} + o(x^{2n+1})$           &        $x + \dfrac{x^3}{6} +  \dfrac{x^5}{5!} + \dfrac{x^7}{7!} + o(x^7) $                     \\
$\cosh(x)$     &  $\displaystyle\sum_{k=0}^{n}\dfrac{x^{2k}}{(2k)!} + o(x^{2n})$                       &   $1+ \dfrac{x^2}{2} +  \dfrac{x^4}{24} + \dfrac{x^6}{6!} + o(x^6)$                          \\
$\arctan(x)$   &   $\displaystyle\sum_{k=0}^{n}(-1)^{k}\dfrac{x^{2k+1}}{2k+1} + o(x^{2n+1})$                     &        $x - \dfrac{x^3}{3} +  \dfrac{x^5}{5} - \dfrac{x^7}{7} + o(x^7) $                   
\end{tabular}
\end{table}
\textbf{N.B.} il simbolo $\binom{\alpha}{n}$ è l'estensione nei reali del binomio di Newton, e funziona come il binomio classico ma $\alpha \in \mathbb{R}^+$ e  quindi sarà
\[
\binom{\alpha}{n}  = \dfrac{\alpha(\alpha-1)(\alpha-2) \cdot ... \cdot (\alpha - n +1 ) (\alpha - n)}{n!}
\]
Chiaramente $\alpha \leq n$, altrimenti non ha senso.

Taylor ha una applicazione nei limiti, infatti proviamo a rifare un limite che abbiamo dovuto risolvere con  De l'Hôpital (che lo abbiamo dovuto applicare 2 volte), ora proviamo a risolverlo con Taylor, e vediamo che fa risparmiare molto tempo.
\begin{esercizio}{}{}
    Calcolare il seguente limite
    \[
    \lim_{x\to 0} \dfrac{\log(1+x) - x}{x^2}
    \]
\end{esercizio}
Infatti abbiamo appena scoperto con Taylor che 
\[
\log(1+x) = x-\dfrac{x^2}{2} + \dfrac{x^3}{3} -\dfrac{x^4}{4} + o(x^4)
\]
Noi però ci possiamo fermare al termine $x^2$ , di conseguenza il limite diventa 
\[
\lim_{x\to 0} \dfrac{\log(1+x) - x}{x^2} = \lim_{x\to 0} \dfrac{x-\dfrac{x^2}{2}+ o(x^2) - x}{x^2} = \lim_{x\to 0} \dfrac{-\dfrac{x^2}{2}}{x^2} +\dfrac{ o(x^2) }{x^2}  = -\dfrac{1}{2} + 0 = -\dfrac{1}{2}
\]







\section{Integrabilità}

\addcontentsline{toc}{subsection}{Definizione di Partizione}
\begin{definizione}{Partizione}{}
    Sia $a,b \in \mathbb{R}$, $a<b$, si dice una \textbf{Partizione di} $[a,b]$ un insieme della forma
    \[
    P = \{x_0, x_1,x_2, ..., x_{n-1}, x_n\}
    \]
    per un qualche $n \in \mathbb{N}$ tale che
    \[
    \begin{array}{c@{\qquad}@{\qquad}c@{\qquad}@{\qquad}c}
        x_0 = a & x_{i-1} < x_{i}\;\;\; \forall i = 1,2,...,n & x_n = b
    \end{array}
    \]
    Poi si definisce \textbf{Ampiezza} della partizione $P$ la quantità:
    \[
    \text{amp}(P) = \max \{x_i-x_{i-1} : i = 1,2,...,n\}
    \]
\end{definizione}
Per esempio se prendiamo l'insieme $[1,5]$, una possibile partizione è 
\[
P_1 = \{1,2,3,4,5\}
\]
E la sua ampiezza è amp$(P) = 1$, ma un altra partizione possibile era
\[
P_2 = \{1,0.1,0.2,0.3, 5\}
\]
Ora però il concetti di partizione ci serve per capire gli integrali, ma facciamo un passo alla volta. Supponiamo che data una funzione $y=f(x)$, io voglia calcolare l'area tra il grafico della funzione, l'asse x e due rette $x=a$, $x=b$.
\begin{figure}[h]
    \centering
    
    % Definizione della funzione
    \def\func(#1){(0.4*((2*(#1))/3)^3 - 2.5*((2*(#1))/3)^2 + 4*((2*(#1))/3) + 2)}

    \begin{tikzpicture}
        \begin{axis}[
            width=8cm, height=5cm,
            axis lines=middle,
            xlabel={$x$}, ylabel={$y$},
            xmin=-0.5, xmax=7,
            ymin=-0.5, ymax=5,
            % Nasconde i numeri sull'asse Y
            ytick=\empty,
            % Imposta i tick sull'asse X solo dove serve e sostituisce i numeri con lettere
            xtick={1,6},
            xticklabels={$a$, $b$},
            samples=200,
            grid=none,
            clip=false
        ]

        % 1. RIEMPIMENTO AREA (Rosso chiaro)
        \addplot [
            draw=none, 
            fill=green!30, 
            domain=1:6
        ] {\func(x)} \closedcycle;

        % 2. LINEE VERTICALI TRATTEGGIATE (su a e b)
        \draw[dashed, thick] (axis cs:1,0) -- (axis cs:1,{\func(1)});
        \draw[dashed, thick] (axis cs:6,0) -- (axis cs:6,{\func(6)});

        % 3. FUNZIONE (Nero, spesso)
        \addplot[black, thick, domain=-0.2:6.8] {\func(x)};
        
        % 4. ETICHETTA y=f(x) (In alto a destra)
        \node[right] at (axis cs:4.5, 4.5) {$y=f(x)$};

        \end{axis}
    \end{tikzpicture}
\end{figure}

Un modo che possiamo usare per calcolare l'area è di \textbf{approssimare}, quindi possiamo partizionare l'asse x, e formare dei rettangoli nella maniera seguente

\begin{figure}[h]
    \centering
    
    % Definizione della funzione
    \def\func(#1){(0.4*((2*(#1))/3)^3 - 2.5*((2*(#1))/3)^2 + 4*((2*(#1))/3) + 2)}

    \begin{tikzpicture}
        \begin{axis}[
            width=8cm, height=5cm,
            axis lines=middle,
            xlabel={$x$}, ylabel={$y$},
            xmin=-0.5, xmax=7,
            ymin=-0.5, ymax=5,
            % Nasconde i numeri sull'asse Y
            ytick=\empty,
            % Definiamo i tick per ogni intervallo unitario da 1 a 6
            xtick={1,2,3,4,5,6},
            % Etichette generiche per mantenere lo stile "senza numeri"
            xticklabels={$a$, $x_1$, $x_2$, $x_3$, $x_4$, $b$},
            samples=200,
            grid=none,
            clip=false
        ]

        % --- RETTANGOLI (Somme sinistre) ---
        % Intervallo [1,2], altezza f(1)
        \draw[fill=green!30, draw=green] (axis cs:1,0) rectangle (axis cs:2, {\func(1)});
        
        % Intervallo [2,3], altezza f(2)
        \draw[fill=green!30, draw=green] (axis cs:2,0) rectangle (axis cs:3, {\func(2)});
        
        % Intervallo [3,4], altezza f(3)
        \draw[fill=green!30, draw=green] (axis cs:3,0) rectangle (axis cs:4, {\func(3)});
        
        % Intervallo [4,5], altezza f(4)
        \draw[fill=green!30, draw=green] (axis cs:4,0) rectangle (axis cs:5, {\func(4)});
        
        % Intervallo [5,6], altezza f(5)
        \draw[fill=green!30, draw=green] (axis cs:5,0) rectangle (axis cs:6, {\func(5)});

        % --- FUNZIONE ---
        \addplot[black, thick, domain=-0.2:6.8] {\func(x)};
        
        % --- ETICHETTA ---
        \node[right] at (axis cs:5.2, 4.2) {$y=f(x)$};

        \end{axis}
    \end{tikzpicture}
\end{figure}
In questa maniera in più la partizione è "fitta", più l'approssizione è corretta.
\newpage
Quindi con questa idea possiamo creare una \textbf{approssimazione per difetto} e una \textbf{approssimazione per eccesso}. Per creare l'approssimazione per difetto prenderemo l'altezza più piccola in quell'intervallo, mentre per l'approssimazione per eccesso prenderemo l'altezza maggiore. Quindi rimanendo con il grafico di prima le due approssi sono
\begin{figure}[h]
    \centering
    
    % --- DEFINIZIONE DELLA NUOVA FUNZIONE ---
    % La funzione originale era f(t) = 0.4t^3 - 2.5t^2 + 4t + 2
    % La nuova funzione è g(x) = f(2x/3).
    % Sostituiamo l'input (#1) con ((2*(#1))/3) nella macro.
    \def\func(#1){(0.4*((2*(#1))/3)^3 - 2.5*((2*(#1))/3)^2 + 4*((2*(#1))/3) + 2)}

    % --- GRAFICO 1: SOMME INFERIORI (Blue) ---
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=7.5cm, height=5cm,
                axis lines=middle,
                xlabel={$x$}, ylabel={$y$},
                xmin=-0.5, xmax=6.5,
                % Adattiamo i limiti Y alla nuova forma della funzione in questo intervallo
                ymin=0, ymax=4.5, 
                ytick=\empty,
                xtick={1,2,3,4,5,6},
            % Etichette generiche per mantenere lo stile "senza numeri"
            xticklabels={$a$, $x_1$, $x_2$, $x_3$, $x_4$, $b$},
                samples=100,
                grid=none,
                clip=true,
                title=\textbf{Approssimazione per Difetto}
            ]

            % Disegno dei rettangoli INFERIORI (Blue - Minimi)
            
            % [1,2]: Contiene il massimo locale. Funzione cresce poi decresce.
            % Il minimo è all'estremo sinistro x=1.
            \draw[fill=blue!30, draw=blue] (axis cs:1,0) rectangle (axis cs:2, {\func(1)});
            
            % [2,3]: Funzione decrescente. Minimo a destra x=3.
            \draw[fill=blue!30, draw=blue] (axis cs:2,0) rectangle (axis cs:3, {\func(3)});
            
            % [3,4]: Funzione decrescente. Minimo a destra x=4.
            \draw[fill=blue!30, draw=blue] (axis cs:3,0) rectangle (axis cs:4, {\func(4)});
            
            % [4,5]: Contiene il minimo locale (a x approx 4.62).
            % Usiamo il valore del minimo locale (y approx 2.29).
            \draw[fill=blue!30, draw=blue] (axis cs:4,0) rectangle (axis cs:5, 2.29);
            
            % [5,6]: Funzione crescente. Minimo a sinistra x=5.
            \draw[fill=blue!30, draw=blue] (axis cs:5,0) rectangle (axis cs:6, {\func(5)});

            % Funzione in Nero
            \addplot[black, thick, domain=-0.2:6.2] {\func(x)};
            
            \node[anchor=north] at (axis cs:3.5, -0.5) {\textbf{Somme Inferiori}};

            \end{axis}
        \end{tikzpicture}
        
    \end{minipage}
    \hfill
    % --- GRAFICO 2: SOMME SUPERIORI (Rosse) ---
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=7.5cm, height=5cm,
                axis lines=middle,
                xlabel={$x$}, ylabel={$y$},
                xmin=-0.5, xmax=6.5,
                ymin=0, ymax=4.5,
                ytick=\empty,
                xtick={1,2,3,4,5,6},
            % Etichette generiche per mantenere lo stile "senza numeri"
            xticklabels={$a$, $x_1$, $x_2$, $x_3$, $x_4$, $b$},
                samples=100,
                grid=none,
                clip=true,
                title=\textbf{Approssimazione per Eccesso}
            ]

            % Disegno dei rettangoli SUPERIORI (Rossi - Massimi)

            % [1,2]: Contiene il massimo locale (a x approx 1.62).
            % Usiamo il valore del massimo locale (y approx 3.91).
            \draw[fill=red!30, draw=red] (axis cs:1,0) rectangle (axis cs:2, 3.91);
            
            % [2,3]: Funzione decrescente. Massimo a sinistra x=2.
            \draw[fill=red!30, draw=red] (axis cs:2,0) rectangle (axis cs:3, {\func(2)});
            
            % [3,4]: Funzione decrescente. Massimo a sinistra x=3.
            \draw[fill=red!30, draw=red] (axis cs:3,0) rectangle (axis cs:4, {\func(3)});
            
            % [4,5]: Contiene il minimo locale. Il massimo è agli estremi.
            % f(4) > f(5). Massimo a sinistra x=4.
            \draw[fill=red!30, draw=red] (axis cs:4,0) rectangle (axis cs:5, {\func(4)});
            
            % [5,6]: Funzione crescente. Massimo a destra x=6.
            \draw[fill=red!30, draw=red] (axis cs:5,0) rectangle (axis cs:6, {\func(6)});

            % Funzione in Nero
            \addplot[black, thick, domain=-0.2:6.2] {\func(x)};
            
            \node[anchor=north] at (axis cs:3.5, -0.5) {\textbf{Somme Superiori}};

            \end{axis}
        \end{tikzpicture}
    \end{minipage}

\end{figure}

Chiaramente per come le abbiamo definite, l'area blu sarà sempre minore di quella rossa, dato che quella blue sarà sempre minore dell'area effettiva, mentre quella rossa sarà sempre maggiore di dell'area esatta. Ora dobbiamo trovare un metodo per alcolarle. Quindi per l'approssimazione, dovremo calcolare l'altezza più piccola di ogni intervallo (che sarà $\displaystyle \inf_{[x_{i-1},x_i]} f$) moltiplicato per la base del rettangolo (cioè $x_i - x_{i-1}$) e poi sommarle per tutti i rettangoli. Per l'area in eccesso fare lo stesso ragionamento ma con $\displaystyle  \sup_{[x_{i-1},x_i]} f$.

\addcontentsline{toc}{subsection}{Definizione di Somma Inferiore e Superiore}
\begin{definizione}{Somma inferiore e superiore}{}
    Sia $a,b \in \mathbb{R}$, $f:[a,b]\to \mathbb{R}$ limitata e data una partizione $P=\{x_0,x_1,...,x_n\}$ di $[a,b]$ con $n \in \mathbb{N}$. allora definiamo
    \begin{itemize}
        \item \textbf{Somma Inferiore} (indicata come $L(f,P)$) nel seguente modo
        \[
        L(f,P) = \sum_{i=1}^{n} \inf_{[x_{i-1}, x_i]} (f) \cdot (x_i-x_{i-1})
        \]
        \item \textbf{Somma Superiore} (indicata come $U(f,P)$) nel seguente modo
        \[
        U(f,P) = \sum_{i=1}^{n} \sup_{[x_{i-1}, x_i]}(f) \cdot (x_i-x_{i-1})
        \]
    \end{itemize}
\end{definizione}
\textbf{N.B.} la scelta delle lettere $L$ e $U$, deriva dall'inglese \textit{Lower} e \textit{Upper}.

Si può notare subito che per qualsiasi partizione $P$ noi scegliamo, vale
\[
L(f, P) \leq U(f, P) \;\;\;\;\; \forall P
\]
\newpage 
Ora capiamo un procedimento fondamentale, ovvero che più rendiamo fitta la partizione più il valore dell'area si avvicina al valore effettivo. Questo vale sia per l'approssimazione per difetto che per eccesso. Segue un esempio di approssimazione per difetto in cui si addenssa maggioremente la partizione. In rosso è segnato lo scarto dal valore effettivo.
\begin{figure}[h]
    \centering
    % --- DEFINIZIONE DELLA FUNZIONE ---
    \def\func(#1){(0.4*((2*(#1))/3)^3 - 2.5*((2*(#1))/3)^2 + 4*((2*(#1))/3) + 2)}
    
    % --- STILE ASSI CORRETTO ---
    \pgfplotsset{
        myaxisstyle/.style={
            % MODIFICA IMPORTANTE: 
            % Usa \linewidth per riempire la minipage in cui si trova il grafico
            width=\linewidth, 
            height=4cm,        
            axis lines=middle,
            xlabel={$x$}, ylabel={$y$},
            xmin=0.5, xmax=6.5,
            ymin=0, ymax=4.8,
            ytick=\empty,
            xtick={1,6},
            xticklabels={$a$, $b$},
            samples=100,
            grid=none,
            clip=false,
            title style={font=\small\bfseries, yshift=-1ex} % Titolo leggermente più in basso
        }
    }

    % --- GRAFICO 1: Delta x = 1 ---
    \begin{minipage}[t]{0.32\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[myaxisstyle, title={5 Intervalli}]
                % Sfondo ROSSO
                \addplot [draw=none, fill=red!80, domain=1:6] {\func(x)} \closedcycle;

                % Rettangoli VERDI (Manuali)
                \draw[fill=green!30, draw=green!60!black] (axis cs:1,0) rectangle (axis cs:2, {\func(1)});
                \draw[fill=green!30, draw=green!60!black] (axis cs:2,0) rectangle (axis cs:3, {\func(3)});
                \draw[fill=green!30, draw=green!60!black] (axis cs:3,0) rectangle (axis cs:4, {\func(4)});
                \draw[fill=green!30, draw=green!60!black] (axis cs:4,0) rectangle (axis cs:5, 2.288);
                \draw[fill=green!30, draw=green!60!black] (axis cs:5,0) rectangle (axis cs:6, {\func(5)});

                % Funzione
                \addplot[black, thick, domain=0.8:6.2] {\func(x)};
            \end{axis}
        \end{tikzpicture}
    \end{minipage}%
    \hfill
    % --- GRAFICO 2: Delta x = 0.5 ---
    \begin{minipage}[t]{0.32\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[myaxisstyle, title={10 Intervalli}]
                % Sfondo ROSSO
                \addplot [draw=none, fill=red!80, domain=1:6] {\func(x)} \closedcycle;

                % Rettangoli VERDI (Ciclo)
                \foreach \x in {1, 1.5, ..., 5.5} {
                    \pgfmathsetmacro{\xn}{\x + 0.5}
                    \pgfmathsetmacro{\yleft}{\func(\x)}
                    \pgfmathsetmacro{\yright}{\func(\xn)}
                    \pgfmathsetmacro{\hmin}{min(\yleft, \yright)}
                    
                    % Espansione variabili per evitare errori nel loop
                    \edef\temp{
                        \noexpand\draw[fill=green!30, draw=green!60!black] (axis cs:\x,0) rectangle (axis cs:\xn, \hmin);
                    }
                    \temp
                }

                % Funzione
                \addplot[black, thick, domain=0.8:6.2] {\func(x)};
            \end{axis}
        \end{tikzpicture}
    \end{minipage}%
    \hfill
    % --- GRAFICO 3: Delta x = 0.25 ---
    \begin{minipage}[t]{0.32\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[myaxisstyle, title={20 Intervalli}]
                % Sfondo ROSSO
                \addplot [draw=none, fill=red!80, domain=1:6] {\func(x)} \closedcycle;

                % Rettangoli VERDI (Ciclo)
                \foreach \x in {1, 1.25, ..., 5.75} {
                    \pgfmathsetmacro{\xn}{\x + 0.25}
                    \pgfmathsetmacro{\yleft}{\func(\x)}
                    \pgfmathsetmacro{\yright}{\func(\xn)}
                    \pgfmathsetmacro{\hmin}{min(\yleft, \yright)}
                    
                    % Espansione variabili
                    \edef\temp{
                        \noexpand\draw[fill=green!30, draw=green!60!black, thin] (axis cs:\x,0) rectangle (axis cs:\xn, \hmin);
                    }
                    \temp
                }

                % Funzione
                \addplot[black, thick, domain=0.8:6.2] {\func(x)};
            \end{axis}
        \end{tikzpicture}
    \end{minipage}

\end{figure}
 
Con questo scopriamo che

\addcontentsline{toc}{subsection}{Relazione tra Sotto-Partizioni e Partizioni}
\begin{teorema}{Relazione tra Sotto-Partizioni e Partizioni}{}
   Sia $P_1$, $P_2$ partizioni di un intervallo $[a,b]$ e $f:[a,b]\to \mathbb{R}$, allora vale
 \[
 P_1 \subseteq P_2 \implies L(f, P_1) \leq L(f, P_2)
 \] 
 Dato che $P_2$ è più "densa" di $P_1$. Per il ragionamento specchiato sappiamo che 
 \[
  P_1 \subseteq P_2 \implies U(f, P_1) \geq U(f, P_2)
 \] 
\end{teorema}
 
Da questo scopriamo un altro teorema importante:
\addcontentsline{toc}{subsection}{Relazione tra Diverse Partizioni}
\begin{teorema}{Relazione tra Diverse Partizioni}{}
   Sia $P_1$, $P_2$ partizioni di un intervallo $[a,b]$ e $f:[a,b]\to \mathbb{R}$, allora vale
 \[
 L(f, P_1) \leq U(f, P_2)
 \] 
\end{teorema}
\begin{proof}
    Definiamo una nuova partizione $P$ tale che $P_1 \subseteq P$ e $P_2 \subseteq P$. In questo modo con il teorema precedente scopriamo che
    \[
    \begin{array}{c@{\qquad}@{\qquad}c}
        L(f, P_1) \leq L(f, P) & U(f, P_1) \geq U(f, P)
    \end{array}
    \]
    Ma per come abbiamo costruito la somma superiore e somma inferiore sappiamo che 
    \[
    L(f, P) \leq U(f, P) 
    \]
    Da cui
    \[
     L(f, P_1) \leq  L(f, P) \leq U(f, P) \leq U(f, P_2)
    \]
    \[
    L(f, P_1) \leq U(f, P_2)
    \]
    Questo vale per qualsiasi partizione $P_1$ e $P_2$ scegliamo. 
\end{proof}

\newpage 
\addcontentsline{toc}{subsection}{Definizione Integrale Inferiore e Superiore}
\begin{definizione}{Integrale Inferiore e Superiore}{}
    Sia $a,b \in \mathbb{R}$, $f:[a,b]\to \mathbb{R}$ limitata, allora definiamo
    \begin{itemize}
        \item \textbf{Integrale Inferiore} (indicato come $L(f)$) nel seguente modo
        \[
        L(f) = \sup\{L(f, P) : \forall P \text{ partizione di } [a,b]\}
        \]
        \item \textbf{Integrale Superiore} (indicato come $U(f)$) nel seguente modo
        \[
        U(f) = \inf\{U(f, P) : \forall P \text{ partizione di } [a,b]\}
        \]
    \end{itemize}
    
\end{definizione}

\textbf{N.B.} Dato che $(f, P_1) \leq U(f, P_2)$ per qualsiasi partizioni $P_1,P_2$ abbiamo che 
\[
L(f) \leq U(f)
\]

\addcontentsline{toc}{subsection}{Definizione Funzione Integrale secondo Riemann}
\begin{definizione}{Funzione Integrale secondo Riemann}{}
    Sia $a,b \in \mathbb{R}$, $f:[a,b]\to \mathbb{R}$ limitata, si dice \textbf{integrabile} se 
    \[
    L(f) = U(f)
    \]
    In tal caso, tale valore si dice \textbf{integrale di $f$ da $a$ a $b$} e si indica con il seguente simbolo
    \[
    \int_{a}^{b} f(x) \,dx =  L(f) = U(f) \in \mathbb{R}
    \] 
\end{definizione}
Per come abbiamo definito il concetto di integrale dobbiamo stare attenti ad una cosa, vediamo il seguente grafico.
\begin{figure}[!h]
    \centering
\begin{tikzpicture}
    % --- DEFINIZIONE DELLA FUNZIONE ---
    % f(x) = 0.4(x-0.5)(x-3)(x-5)
    \def\func(#1){(0.2*((#1)-0.5)*((#1)-3)*((#1)-5))}

    \begin{axis}[
        width=12cm, height=8cm,
        axis lines=middle,
        xlabel={$x$}, ylabel={$y$},
        xmin=0.5, xmax=6.5,
        ymin=-1.5, ymax=4, % Adattato per focalizzare sulla curva
        ytick=\empty,
        xtick={1,6},
        xticklabels={$a$, $b$},
        samples=100,
        grid=none,
        clip=false
    ]

    % --- GENERAZIONE AUTOMATICA RETTANGOLI (VERDI) ---
    % Ciclo da 1 a 5.75 con passo 0.25
    \foreach \x in {1, 1.25, ..., 5.75} {
        
        % Definiamo l'estremo destro dell'intervallo corrente
        \pgfmathsetmacro{\xn}{\x + 0.25}
        
        % Calcoliamo il valore della funzione agli estremi e nel mezzo
        % per trovare il vero minimo (utile vicino al vertice della parabola)
        \pgfmathsetmacro{\yLeft}{\func(\x)}
        \pgfmathsetmacro{\yRight}{\func(\xn)}
        \pgfmathsetmacro{\yMid}{\func(\x+0.125)}
        
        % L'altezza è il minimo tra i punti campionati (Somma Inferiore)
        \pgfmathsetmacro{\hRect}{min(\yLeft, min(\yRight, \yMid))}

        % Disegno del rettangolo (uso \edef per espandere i valori numerici)
        \edef\temp{
            \noexpand\draw[fill=green!30, draw=green!60!black, thin] 
            (axis cs:\x,0) rectangle (axis cs:\xn, \hRect);
        }
        \temp
    }

    % --- LINEE DI DELIMITAZIONE (a e b) ---
    \draw[dashed] (axis cs:1,0) -- (axis cs:1,{\func(1)});
    \draw[dashed] (axis cs:6,0) -- (axis cs:6,{\func(6)});

    % --- GRAFICO DELLA FUNZIONE ---
    \addplot[black, thick, domain=0.5:6.1] {\func(x)};

    % Etichetta
    \node[right] at (axis cs:5.8, 3.5) {$f(x)$};

    \end{axis}
\end{tikzpicture}
\end{figure}
Con questo grafico notiamo che c'è una porzione di funzione che va sotto l'asse x, e ne calcola l'area come abbiamo sempre visto. Però, per come abbiamo definito la somma superiore e inferiore, abbiamo che l'altezza dei rettangoli ($f(x_i)$) i quei intervalli avrà segno negativo e quindi in quell'intervallo l'avrea complessiva avrà segno negativo, e questo è voluto! Quindi, migliorando la definizione, l'integrale è l'\textbf{area segnata} (che vuol dire che si tiene conto del segno) tra la funzione e l'asse x.

\newpage
Con questo possiamo dedurre che $\displaystyle\int_{0}^{2\pi} \sin(x)\,dx = 0$, dato che l'area sopra l'asse x è uguale (di valore) a quella sotto l'asse x, ma per questa caratteristica dell'integrale avranno segno opposto e si cancelleranno.
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        axis lines = middle,   % Assi centrati nell'origine
        xlabel = $x$,
        ylabel = $y$,
        % Impostiamo i limiti degli assi con un po' di margine
        xmin = -0.5, xmax = 7, % un po' più di 2*pi (che è circa 6.28)
        ymin = -1.2, ymax = 1.2,
        % --- Configurazione Tacche Asse X (Radianti) ---
        % Definiamo le posizioni numeriche delle tacche
        xtick = {0, 1.5708, 3.14159, 4.7123, 6.28318},
        % Sostituiamo i numeri con le etichette simboliche
        xticklabels = {$0$, $\frac{\pi}{2}$, $\pi$, $\frac{3\pi}{2}$, $2\pi$},
        % Posizioniamo le etichette dell'asse x sotto l'asse
        xticklabel style={anchor=north},
        % -----------------------------------------------
        samples = 200,         % Aumentiamo i campioni per una curva liscia
        grid =none,
        width=10cm, height=5.5cm, % Dimensioni del grafico
        % IMPORTANTE: Specifica che le funzioni trig usano i radianti
        trig format plots=rad,
        clip=false             % Evita di tagliare etichette vicine ai bordi
    ]

    % 1. Area Rossa (da 0 a pi)
    % Usiamo draw=none perché vogliamo solo il riempimento
    % \closedcycle chiude il tracciato sull'asse x per riempire l'area
    \addplot [
        draw=none,      
        fill=red!30,    % Colore rosso chiaro
        opacity=0.7,    % Leggera trasparenza per vedere la griglia
        domain=0:pi,    
    ] {sin(x)} \closedcycle;

    % 2. Area Blu (da pi a 2*pi)
    \addplot [
        draw=none,
        fill=blue!30,   % Colore blu chiaro
        opacity=0.7,
        domain=pi:2*pi, 
    ] {sin(x)} \closedcycle;

    % 3. La curva principale della funzione (disegnata sopra le aree)
    \addplot [
        thick,
        color=black,
        domain=0:2*pi
    ] {sin(x)};

    % Aggiungiamo l'etichetta della funzione
    \node at (axis cs: 5, 0.8) {$y=\sin(x)$};

    \end{axis}
\end{tikzpicture}
\end{figure}

Ora dopo tutto questo proviamo a calcolare il nostro primo integrale.
\begin{esercizio}{}{}
    Sia $a,b \in \mathbb{R}$, $a<b$, $f:[a,b]\to \mathbb{R}$, $k \in \mathbb{R}$, $f(x) = k$ $\forall x \in [a,b]$. Calcolare
    \[
    \int_{a}^{b}f(x) \,dx 
    \]
\end{esercizio}
Definiamo una qualsiasi partizione $P = \{x_0, x_1, ... , x_n\}$ di $[a,b]$. Allora calcoliamo la somma superiore e inferiore. Per farlo dobbiamo ricardarci che, dato che la funzione è costante, averemo che
\[
\inf_{I}(f) = \sup_{I}(f) = k \;\;\;\; \forall I \subseteq [a,b]
\]
Quindi
\begin{align*}
   L(f, P) &= \sum_{i=1}^{n} \inf_{[x_{i-1}, x_i]}(f) \cdot (x_{i} - x_{i-1} ) \\
   &= \sum_{i=1}^{n} k \cdot (x_{i} - x_{i-1} ) \\
   &= k \sum_{i=1}^{n} (x_{i} - x_{i-1} )
\end{align*}
Ricordiamo che $\displaystyle\sum_{i=1}^{n} (x_{i} - x_{i-1} )$ è una serie telescopica, e che per definizione di partizione abbiamo che $x_0 = a$ e $x_n= b$
\begin{align*}
    L(f, P) &= k\cdot (x_{n} - x_0) \\
    &= k\cdot (b - a)
\end{align*}
Ragionamento analogo per la somma superiore
\begin{align*}
   U(f, P) &= \sum_{i=1}^{n} \sup_{[x_{i-1}, x_i]}(f) \cdot (x_{i} - x_{i-1} ) \\
   &= \sum_{i=1}^{n} k \cdot (x_{i} - x_{i-1} ) \\
   &= k\cdot (x_{n} - x_0) \\
   &= k\cdot (b - a)
\end{align*}
Ma dato che questi conti sono uguali per ogni partizione $P$ di $[a,b]$ abbiamo che
\[
L(f) = \sup\{L(f,P) : \forall P\} = L(f,P) = k(b-a)
\]
\[
U(f) = \inf\{U(f,P) : \forall P\} = U(f,P) = k(b-a)
\]
Dato che $L(f) = U(f)$ abbiamo che
\[
\int_{a}^{b}k \,dx = k(b-a)
\]
Questo risultato era intuibile anche graficamente, infatti $f(x)$ è una funzione costante e presi $a,b \in \mathbb{R}$ notiamo che
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        axis lines = middle,
        xlabel = $x$,
        ylabel = $y$,
        % Impostiamo i limiti per vedere bene il rettangolo
        xmin = -0.5, xmax = 6.5,
        ymin = -0.5, ymax = 3.5,
        % Tacche sull'asse X: posizionate a 1 e 5, etichettate a e b
        xtick = {1, 5},
        xticklabels = {$a$, $b$},
        % Tacche sull'asse Y: posizionata a 2, etichettata k
        ytick = {2},
        yticklabels = {$k$},
        width=8cm, height=5cm,
        grid=none,
        clip=false
    ]

    % 1. AREA VERDE (Il rettangolo)
    % Disegniamo l'area tra la funzione y=2 e l'asse x, da 1 a 5
    \addplot [
        draw=none, 
        fill=green!30, 
        domain=1:5
    ] {2} \closedcycle;

    % 2. LINEE TRATTEGGIATE DI RIFERIMENTO
    % Linee verticali da a e b fino alla funzione
    \draw[dashed] (axis cs:1,0) -- (axis cs:1,2);
    \draw[dashed] (axis cs:5,0) -- (axis cs:5,2);

    % 3. FUNZIONE COSTANTE
    % Disegnata su tutto il dominio visibile
    \addplot [
        thick,
        black,
        domain=0:6.5
    ] {2};

    % 4. ETICHETTA FUNZIONE
    \node[above] at (axis cs:3, 2) {$f(x)=k$};

    \end{axis}
\end{tikzpicture}
\end{figure}

Si nota che l'area che volevamo calcolare non è altro che un rettangolo, quindi l'area è base per altezza. In questo caso l'altezza è $k$ e la base $b-a$, quindi l'area è $k(b-a)$ come abbiamo trovato con l'integrale. In più, rimarcando sul concetto di area segnata, se $k<0$ abbiamo che il grafico è 
\begin{figure}[!h]
    \centering
\begin{tikzpicture}
    \begin{axis}[
        axis lines = middle,
        xlabel = $x$,
        ylabel = $y$,
        xmin = -0.5, xmax = 6.5,
        ymin = -3.5, ymax = 1.5,
        xtick = {1, 5},
        xticklabels = {$a$, $b$},
        % Opzione per spostare le lettere sopra l'asse:
        xticklabel style={anchor=south, yshift=2pt},
        ytick = {-2},
        yticklabels = {$k$},
        width=8cm, height=5cm,
        grid=none,
        clip=false
    ]

    % 1. Area Purple (Sotto l'asse)
    \addplot [
        draw=none, 
        fill=purple!30, 
        domain=1:5
    ] {-2} \closedcycle;

    % 2. Linee tratteggiate
    \draw[dashed] (axis cs:1,0) -- (axis cs:1,-2);
    \draw[dashed] (axis cs:5,0) -- (axis cs:5,-2);

    % 3. Funzione costante
    \addplot [
        thick,
        black,
        domain=0:6.5
    ] {-2};

    % 4. Etichetta
    \node[below] at (axis cs:3, -2) {$f(x)=k$};

    \end{axis}
\end{tikzpicture}
\end{figure}
L'area è sotto l'asse x, e quindi l'integrale dovrà essere negativo. Ma dato che $k<0$ e $b-a>0$ (questo perchè abbiamo scelto $a<b$) abbiamo che il valore dell'integrale è negativo, cioè $k(b-a) < 0$ come volevamo.
\newpage

\addcontentsline{toc}{subsection}{Caratterizzazione dell'Integrabilità}
\begin{teorema}{Caratterizzazione dell'Integrabilità}{}
    Siano $a,b \in \mathbb{R}$, $a<b$, $f:[a,b]\to \mathbb{R}$ limitata. Tutte le seguenti affermazioni sono equivalenti:
    \begin{enumerate}[label=(\roman*)]
        \item $f$ è integrabile su $[a,b]$
        \item $\exists ! c \in \mathbb{R}$ tale che
        \[
        L(f,P) \leq c \leq U(f,P) \;\;\; \forall P \text{ partizione di } [a,b]
        \]
        \item $\forall \varepsilon >0 \exists P_\varepsilon$ partizione di $[a,b]$ tale che
        \[
            U(f, P_\varepsilon) - L(f, P_\varepsilon) < \varepsilon
        \]
        \item $\exists (P_n)_{n\in \mathbb{N}}$ successione di partizioni di $[a,b]$ tale che
        \[
            \lim_{n\to+\infty} \left(U(f,P_n) - L(f, P_n)\right) = 0
        \]
        Si può scrivere anche come
        \[
        \exists\lim_{n\to+\infty} L(f, P_n) =  \lim_{n\to+\infty} U(f, P_n)  = c \in\mathbb{R}
        \]
        \item $\exists c \in \mathbb{R}$ tale che $\forall \varepsilon > 0$ $\exists \delta > 0$ tale che
        \[
        \text{amp}(P) < \delta \implies \begin{cases}
            c-\varepsilon < L(f, P) \\
            c+\varepsilon < U(f, P) \\
        \end{cases}
        \] 
    \end{enumerate}
    In tutti i casi vale che 
    \[
    c = \int_{a}^{b} f(x) \, dx
    \]
\end{teorema}
Con queste caratterizzazioni possiamo scoprire varie proprietà degli integrali:

\addcontentsline{toc}{subsection}{Integrale di Funzioni pari e Dispari}
\begin{teorema}{Integrale di Funzioni pari e Dispari}{}
    Sia $a \in \mathbb{R}$, $f:[-a,a]\to \mathbb{R}$ limitata. Allora:
    \begin{itemize}
        \item Se $f$ è pari allora vale
        \[
        \int_{-a}^{a} f(x)=2\int_{0}^{a} f(x)
        \]
        \item Se $f$ è dispari allora vale
        \[
        \int_{-a}^{a} f(x)=0
        \]
    \end{itemize}
    
\end{teorema}
\begin{esercizio}{}{}
    Sia $k > 0$, $f:[0,1]\to \mathbb{R}$ definita come $f(x)=kx$, calcolare
    \[
    \int_{0}^{1} f(x) \,dx
    \]
\end{esercizio}
Usiamo la caratterizzazione $(iv)$. Quindi creiamo la successione di partizioni con $n\in\mathbb{N}$
\[
P_n = \left\{ 0, \dfrac{1}{n}, \dfrac{2}{n}, \dfrac{3}{n}, \dots, \dfrac{n-1}{n}, 1 \right\}
\]
Calcoliamo la somma superiore e inferiore, ficordandoci che $f(x)$ è una retta con coefficente angolare positivo, e quindi l'estremo inferiore sarà sempre la funzione calcolata nel punto sinistro dell'intervallo, mentre l'estremo superiore è la funzione calcolata nel punto destro dell'intervallo. Della partizione possiamo dire che i-esimo numero (cioè $x_i$) è $\dfrac{i}{n}$, e per questo distanza tra due punti della partizione ($x_i-x_{i-1}$) è sempre $\dfrac{1}{n}$. Quindi
\begin{align*}
    L(f,P_n) &= \sum_{i=1}^{n} \inf_{[\frac{i-1}{n}, \frac{i}{n}]}(f) \cdot (x_{i}-x_{i-1}) \\
    &= \sum_{i=1}^{n} f\left(\dfrac{i-1}{n}\right) \cdot \dfrac{1}{n} \\
    &= \sum_{i=1}^{n} k \cdot \dfrac{i-1}{n} \cdot \dfrac{1}{n} \\
    &= \dfrac{k}{n^2} \sum_{i=1}^{n}  (i-1)  \\
    &= \dfrac{k}{n^2}\left(\dfrac{n(n+1)}{2}  - n\right) \\
    &= \dfrac{k}{n^2}\cdot \dfrac{n^2-n}{2} \\
    &= \dfrac{k}{2}\cdot \dfrac{n-1}{n}
\end{align*}
Ragionamento anagloro per la somma superiore
\begin{align*}
    U(f,P_n) &= \sum_{i=1}^{n} \sup_{[\frac{i-1}{n}, \frac{i}{n}]}(f) \cdot (x_{i}-x_{i-1}) \\
    &= \sum_{i=1}^{n} k \cdot \dfrac{i}{n} \cdot \dfrac{1}{n} \\
    &= \dfrac{k}{n^2} \sum_{i=1}^{n}  i  \\
    &= \dfrac{k}{n^2}\cdot \dfrac{n(n+1)}{2} \\
    &= \dfrac{k}{2}\cdot \dfrac{n+1}{n}
\end{align*}
Ora notiamo che se calcoliamo i limiti di queste somme
\[
\begin{array}{c@{\qquad}@{\qquad} c}
    \displaystyle\lim_{n\to +\infty} L(f,P_n) = \lim_{n\to +\infty} \dfrac{k}{2}\cdot \dfrac{n-1}{n} = \dfrac{k}{2} & \displaystyle\lim_{n\to +\infty} U(f,P_n) = \lim_{n\to +\infty} \dfrac{k}{2}\cdot \dfrac{n+1}{n} = \dfrac{k}{2}
\end{array}
\]
Notiamo che i limiti sono uguali, e quindi per la $(iv)$ caratterizzazione scopriamo che
\[
\int_{0}^{1} kx \, dx = \dfrac{k}{2}
\]
Proviamo a vedere se anche graficamente saremmo riusciti a trovare questo integrale. Quindi vediamo il grafico di $f(x)$
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        axis lines = middle,   % Assi centrati nell'origine
        xlabel = $x$,          % Etichetta asse x
        ylabel = $y$,          % Etichetta asse y
        xmin = -1, xmax = 2, % Limiti visualizzazione asse x
        ymin = -0.5, ymax = 4, % Limiti visualizzazione asse y
        grid = none,          % Griglia leggera di sfondo
        ytick = {2},
        yticklabels = {$k$},
        width=7cm, height=5cm, % Dimensioni del grafico
        clip = false           % Evita di tagliare elementi vicino ai bordi
    ]

    % --- 1. L'area verde sottesa ---
    % La disegniamo per prima così rimane sullo sfondo.
    % 'domain=0:1' definisce l'intervallo dell'area.
    % '\closedcycle' chiude il percorso tornando all'asse x.
    \addplot [
        draw=none,      % Non disegnare il bordo della linea qui
        fill=green!30,  % Colore di riempimento (verde chiaro)
        domain=0:1,     % Intervallo di integrazione
    ] {2*x} \closedcycle;


    % --- 2. La linea tratteggiata su x=1 ---
    % Disegniamo una linea dal punto (1,0) al punto (1, f(1)), cioè (1,2)
    % Usiamo 'axis cs:' per specificare che usiamo le coordinate del grafico.
    \draw [dashed, thick, black] (axis cs:1, 0) -- (axis cs:1, 2);


    % --- 3. Il grafico della funzione f(x)=2x ---
    % Disegnata per ultima per stare in primo piano.
    \addplot [
        thick,          % Linea spessa
        color=blue,     % Colore blu
        domain=-0.5:1.8 % Dominio visibile della retta
    ] {2*x};

    % (Opzionale) Aggiungiamo un'etichetta per la funzione
    \node at (axis cs:2, 4) [anchor=west] {$f(x)=kx$};

    \node at (axis cs:1, 1.9) [anchor=west] {$(1,k)$};

    \end{axis}
\end{tikzpicture}
\end{figure}

Notiamo quindi che l'area tra la funzione e l'asse x è un triangolo, la cui base è 1 e l'altezza $k$, quindi per calcolare l'area sarà $\frac{1 \cdot k}{2} = \frac{k}{2}$ che è lo stesso risultato dell'integrale.

\addcontentsline{toc}{subsection}{Proprietà dell'integrale}
\begin{teorema}{Proprietà dell'integrale}{}
    Date $a,b \in \mathbb{R}$, $a<b$, $f,g:[a,b]\to \mathbb{R}$ limitate e integrabili su $[a,b]$ allora
    \begin{itemize}
        \item $(\alpha f + \beta g)(x)$ è integrabile su $[a,b]$ $\forall \alpha, \beta \in \mathbb{R}$ e vale
        \[
        \int_{a}^{b}(\alpha f + \beta g)(x) \, dx = \alpha\int_{a}^{b}f(x)\,dx + \beta\int_{a}^{b}g(x)\,dx
        \]
        \item $(f\cdot g)(x)$ è integrabile su $[a,b]$ ma non esiste una formula per calcolarlo l'integrale
        \item sia $c \in [a,b]$ allora $f$ è integrabile su $[a,c]$ e $[c,b]$ e vale
        \[
         \int_{a}^{b} f(x)\,dx = \int_{a}^{c} f(x)\,dx + \int_{c}^{b} f(x)\,dx
        \]
        \item (Monotonia dell'integrale) se $f(x) \leq g(x)$ $\forall x \in [a,b]$ allora
        \[
        \int_{a}^{b} f(x)\,dx \leq \int_{a}^{b} g(x)\,dx
        \]
        \item $|f|:[a,b]\to \mathbb{R}$ è integrabile su $[a,b]$ e vale
        \[
        \int_{a}^{b} |f(x)|\,dx \geq \left|\int_{a}^{b} f(x)\,dx\right|
        \]
    \end{itemize}
\end{teorema}
\newpage

\addcontentsline{toc}{subsection}{Classi di Funzioni Integrabili}
\begin{teorema}{Classi di Funzioni Integrabili}{}
    Date $a,b \in \mathbb{R}$, $a<b$, $f:[a,b]\to \mathbb{R}$ limitata allora
   \begin{itemize}
    \item se $f$ è monotona su $[a,b] \implies f$ è integrabile su $[a,b]$ 
    \item Se esistono $a=x_0 < x_1 < x_2 < ... <x_{n-1} < x_n=b$ con $n \in \mathbb{N}$ tali che $f \in C^0((x_{i-1},x_i))$ per ogni $i = 1,2,...,n$ allora $f$ è integrabile su $[a,b]$.   
   \end{itemize}
\end{teorema}
Analiziamo meglio il secondo punto. Infatti questo vuol dire che noi possiamo avere delle discontinuità di eliminabili o di prima specie (non di seconda specie perchè in quei casi la funzione tende all'infinito e quindi non è  limitata, come richiesto dal teorema) possiamo calcolare lo stesso l'area.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=12cm, height=8cm, % Dimensioni fisse per evitare lo schiacciamento
            axis lines = middle,
            xlabel = $x$,
            ylabel = $y$,
            xmin = -0.5, xmax = 11,
            ymin = -0.5, ymax = 6.5,
            xtick = {1, 3, 5, 8, 9.5},
            xticklabels = {$a$, $x_1$, $x_2$, $x_3$, $b$},
            ytick = \empty,
            grid = none,
            clip = false
        ]

        % --- DEFINIZIONI FUNZIONI ---
        % 1. Parabola (0 a 2)
        \def\funcOne(#1){0.4*(#1-1)^2 + 1}
        
        % 2. Retta discendente (2 a 5)
        \def\funcTwo(#1){-0.3*(#1-2) + 5}

        % 3. ESPONENZIALE DECRESCENTE (5 a 8)
        % Parte da y=4.1 (x=5) e deve arrivare a y=1 (x=8)
        % y = 4.1 * e^(-0.47 * (x-5))
        \def\funcThree(#1){4.1 * exp(-0.4703*((#1)-5))}

        % 4. Retta finale crescente (8 a 9.5)
        % Adattata per ripartire da y=1
        \def\funcFour(#1){1.0*((#1)-8) + 1}


        % --- AREE COLORATE (Sfondo) ---
        \addplot [draw=none, fill=cyan!30, domain=1:3] {\funcOne(x)} \closedcycle;
        \addplot [draw=none, fill=cyan!30, domain=3:5] {\funcTwo(x)} \closedcycle;
        \addplot [draw=none, fill=cyan!30, domain=5:8] {\funcThree(x)} \closedcycle;
        \addplot [draw=none, fill=cyan!30, domain=8:9.5] {\funcFour(x)} \closedcycle;


        % --- LINEE TRATTEGGIATE ---
        \draw[dashed, gray] (axis cs:3,0) -- (axis cs:3,4.7);       % x1
        \draw[dashed, gray] (axis cs:5,0) -- (axis cs:5,4.1);     % x2
        \draw[dashed, gray] (axis cs:8,0) -- (axis cs:8,5);       % x3 (fino al punto isolato)


        % --- GRAFICI FUNZIONI (Blu Spesso) ---
        \addplot [thick, blue, domain=1:3] {\funcOne(x)};
        \addplot [thick, blue, domain=3:5] {\funcTwo(x)};
        \addplot [thick, blue, domain=5:8] {\funcThree(x)};
        \addplot [thick, blue, domain=8:9.5] {\funcFour(x)};


        % --- PUNTI DI DISCONTINUITÀ ---

        % x1: Salto (I Specie)
        \addplot[only marks, mark=*, mark options={fill=blue}, color=blue] coordinates {(3, 2.6)}; 
        \addplot[only marks, mark=o, mark options={thick, fill=white}, color=blue] coordinates {(3, 4.7)};

        % x2: Buco (III Specie - Eliminabile)
        \addplot[only marks, mark=o, mark options={thick, fill=white}, color=blue] coordinates {(5, 4.1)};

        % x3: Punto isolato (III Specie)
        % Il cerchio vuoto ora è basso a (8, 1)
        \addplot[only marks, mark=o, mark options={thick, fill=white}, color=blue] coordinates {(8, 1)};
        % Il punto pieno rimane alto (o dove vuoi tu) a (8, 5)
        \addplot[only marks, mark=*, mark options={fill=blue}, color=blue] coordinates {(8, 5)};

        % Etichetta
        \node[right] at (axis cs:9.5, 2.5) {$f(x)$};

        \draw[dashed] (axis cs:9.5,0) -- (axis cs:9.5,2.55);
        \draw[dashed] (axis cs:1,0) -- (axis cs:1,1);
        \end{axis}
    \end{tikzpicture}
\end{figure}


Quindi notiamo che possiamo calcolare l'integrale di una funzione affinchè la funzione abbiamo un limite finito di discontinuità, perchè in quel caso sia $X=\{x_0, x_1, x_2, ..., x_n\}$ l'insieme di tutte le discontinuità in $[a,b]$ di $f$ abbiamo che
\[
\int_{a}^{b}f(x)\,dx = \sum_{i = 1}^{n}\int_{x_{i-1}}^{x_i}f(x)\,dx
\]
Per ora sembra che tutte le funzioni limitate sono integrabili, ma andiamo c'è una funzione che non è integrabile secondo Riemann: la funzione di Dirichlet. Infatti se proviamo una qualsiasi partizione $P$ dell'intervallo $[a,b]$, e calcoliamo la somma superiore noteremo che per qualsiasi intervallo $[x_{i-1},x_i]$ della partizione potremo sempre trovare un numero razionale (per il teorema della densità dei numeri razionali), e quindi l'estremo superiore sarè sempre 1. Quindi
\[
U(f_D,P) = \sum_{i=1}^{n} \sup_{[x_{i-1},x_i]} (f_D) (x_i-x_{i-1}) = \sum_{i=1}^{n}1\cdot (x_i-x_{i-1}) = x_{n} - x_0 = b-a
\]
Ragionamento analogo per la somma inferiore, solo che per ogni intervallo troveremo un numero irrazionale e quindi l'estremo inferiore sarà 0
\[
L(f_D,P) = \sum_{i=1}^{n} \inf_{[x_{i-1},x_i]} (f_D) (x_i-x_{i-1}) = \sum_{i=1}^{n}0\cdot (x_i-x_{i-1}) = 0
\]
Quindi notiamo che le due somme fanno sempre $b-a$ e $0$ per ogni partizione, quindi affinchè sia integrabile bisogna che siano uguali
\[
L(f_D) = U(f_D) \implies 0 = b-a \implies b=a
\]
Quindi notiamo che è integrabile se $b=a$, ma quando abbiamo definito l'intervallo $[a,b]$, affinchè sia un intervallo deve essere $a<b$, e quindi il valore che abbiamo trovato non è valido, e quindi la funzione di Dirichlet non è integrabile.

\addcontentsline{toc}{subsection}{Teorema della Media Integrale}
\begin{teorema}{Teorema della Media Integrale}{}
     Sia $a,b \in \mathbb{R}$, $a<b$, $f:[a,b]\to \mathbb{R}$ limitata e integrabile, poniamo
     \[
\begin{array}{c@{\qquad}@{\qquad}c}
    \displaystyle m = \inf_{[a,b]}f & \displaystyle M = \sup_{[a,b]}f
\end{array}
     \]
     Allora vale
     \[
     m(b-a) \leq \int_{a}^{b}f(x)\,dx \leq M(b-a)
     \]
     Inoltre se $f \in C^0([a,b])$ allora $\exists c \in [a,b]$ tale che
     \[
     f(c) = \dfrac{\int_{a}^{b}f(x)\,dx}{b-a}
     \]
\end{teorema}
\begin{proof}
    Per definizione di estremo superiore e inferiore vale
    \[
    m \leq f(x) \leq M
    \]
    Da qui usiamo la monotonia dell'integrale
    \[
     \int_{a}^{b}m \,dx \leq \int_{a}^{b} f(x) \,dx\leq \int_{a}^{b}M\,dx
    \]
    Dato che $m$,$M$ sono delle costanti, possiamo calcolare l'integrale
    \[
    m(b-a) \leq \int_{a}^{b}f(x)\,dx \leq M(b-a)
    \]
    Da questo deduciamo che
    \begin{equation}\label{eq:media}
    m \leq \dfrac{\displaystyle\int_{a}^{b}f(x)\,dx}{b-a} \leq M
    \end{equation}
    
    Ora se $f\in C^0([a,b])$ allora per il teorema dei valori intermedi sappiamo che per ogni valore dell'immagine della funzione nell'intervallo ha una controimmagine. Ma dato che (\ref{eq:media}) è compreso tra l'immagine della funzione in $[a,b]$ avremo che $\exists c \in [a,b]$ tale che
    \[
    f(c) = \dfrac{1}{b-a}\int_{a}^{b}f(x)\,dx
    \]
\end{proof}

\addcontentsline{toc}{subsection}{Teorema Fondamentale del Calcolo Integrale}
\begin{teorema}{Teorema Fondamentale del Calcolo Integrale}{}
    Sia $a,b \in \mathbb{R}$, $a<b$, $f:[a,b]\to \mathbb{R}$ limitata e integrabile su $[a,b]$. Definiamo $F:[a,b]\to \mathbb{R}$ ponendo
    \[
    F(x) = \int_{a}^{x}f(t)\,dt
    \]
    Se $f$ è continua su $x_0 \in (a,b)$ allora $F$ è derivabile in $x_0$ e vale
    \[
    F'(x_0) = f(x_0) 
    \] 
\end{teorema}
\begin{proof}
    Per dimostrare questo teorema dobbiamo verificare che
    \[
    F'(x_0) = \lim_{x\to x_0} \dfrac{F(x)-F(x_0)}{x-x_0} = f(x_0)
    \]
    Che usando la caratterizzazione $\varepsilon-\delta$ abbiamo che
    \begin{equation}\label{eq:fond}
        \dfrac{F(x)-F(x_0)}{x-x_0} \in (f(x_0)-\varepsilon,f(x_0)+\varepsilon)\;\;\; \forall x \in (x_0-\delta,x_0+\delta)
    \end{equation}
    Quindi se riusciamo a dimostrare che (\ref{eq:fond}) è vera, allora abbiamo dimostrato il teorema. 
    
    Quindi iniziamo analizzando il rapporto incrementale
\begin{align}
    \dfrac{F(x)-F(x_0)}{x-x_0} &= \dfrac{\mathunderline{red}{\int_{a}^{x}f(t)\,dt}-\int_{a}^{x_0}f(t)\,dt}{x-x_0} \nonumber \\
    &= \dfrac{\mathunderline{red}{\int_{a}^{x_0}f(t)\,dt+\int_{x_0}^{x}f(t)\,dt}-\int_{a}^{x_0}f(t)\,dt}{x-x_0} \nonumber \\
    &= \dfrac{\int_{x_0}^{x}f(t)\,dt}{x-x_0} =\dfrac{1}{x-x_0} \int_{x_0}^{x}f(t)\,dt \label{eq:fond2}
\end{align}
Quindi abbiamo scoperto che il rapporto incrementale non è altro che la media integrale sull'intervallo $[x_0,x]$. Ora analiziamo un altro punto, ovvero che per ipotesi sappiamo che $f$ è continua, quindi possiamo usare la caratterizzazione $\varepsilon-\delta$ in $x_0$, quindi sappiamo che $\forall \varepsilon >0$, $\exists\delta >0$ tale che
\[
f(x) \in (f(x_0) - \varepsilon, f(x_0)+\varepsilon) \;\;\;\;\; \forall x \in (x_0-\delta, x_0+\delta)
\]
\[
f(x_0) - \varepsilon < f(x)< f(x_0)+\varepsilon \;\;\;\;\; \forall x \in (x_0-\delta, x_0+\delta)
\]
Da cui per monotonia dell'integrale sappiamo che
\[
\int_{x_0}^{x}(f(x_0) - \varepsilon) \,dt< \int_{x_0}^{x}f(t)\,dt< \int_{x_0}^{x}(f(x_0)+\varepsilon) \,dt\;\;\;\;\; \forall x \in (x_0-\delta, x_0+\delta)
\]
Dato che $(f(x_0) - \varepsilon)$ e  $(f(x_0) + \varepsilon)$ sono costanti abbiamo che
\[
(f(x_0) - \varepsilon)(x-x_0)< \int_{x_0}^{x}f(t)\,dt<(f(x_0)+ \varepsilon)(x-x_0)
\]
\newpage 
Ora per dividiamo per $x-x_0$, però dobbiamo dividere in due casi $x<x_0$ e $x>x_0$.  


caso $x>x_0$
\[
f(x_0) - \varepsilon <\dfrac{1}{x-x_0}\int_{x_0}^{x}f(t)\,dt< f(x_0) + \varepsilon
\]
Mentre per $x<x_0$
\[
f(x_0) - \varepsilon > \dfrac{1}{x-x_0}\int_{x_0}^{x}f(t)\,dt> f(x_0) + \varepsilon
\]
Ma entrambi i casi li possiamo riscrivere come
\[
\left|\dfrac{1}{x-x_0}\int_{x_0}^{x}f(t)\,dt - f(x_0)\right| < \varepsilon \;\;\;\;\; \forall x \in (x_0-\delta, x_0+\delta)
\]
Ma per (\ref{eq:fond2}) sappiamo che 
\[
\left|\dfrac{F(x)-F(x_0)}{x-x_0} - f(x_0)\right| < \varepsilon \;\;\;\;\; \forall x \in (x_0-\delta, x_0+\delta)
\]
Ma questo verifica (\ref{eq:fond}), e quindi abbiamo verifica il teorema
\end{proof}

\addcontentsline{toc}{subsection}{Corollario del Teorema Fondamentale del Calcolo Integrale}
\begin{corollario}{Corollario del TFCI}{}
    Se $f\in C^0([a,b])$, ed $F$ è definita come sopra, allora 
    \begin{enumerate}[label=(\roman*)]
        \item $F \in C^1([a,b])$
        \item $\displaystyle \int_{c}^{b} f(x)\,dx = F(b) - F(c) \;\;\;\;\; \forall c \in [a,b]$
    \end{enumerate}
\end{corollario}
\begin{proof}
    $(i)$ Per ipotesi sappiamo che $f\in C^0([a,b])$, ma per il TFCI\footnote{TFCI: acronimo di  \textit{Teorema fondamentale del Calcolo Integrale}} sappiamo che $F'(x) = f(x)$ ma allora 
    \[
    f\in C^0([a,b]) \implies F' \in C^0([a,b]) \implies F \in C^1([a,b])
    \]
    $(ii)$ Utiliaziamo la definizione che abbiamo dato a $F(x)$ e prendendo un punto $c \in [a,x]$ abbiamo  
    \begin{align*}
       \int_{a}^{x}f(t)\,dt =\int_{a}^{c}f(t)\,dt + \int_{c}^{x}f(t)\,dt \implies \int_{c}^{x}f(t)\,dt &=\int_{a}^{x}f(t)\,dt-\int_{a}^{c}f(t)\,dt \\
     \int_{c}^{x}f(t)\,dt&=  F(x) - F(c) 
    \end{align*}
    Quindi se scegliamo $x=b$ abbiamo che
    \[
    \int_{c}^{b} f(x)\,dx = F(b) - F(c) \;\;\;\;\; \forall c \in [a,b]
    \]
\end{proof}
\newpage
\addcontentsline{toc}{subsection}{Definizione di Primitiva}
\begin{definizione}{Primitiva}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ intervallo, $f:I\to \mathbb{R}$. Si dice \textbf{Primitiva di  $f$ su $I$} una funzione $F:I\to \mathbb{R}$ tale che
    \[
    F'(x) = f(x) \;\;\;\;\; \forall x \in I
    \]
\end{definizione}
Con questa definizione notiamo che esistono più primitive di una funzione infatti se prendiamo le seguenti funzioni
\[
\begin{array}{c@{\qquad}@{\qquad}c}
    \dfrac{d}{dx}(x^2) = 2x &  \dfrac{d}{dx}(x^2+5) = 2x 
\end{array}
\]
Notiamo che entrambe hanno come derivata $2x$ e quindi entrambe sono primitive di $2x$. Ma non ci sono solo queste, infatti anche $x^2+3$, $x^2-7$ e $x^2+3\pi$ sono primitive però differiscono tutte di una costante. Quindi generalizzando abbiamo il seguente teorema
\addcontentsline{toc}{subsection}{Relazione tra le Primitive di una Funzione}
\begin{teorema}{Relazione tra le Primitive di una Funzione}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ intervallo, $f:I\to \mathbb{R}$. Siano $F;G:I\to\mathbb{R}$ primitive di $f$ su $I$ allora $\exists c \in \mathbb{R}$ tale che
    \[
    G(x) = F(x) + c \;\;\;\;\; \forall x \in I
    \]
\end{teorema}
\begin{proof}
    Definiamo una funzione $H:I \to \mathbb{R}$ definita come $H(x)=G(x)-F(x)$, notiamo che $H$ è derivabile su $I$ dato che è la composizione di funzioni derivabili. Poi notiamo anche che 
    \[
    H'(x) = (G(x)-F(x))' = f(x) - f(x) = 0\;\;\;\;\; \forall x \in I
    \] 
    Per la caratterizzazione delle costanti (corollario del teorema di Lagrange) sappiamo che $\exists c \in \mathbb{R}$ tale che
    \[
    H'(x) = 0 \iff H(x) = c\;\;\;\;\; \forall x \in I
    \]
    Ma allora 
    \[
    H(x) = c \implies  G(x)-F(x) = c \implies G(x)=F(x)+c\;\;\;\;\; \forall x \in I
    \]
\end{proof}
\addcontentsline{toc}{subsection}{Definizione Integrale Indefinito}
\begin{definizione}{Integrale Indefinito}{}
    Chiamiamo \textbf{integrale Indefinito di $f$} l'insieme di tutte e sole le primitive di $f:I\to \mathbb{R}$, e lo indichiamo come
    \[
    \int f(x)\,dx = \{F : F'(x)=f(x)\}
    \] 
\end{definizione}
Vediamo qualche esempio:
\[
\begin{array}{c@{\qquad}@{\qquad}c}
    \displaystyle\int k\,dx = \{kx + c : \forall c \in \mathbb{R}\} &  \displaystyle\int x^\alpha \,dx = \left\{\frac{x^{\alpha+1}}{\alpha+1} + c : \forall c \in \mathbb{R}\right\} \\
    \displaystyle\int e^x\,dx = \{e^x + c : \forall c \in \mathbb{R}\} &  \hspace{0.6cm}\displaystyle\int \cos(x) \,dx = \{\sin(x) + c : \forall c \in \mathbb{R}\} \\
\end{array}
\]
\newpage
Notiamo che per calcolare il un integrale definito, possiamo prendere una qualsiasi primitiva, infatti noi sappiamo che per calcolare l'integrale indefinito usiamo la formula 
\[
\int_{a}^{b} f(x) \,dx = F(b)-F(a)
\]
Però posso usare una qualsiasi primitiva, infatti se avessimo usato la primita $G(x)=F(x)+c$ avremmo avuto
\[
\int_{a}^{b} f(x) \,dx = G(b)-G(a) = (F(b)+c) - (F(a)+c) =  F(b)-F(a)
\]
Quindi notiamo che non cambia nulla nel risultato.

\addcontentsline{toc}{subsection}{Tabella Integrali Principali}
Vediamo le principali primitive
\begin{table}[!h]
    \centering
    % Questo comando aumenta lo spazio verticale tra le righe (2.5 è un buon valore per le frazioni)
    \renewcommand{\arraystretch}{2.5}
    \begin{tabular}{l|c}
        \multicolumn{1}{c|}{Funzione}  & Primitiva \\ \hline
        $\displaystyle\int k\,dx\;\;\; k \in \mathbb{R}$ & $\displaystyle kx+c$ \\
        $\displaystyle\int x^\alpha\,dx \;\;\;\alpha \ne -1$ & $\displaystyle\dfrac{x^{\alpha+1}}{\alpha+1} + c$ \\
        $\displaystyle\int \dfrac{1}{x}\,dx$ & $\displaystyle\log|x| + c$ \\
        $\displaystyle\int \cos(x)\,dx$ & $\displaystyle\sin(x) + c$ \\
        $\displaystyle\int \sin(x)\,dx$ & $\displaystyle-\cos(x) + c$ \\
        $\displaystyle\int \dfrac{1}{1+x^2}\,dx$ & $\displaystyle\arctan(x) + c$ \\
        $\displaystyle\int \dfrac{1}{\sqrt{1-x^2}}\,dx$ & $\displaystyle\arcsin(x) + c$ \\
        $\displaystyle\int e^x \,dx$ & $\displaystyle e^x+c$ \\
        $\displaystyle\int \sinh(x)\,dx$ & $\displaystyle\cosh(x) + c$ \\
        $\displaystyle\int \cosh(x) \,dx$ & $\displaystyle\sinh(x)+c$
    \end{tabular}
\end{table}

\newpage
\addcontentsline{toc}{subsection}{Tecniche di Integrazione: Per Parti}
\begin{teorema}{Integrazione per Parti}{}
    Sia $a,b \in \mathbb{R}$, $a<b$, $f,g:[a,b]\to \mathbb{R}$, $f$ e $g$ derivabili, allora vale
    \[
    \int fg' \, dx = fg - \int f'g \,dx
    \] 
    La versione definita:
    \[
    \int_{a}^{b} fg' \, dx = \left.fg\right|_{x=a}^{x=b} - \int_{a}^{b}  f'g \,dx
    \] 
\end{teorema}
\begin{proof}
    Ricordiamo la formula del prodotto di derivate
    \[
    (fg)' = f'g + fg'
    \]
    Applichiamo l'integrale parte per parte
    \[
    \int(fg)'\,dx = \int f'g\,dx + \int fg'\,dx
    \]
    Per TFCI sappiamo che l'integrale è la funzione inversa della derivata e quindi 
    \[
    fg= \int f'g\,dx + \int fg'\,dx \implies \int fg' \, dx = fg - \int f'g \,dx
    \]
    Ragionamento analogo per la forma determinata. 
\end{proof}

Per ricordare questa formula, vi consiglio di usare la tecnica denominata \textbf{DI Method}. In sostanza dobbiamo creare una tabella 2x2, dove nella prima colonna mettiamo la funzione da derivare, mentre nella seconda si mette la funzione da integrare. Nella seconda riga si calcola la derivata/integrale della funzione sopra. Poi per scrivere la soluzione dobbiamo calcolare il prodotto della diagonale, che sarà fuori dall'integrale, mentre il prodotto dell'ultima riga sarà dentro l'integrale. In più la prima riga parte con un segno positivo, e da lì in poi ogni riga ha segno alterno (vedremo prossimamente come mai si possono avere più righe). Il diagramma di questo metodo:
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}
    
    % Definizione della matrice
    % 'matrix of math nodes' mette tutto in math mode ($...$) automaticamente
    \matrix (m) [matrix of math nodes, row sep=1.2em, column sep=1.5em, minimum width=2em]
    {
        \text{\textbf{Segno}} & \text{\textbf{Deriva}} & \text{\textbf{Integra}} \\
        +                     & f                      & g'                      \\
        -                     & f'                     & g                       \\
    };

    % Linea orizzontale sotto l'intestazione
    \draw (m-1-1.south west) -- (m-1-3.south east);

    % Freccia diagonale (Prodotto parziale)
    % Collega f (riga 2, col 2) a g (riga 3, col 3)
    \draw[->, blue, thick] (m-2-2) -- (m-3-3);

    % Freccia orizzontale (Integrale residuo)
    % Collega f' a g
    \draw[->, red, thick] (m-3-2) -- (m-3-3);

\end{tikzpicture}
\end{figure}

Vi segnalo questo video YouTube dove viene spiegato con molti esempi: \href{https://www.youtube.com/watch?v=2I-_SV8cwsw}{link}


\newpage
\begin{esercizio}{}{}
    Determinare 
    \[
    \int xe^x \,dx
    \]
\end{esercizio}

Usiamo la ternica dell'integrazione per parti, e scegliamo $f(x) = x$, e $g'(x) = e^x$, quindi usando la tecnica DI Method, abbiamo
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}
    
    % Definizione della matrice
    % 'matrix of math nodes' mette tutto in math mode ($...$) automaticamente
    \matrix (m) [matrix of math nodes, row sep=0.6em, column sep=1.5em, minimum width=2em]
    {
        \text{\textbf{Segno}} & \text{\textbf{Deriva}} & \text{\textbf{Integra}} \\
        +                     & x                      & e^x                      \\
        -                     & 1                     & e^x                       \\
    };

    % Linea orizzontale sotto l'intestazione
    \draw (m-1-1.south west) -- (m-1-3.south east);

    % Freccia diagonale (Prodotto parziale)
    % Collega f (riga 2, col 2) a g (riga 3, col 3)
    \draw[->, blue, thick] (m-2-2) -- (m-3-3);

    % Freccia orizzontale (Integrale residuo)
    % Collega f' a g
    \draw[->, red, thick] (m-3-2) -- (m-3-3);

\end{tikzpicture}
\end{figure}
Da cui
\[
\int xe^x \,dx = \mathunderline{blue}{xe^x} - \int \mathunderline{red}{1\cdot e^x} \,dx = xe^x- e^x + c
\]

\begin{esercizio}{}{}
    Determinare 
    \[
    \int x^3e^x \,dx
    \]
\end{esercizio}
Se proviamo ad usare la integrazione per parti avremmo
\[
\int x^3e^x \,dx = x^3e^x - \int 3x^2e^x\,dx
\]
E noi ora dovremmo rifare l'integrazione per parti per altre 3 volte, ma diventa molto scomodo. Infatti è meglio usare il DI Method, infatti noi possiamo estendere la tabella a più righe, e chiaramente ogni riga nuova sarà la derivata o l'integrale di quella sopra. La soluzione all'integrale sarà sempre la somma delle diagonali (stando attenti ai segno che si alternano) fuori dall'integrale, mentre l'ultima riga sarà dentro l'integrale. Quindi per questo esercizio abbiamo
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}
    
    % Definizione della matrice
    % 'matrix of math nodes' mette tutto in math mode ($...$) automaticamente
    \matrix (m) [matrix of math nodes, row sep=0.6em, column sep=1.5em, minimum width=2em]
    {
        \text{\textbf{Segno}} & \text{\textbf{Deriva}} & \text{\textbf{Integra}} \\
        +                     & x^3                      & e^x                      \\
        -                     & 3x^2                     & e^x                       \\
        +                     & 6x                      & e^x                      \\
        -                     & 6                    & e^x                       \\
        +                     & 0                     & e^x                       \\
    };

    % Linea orizzontale sotto l'intestazione
    \draw (m-1-1.south west) -- (m-1-3.south east);

    % Freccia diagonale (Prodotto parziale)
    % Collega f (riga 2, col 2) a g (riga 3, col 3)
    \draw[->, blue, thick] (m-2-2) -- (m-3-3);
    \draw[->, blue, thick] (m-3-2) -- (m-4-3);
    \draw[->, blue, thick] (m-4-2) -- (m-5-3);
    \draw[->, blue, thick] (m-5-2) -- (m-6-3);

    % Freccia orizzontale (Integrale residuo)
    % Collega f' a g
    \draw[->, red, thick] (m-6-2) -- (m-6-3);

\end{tikzpicture}
\end{figure}
\[
\int x^3e^x\,dx = \mathunderline{blue}{x^3e^x} - \mathunderline{blue}{3x^2e^x} + \mathunderline{blue}{6xe^x}-\mathunderline{blue}{6e^x} + \int \mathunderline{red}{0\cdot e^x} \,dx = x^3e^x - 3x^2e^x+6xe^x-6e^x +c
\]

\newpage
\begin{esercizio}{}{}
    Determinare 
    \[
    \int \log(x) \,dx
    \]
\end{esercizio}

In questo caso dobbiamo usare uno stratagemma, ovvero riscriviamo l'integrale come
\[
  \int \log(x) \,dx=\int 1\cdot \log(x) \,dx
\]
Ora possiamo applicare il teorema dell'integrazione per parti, scegliendo $f(x)=\log(x)$ e $g'(x)=1$, quindi 
\[
\int 1\cdot \log(x) \,dx = x\log(x) - \int x \cdot \dfrac{1}{x} \,dx = x\log(x) - \int 1 \,dx = x\log(x) -x+ c
\]

\begin{esercizio}{}{}
    Determinare
    \[
    \int x\arctan(x)\,dx 
    \]
\end{esercizio}
In questo caso applichiamo il teorema con $f(x)=\arctan(x)$ e $g(x)=x$
\[
\int x\arctan(x)\,dx = \dfrac{x^2}{2}\arctan(x) - \int \dfrac{1}{x^2+1} \cdot \dfrac{x^2}{2} \,dx
\]
Ora però notiamo che quello che abbiamo dentro l'integrale è più o meno complesso, ma vedremo bene nella prossime pagine come fare gli integrali di rapporti tra polinomi. Ora però era più furbo scegliere un altra primitiva, infatti noi nella formula dell'integrazioni per parti dobbiamo integrare $g'(x)$, ma come abbiamo sempre visto quando integriamo abbiamo la costante di integrazione, che fino ad esso abbiamo sempre ignorato, ma se scegliamo una primitiva più adeguata potrebbe semplificarci notevolmente i conti. In questo caso se scegliessimo la primitiva con $c=\frac{1}{2}$ avremmo
\[
\int g'(x) = \int x \,dx = \dfrac{x^2}{2} + c = \dfrac{x^2}{2} + \dfrac{1}{2} = \dfrac{x^2+1}{2}
\]
In questo modo 
\begin{align*}
    \int x\arctan(x)\,dx &= \dfrac{x^2+1}{2}\arctan(x) - \int \dfrac{1}{x^2+1} \cdot \dfrac{x^2+1}{2} \,dx \\
    &= \dfrac{x^2+1}{2}\arctan(x) - \int \dfrac{1}{2} \,dx \\
    &= \dfrac{x^2+1}{2}\arctan(x) -\dfrac{x}{2} + c
\end{align*}
\newpage
\addcontentsline{toc}{subsection}{Tecniche di Integrazione: Per Sostituzione}
\begin{teorema}{Integrazione per Sostituzione}{}
    Sia $a,b,c,d \in \mathbb{R}$, $a<b$ e $c<d$, $f:[a,b]\to \mathbb{R}$, $f \in C^0([a,b])$ sia $\varphi : [c,d] \to [a,b]$ tale che $\varphi \in C^1([c,d])$,  $\varphi(c)=a$ e $\varphi(d)=b$, allora vale
    \[
    \int_{c}^{d} f(\varphi(x))\varphi'(x) \,dx = \int_{a}^{b} f(x) \,dx
    \]   
    Forma indeterminata, poniamo $t=\varphi(x)$
     \[
    \int f(\varphi(x))\varphi'(x) \,dx = \int f(t) \,dt
    \] 
\end{teorema}
\begin{proof}
    Ricordiamo la regola della catena delle derivate, e con $F$ indichiamo una primitiva di $f$
    \[
    (F \circ \varphi)'(x) = F'(\varphi(x))\varphi'(x) = f(\varphi(x))\varphi'(x)
    \]
    Se applichiamo l'integrale parte per parte
    \begin{align*}
        \int_{c}^{d} f(\varphi(x))\varphi'(x) \,dx  &= \int_{c}^{d} (F \circ \varphi)'(x) \,dx  \\
        &= \left.(F \circ \varphi)(x)\right|_{x=c}^{x=d}\\
        &= F(\varphi(d)) - F (\varphi(c))\\
        &=F(b) - F(a) \\
        &= \int_{a}^{b} f(x)\,dx
    \end{align*}
\end{proof}

\begin{esercizio}{}{}
    Determinare
    \[
    \int x^6\cos(x^7)\,dx
    \]
\end{esercizio}
Notiamo che dentro il coseno abbiamo $\varphi(x)=x^7$ e notiamo che la sua derivata è $\varphi'(x)=7x^6$, che quasi abbiamo, infatti noi dentro l'integrale abbiamo $x^6$, quindi possiamo "compensare" nel seguente modo
\[
\int x^6\cos(x^7)\,dx = \int\dfrac{1}{7}\cdot 7x^6\cdot \cos(x^7)\,dx = \dfrac{1}{7}\int  \cos(x^7) \cdot 7x^6\,dx
\] 
Ora possiamo usare la formula infatti ponendo $t=x^7$ abbiamo che
\[
\dfrac{1}{7}\int  \cos(x^7) \cdot 7x^6\,dx = \dfrac{1}{7}\int  \cos(t) \,dt =  \dfrac{1}{7}\sin(t) + c 
\]
L'integrale inizialmente era rispetto alla $x$, ora però la soluzione è rispetto alla variabile $t$, quindi torniamo alla variabile $x$ con la sostituzione $t=x^7$
\[
\dfrac{1}{7}\sin(t) + c = \dfrac{1}{7}\sin(x^7) + c
\]
\newpage
In molti esercizi più difficili può essere necessario fare sia l'integrazione per parti che per sostituzione. Vediamo un esempio particolarmente avanzato
\begin{esercizio}{}{}
    Determinare
    \[
    \int x^9\sin(x^5)\,dx
    \]
\end{esercizio}
Se provassimo a fare come l'esercizio precedente, noteremmo che non si potrebbe applicare il principio di sostituzione, infatti se poniamo $\varphi(x)=x^5$, la sua derivata sarebbe $\varphi(x)=5x^4$, ma noi abbiamo $x^9$ quindi così non si può fare, però se fosse $x^4$ anzichè $x^9$ sarebbe
\[
\int x^4\sin(x^5)\,dx = \dfrac{1}{5}\int 5x^4\sin(x^5)\,dx = \dfrac{1}{5}\int \sin(t)\,dt = \dfrac{-1}{5} \cos(t) = \dfrac{-1}{5} \cos(x^5) +c 
\]
Quindi, dato che possiamo fare questo integrale proviamo a "forzare" questo integrale, infatti l'integrale iniziale lo possiamo riscrivere come 
\[
\int x^9\sin(x^5)\,dx = \int (x^5)\cdot(x^4\sin(x^5))\,dx
\]
Ora su questo integrale possiamo applicare l'integrazione per parti ponendo $f(x)=x^5$, $g'(x)=x^4\sin(x^5)$, avremmo
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}
    
    % Definizione della matrice
    % 'matrix of math nodes' mette tutto in math mode ($...$) automaticamente
    \matrix (m) [matrix of math nodes, row sep=0.6em, column sep=1.5em, minimum width=2em]
    {
        \text{\textbf{Segno}} & \text{\textbf{Deriva}} & \text{\textbf{Integra}} \\
        +                     & x^5                      & x^4\sin(x^5)                     \\
        -                     & 5x^4                     & \dfrac{-1}{5} \cos(x^5)                        \\
    };

    % Linea orizzontale sotto l'intestazione
    \draw (m-1-1.south west) -- (m-1-3.south east);

    % Freccia diagonale (Prodotto parziale)
    % Collega f (riga 2, col 2) a g (riga 3, col 3)
    \draw[->, blue, thick] (m-2-2) -- (m-3-3);

    % Freccia orizzontale (Integrale residuo)
    % Collega f' a g
    \draw[->, red, thick] (m-3-2) -- (m-3-3);

\end{tikzpicture}
\end{figure}
\begin{align*}
    \int (x^5)\cdot(x^4\sin(x^5))\,dx &= \mathunderline{blue}{(x^5)\left(\dfrac{-1}{5} \cos(x^5) \right)} - \int \mathunderline{red}{(5x^4)\left(\dfrac{-1}{5} \cos(x^5)\right)} \,dx \\
&= \mathunderline{blue}{\dfrac{-x^5}{5} \cos(x^5)}  + \int \mathunderline{red}{x^4\cos(x^5)} \,dx
\end{align*}
Ora l'integrale rimanente possiamo riusare la tecnica dell'esercizio prima
\[
 \int x^4\cos(x^5) \,dx =  \dfrac{1}{5}\int 5x^4\cos(x^5) \,dx =  \dfrac{1}{5}\int \cos(t) \,dt = \dfrac{1}{5}\sin(t) + c = \dfrac{1}{5}\sin(x^5) + c 
\]
Quindi l'integrale iniziale vale
\[
\int x^9\sin(x^5)\,dx = \dfrac{1}{5}\sin(x^5) - \dfrac{x^5}{5} \cos(x^5) +c
\]
\newpage
\begin{esercizio}{}{}
    Determinare
    \[
    \int \dfrac{1}{x\log(x)}\,dx
    \]
\end{esercizio}
Per risolvere questo integrale dobbiamo riscrivere come
\[
\int \dfrac{1}{x\log(x)}\,dx = \int \dfrac{1}{\log(x)} \cdot \dfrac{1}{x}\,dx
\]
Possiamo notare che se poniamo $\varphi(x)=\log(x)$ la sua derivata sarebbe $\varphi'(x)=\dfrac{1}{x}$, di conseguenza abbiamo
\[
\int \dfrac{1}{\log(x)} \cdot \dfrac{1}{x}\,dx =\int \dfrac{1}{t} \,dt = \log|t| + c = \log|\log(x)| + c  
\]
Con questo esercizio abbiamo notato che 
\[
\int \dfrac{1}{\varphi(x)} \varphi'(x) \,dx = \int \dfrac{\varphi'(x)}{\varphi(x)} \,dx = \log|\varphi(x)| +c
\]
E questo vale per qualsiasi funzione $\varphi(x)$, e vedremo che ci tornerà comodo anche per le prossime tecniche di integrazione.
\begin{esercizio}{}{}
    Determinare
    \[
        \int \dfrac{x}{\sqrt[2]{x-2}}\,dx
    \]
\end{esercizio}
Per questo esercizio dobbiamo usare il principio di sostituzione al contrario, infatti se di solito imponiamo $t=\varphi(x)$, adesso imponiamo $x=\varphi(t)$, quindi se imponiamo
\[
\sqrt[2]{x-2} = t \implies x-2 = t^2 \implies x=t^2-2 \implies \varphi'(t) = t^2-2
\]
Quindi ora facciamo la derivata parte per parte
\[
\varphi'(t)=2t
\]
Quindi l'integrale iniziale diventa
\[
\int \dfrac{x}{\sqrt[2]{x-2}}\,dx = \int \dfrac{t^2-2}{t} 2t\,dt = \int 2t^2 -4 \,dt = \dfrac{2t^3}{3} - 4t + c
\]
Ma come al solito dobbiamo "tornare" alla funzione rispetto alla variabile $x$.
\[
\dfrac{2t^3}{3} - 4t + c = \dfrac{2\sqrt[2]{x-2}^3}{3} - 4\sqrt[2]{x-2} + c = \dfrac{2(x-2)\sqrt[2]{x-2}}{3} - 4\sqrt[2]{x-2} + c
\]

\newpage
\addcontentsline{toc}{subsection}{Tecniche di Integrazione: Frazioni di Polinomi}
Nei prossimi esercizi vedremo degli integrali della forma
\[
\int \dfrac{P(x)}{Q(x)}
\]
Con $P(x)$ e $Q(x)$ due polinomi di grado rispettivamente $\delta P$ e $\delta Q$. Ora dovremo studiare singolarmente tutti i singoli casi facendo un esempio per ogni casistica.
\begin{esercizio}{}{}
    \textbf{Caso $\mathbf{\delta P \geq \delta Q}$} 
    \[
        \int \dfrac{2x^3+x^2+4x-1}{x^2+1}\,dx
    \]
\end{esercizio}
Dato che $\delta P \geq \delta Q$ allora possiamo scrivere il rapposto come
\[
\dfrac{P(x)}{Q(x)} = \dfrac{N(x)}{Q(x)} + R(x)
\]
Con $\delta N < \delta Q$ e $\delta R < \delta Q$, rendendo più semplice l'integrale. Quindi per trovare $N(x)$ e $R(x)$ dobbiamo fare la divisione in colonna dei polinomi.
\begin{table}[!h]
    \centering
\begin{tabular}{rrrr|l}
$2x^3$ & $+x^2$ & $+4x$ & $-1$ & $x^2+1$ \\ \cline{5-5} 
$-2x^3$                    &        & $-2x$ &      & $2x+1$  \\ \cline{1-4}
                           & $x^2$  & $+2x$ & $-1$ &         \\
                           & $-x^2$ &       & $-1$ &         \\ \cline{2-4}
                           &        & $2x$  & $-2$ &        
\end{tabular}
\end{table}
Quindi con questo scopriamo che $R(x)=2x+1$ mentre $N(x)=2x-2$, e quindi possiamo riscrivere l'integrale come
\[
 \int \dfrac{2x^3+x^2+4x-1}{x^2+1} = \int \left(\dfrac{2x-2}{x^2+1} + (2x+1)\right) \,dx
\]
Ora possiamo dividere l'integrale in due, e notiamo che $R(x)$ lo possiamo calcolare dato che non ha più frazioni, mentre per $\frac{N(x)}{Q(x)}$ dobbiamo aspettare le prossime casistiche per capire come fare.
\[
 \int \left(\dfrac{2x-2}{x^2+1} + (2x+1)\right) \,dx =  \int \dfrac{2x-2}{x^2+1}\,dx +  \int 2x+1 \,dx =  \int \dfrac{2x-2}{x^2+1}\,dx +  x^2 + x 
\]
\begin{esercizio}{}{}
    \textbf{Caso $\mathbf{\delta P =\delta Q-1}$} 
    \[
        \int \dfrac{x}{x^2+1}\,dx
    \]
\end{esercizio}
Cerchiamo se al numeratore abbiamo la derivata del denominatore, così da cascare nella casistica del logaritmo. In questo esercizio notiamo che la derivata del denominatore è $Q'(x)=2x$, e quindi possiamo compensare
\[
\int \dfrac{x}{x^2+1}\,dx = \dfrac{1}{2}\int \dfrac{2x}{x^2+1}\,dx = \dfrac{1}{2}\log|x^2+1| + c
\]
\newpage
\begin{esercizio}{}{}
    \textbf{Caso} $\mathbf{\delta P=0,\delta Q=1}$ 
    \[
    \int \dfrac{3}{2x+7}\,dx
    \]
\end{esercizio}
Questo casistica sembra simile alla casistica del logaritmo, ma prima dobbiamo fare una sostituzione
\[
2x+7=t \implies x=\dfrac{t}{2} + \dfrac{-7}{2} \implies \varphi(t) = \dfrac{t}{2} + \dfrac{-7}{2}  \implies \varphi'(t) = \dfrac{1}{2}
\]
Quindi
\[
\int \dfrac{3}{2x+7}\,dx = \int \dfrac{3}{t} \cdot \dfrac{1}{2}\,dt = \dfrac{3}{2}\int \dfrac{1}{t} \,dt= \dfrac{3}{2}\log|t| +c
\]
Come al solito dobbiamo tornare alla variabile $x$
\[
\dfrac{3}{2}\log|t| +c = \dfrac{3}{2}\log|2x+7| +c
\]
Ora ci sono i casi con $\delta Q = 2$, quindi vuol dire che il denominatore è della forma 
\[
Q(x)=ax^2+bx+c
\]
Per comodità però negli esercici useremo sempre \textbf{polinomi monici}, cioè della forma
\[
Q(x)=x^2+bx+c
\]
Qualora negli esercizi futuri dovesse eserci un polinomio al denominatore non monico, basta raccogliere il coefficiente di grado massimo e portarlo fuori dall'integrale. Vediamo un esempio
\[
\int \dfrac{1}{2x^2+4x+4} \,dx = \int \dfrac{1}{2(x^2+2x+2)} \,dx =  \dfrac{1}{2}\int \dfrac{1}{x^2+2x+2} \,dx 
\]
In questa maniera il denominatore è monico e potremo risolverli con le tecniche che vedremo nei prossimi esercizi. 

Poi dato che ora il denominatore è di grado due sarà fondamentale capire se è scomponibile, e questo lo capiamo con il $\Delta=b^2-4ac$. Quindi d'ora in più nei prossimi esercizi, quando scriverò $\Delta$ intenderò il $\Delta$ della formula quadratica del denominatore.
\begin{esercizio}{}{}
    \textbf{Caso} $\mathbf{\delta Q=2, \Delta >0}$
    \[
    \int \dfrac{3x-5}{x^2-3x+2} \,dx
    \] 
\end{esercizio}
Ora dato che $\Delta >0$ vuol dire che possiamo scrivere il denominatore come
\[
x^2-3x+2 = (x-1)(x-2)
\]
Ora quindi possiamo "spaccare" la frazione e cercare $A,B \in \mathbb{R}$ tali che
\[
\dfrac{3x-5}{x^2-3x+2} = \dfrac{A}{x-1} + \dfrac{B}{x-2} 
\]
Per trovare $A$ e $B$ dobbiamo fare il denominatore comune e far sì che i coefficienti dei vari monomi siano uguali
\begin{align*}
    \dfrac{3x-5}{x^2-3x+2} &= \dfrac{A(x-2) + B(x-1)}{(x-1)(x-2)} \\
    &= \dfrac{Ax-2A + Bx-B}{(x-1)(x-2)} \\
    &= \dfrac{(A+B)x +(-2A -B)}{(x-1)(x-2)} 
\end{align*}
Quindi è necessario che entrambi i coefficienti di $x$ al numeratore siano uguali e quindi
\[
3 = A+B
\]
Poi serve anche che il termine noto sia uguale
\[
-5 = -2A -B
\]
Ora possiamo fare un sistemando
\[
\begin{cases}
    3 = A+B \\
    -5 = -2A -B
\end{cases}
 \implies
\begin{cases}
     A=2 \\
     B=1
\end{cases}
 \]
 Quindi possiamo riscrivere l'integrale iniziale come

\[
\int \dfrac{3x-5}{x^2-3x+2} \,dx = \int \left(\dfrac{2}{x-1} + \dfrac{1}{x-2} \right) \,dx = \int \dfrac{2}{x-1} \,dx + \int \dfrac{1}{x-2}  \,dx
\]
Ed ora questa tipologia di fratte le abbiamo già incontrate nel caso $\mathbf{\delta P=0,\delta Q=1}$, quindi sappiamo come risolverle
\[
 \int \dfrac{2}{x-1} \,dx + \int \dfrac{1}{x-2}  \,dx = 2\log|x-1| + \log|x-2| + c
\]
Qualora non ci fosse il termine $x$ non cambierebbe nulla. Per esempio
\[
\int \dfrac{1}{x^2-3x+2} \,dx
\]
Avremmo sempre spaccato la frazione solamente che al momento del sistema metteremo il coefficiente del termine con la $x$ uguale a 0
\[
\dfrac{1}{x^2-3x+2} = \dfrac{A}{x-1} + \dfrac{B}{x-2}  \implies \dfrac{1}{x^2-3x+2} = \dfrac{(A+B)x +(-2A -B)}{(x-1)(x-2)} 
\]
Quindi il sistema sarebbe
\[
\begin{cases}
    0=A+B \\
    1=-2A -B
\end{cases}
\]
E poi i passaggi sono i medesi di prima.
\newpage 
\begin{esercizio}{}{}
     \textbf{Caso} $\mathbf{\delta P = 0, \delta Q=2, \Delta =0}$
     \[
     \int \dfrac{17}{x^2+6x+9}\,dx
     \]
\end{esercizio}
In questo caso, dato che $\Delta=0$ allora abbiamo che possiamo scrivere il denominatore come
\[
Q(x) = (x+3)^2
\]
Con questo possiamo usare la sostituzione, sostituendo 
\[
x+3 = t \implies x=t-3 \implies \varphi(t) = t-3 \implies \varphi'(t)=1
\]
L'integrale diventa 
\begin{align*}
\int \dfrac{17}{x^2+6x+9}\,dx &= \int \dfrac{17}{(x+3)^2}\,dx \\
&= \int \dfrac{17}{t^2} \cdot 1\,dt \\
&= 17\int t^{-2}\,dt \\
&= \dfrac{-17}{t} +c = \dfrac{-17}{x+3} + c    
\end{align*}
\begin{esercizio}{}{}
     \textbf{Caso} $\mathbf{\delta P = 1, \delta Q=2, \Delta =0}$
     \[
     \int \dfrac{3x+4}{x^2+6x+9}\,dx
     \]
\end{esercizio}
In questo caso dobbiamo fare qualcosa di molto simile al caso $\mathbf{\delta Q=2, \Delta >0}$. Infatti dato che $\Delta =0$ possiamo il denominatore l'esercizio precedente. Però la "spaccatura" in questo caso si fa con la moltiplicità, infatti la frazione di questo esercizio diventa
\[
\dfrac{3x+4}{x^2+6x+9} = \dfrac{A}{x+3} + \dfrac{B}{(x+3)^2}
\]
Ed ora la risoluzione è analoga al caso $\mathbf{\delta Q=2, \Delta >0}$.
\begin{align*}
    \dfrac{3x+4}{x^2+6x+9} &= \dfrac{A}{x+3} + \dfrac{B}{(x+3)^2} \\
    &= \dfrac{A(x+3) + B}{x+3} \\
     &= \dfrac{Ax+3A+ B}{x+3} 
\end{align*}
Da cui
\[
\begin{cases}
    3=A \\
    4=3A + B 
\end{cases}
\implies
\begin{cases}
    A=4 \\
    B=-5
\end{cases}
\]
L'integrale diventa
\[
\int \dfrac{3x+4}{x^2+6x+9}\,dx = \int \left(\dfrac{3}{x+3} + \dfrac{-5}{(x+3)^2}\right)\,dx = \int\dfrac{3}{x+3}\,dx + \int \dfrac{-5}{(x+3)^2}\,dx
\]
Ora ci siamo ricondotti alle casistiche $\mathbf{\delta P=0, \delta Q=1}$ e $\mathbf{\delta P = 0, \delta Q=2, \Delta =0}$.
\[
\int\dfrac{3}{x+3}\,dx + \int \dfrac{-5}{(x+3)^2}\,dx = 3\log|x+3| + \dfrac{5}{x+3} + c
\]
Ora vediamo un caso simile a quello che abbiamo appena risolto
\begin{esercizio}{}{}
    \[
    \int \dfrac{2x^2+4}{(x+1)^3} \,dx
    \]
\end{esercizio}
In questo caso dobbiamo scomporre nello stesso modo dell'esercizio precedente, chiaramente aggiungendo anche il termine con potenza 3
\begin{align*}
    \dfrac{2x^2+4}{(x+1)^3} &= \dfrac{A}{x+1} + \dfrac{B}{(x+1)^2} + \dfrac{C}{(x+1)^3} \\
    &= \dfrac{A(x+1)^2 + B(x+1) + C}{(x+1)^3} \\
    &= \dfrac{Ax^2 + 2Ax + A + Bx+B + C}{(x+1)^3} \\
    &= \dfrac{Ax^2 + (2A + B)x+ A +B + C}{(x+1)^3} 
\end{align*}
Da cui
\[
\begin{cases}
    2=A \\
    0 = 2A + B \\
    4 = A+B+C
\end{cases}
\implies
\begin{cases}
    A=2 \\
    B=-4 \\
    C=6
\end{cases}
\]
L'integrale diventa
\begin{align*}
    \int \dfrac{2x^2+4}{(x+1)^3} \,dx &= \int \left(\dfrac{2}{x+1} + \dfrac{-4}{(x+1)^2} + \dfrac{6}{(x+1)^3}\right)\,dx\\
     &= \int \dfrac{2}{x+1}\,dx + \int\dfrac{-4}{(x+1)^2} \,dx+ \int \dfrac{6}{(x+1)^3}\,dx
\end{align*}
Ed ora sappiamo risolverli singolarmente
\[
\int \dfrac{2}{x+1}\,dx + \int\dfrac{-4}{(x+1)^2} \,dx+ \int \dfrac{6}{(x+1)^3}\,dx = 2\log|x+1| + \dfrac{4}{x+1} - \dfrac{3}{(x+1)^2} + c
\]
In maniera più generale, se abbiamo un denominatore della forma $Q(x)=(x-x_0)^n$, lo possiamo riscrivere come
\[
\dfrac{P(x)}{Q(x)} = \dfrac{P(x)}{(x-x_0)^n} = \dfrac{A_1}{x-x_0} + \dfrac{A_2}{(x-x_0)^2} +\dfrac{A_3}{(x-x_0)^3} + ... + \dfrac{A_n}{(x-x_0)^n}
\]
\newpage 
Per semplificarci la vita, calcoliamo un integrale che ci sarà fondamentale per i prossimi due esercizi:
\[
\int \dfrac{1}{x^2+b}\,dx
\]
Questo integrale assomiglia molto all'arcotangente, soltanto che c'è il $b$, quindi possiamo raccoglierlo
\begin{align*}
    \int \dfrac{1}{x^2+b}\,dx = \int \dfrac{1}{b\left(\frac{x^2}{b}+1\right)}\,dx =\dfrac{1}{b} \int \dfrac{1}{\left(\frac{x}{\sqrt{b}}\right)^2+1}\,dx
\end{align*}
Ora applichiamo una sostituzione
\[
\dfrac{x}{\sqrt{b}} = t \implies x = \sqrt{b}t \implies \varphi(t) = \sqrt{b}t \impliedby \varphi'(t) = \sqrt{b}
\]
Da cui
\begin{align*}
   \dfrac{1}{b} \int \dfrac{1}{\left(\frac{x}{\sqrt{b}}\right)^2+1}\,dx = \dfrac{1}{b} \int \dfrac{1}{t^2+1}\sqrt{b}\,dt = \dfrac{1}{\sqrt{b}}\arctan(t) + c =\dfrac{1}{\sqrt{b}}\arctan\left(\dfrac{x}{\sqrt{b}}\right) + c
\end{align*}
\begin{esercizio}{}{}
    \textbf{Caso} $\mathbf{\delta P = 0, \delta Q=2, \Delta < 0}$
    \[
    \int \dfrac{8}{x^2+6x+11} \,dx
    \]
\end{esercizio}
Ora dato che $\Delta <0$, il nostro polinomio non è scomponibile, quindi cerchiamo per quali valori $A,B\in \mathbb{R}$ il denominatore è uguale a
\begin{align*}
x^2+6x+11 &= (x+A)^2 + B \\
    &= x^2+2Ax +A^2 + B 
\end{align*}
Come per gli altri casi, mettiamo a sistema tutte le costanti
\[
\begin{cases}
    6=2A \\
    11=A^2 + B
\end{cases}
\implies
\begin{cases}
    A=3 \\
    B=2
\end{cases}
\]
Quindi l'integrale diventa
\[
    \int \dfrac{8}{x^2+6x+10} \,dx = \int \dfrac{8}{(x+3)^2 +2} \,dx
\]
Ora possiamo fare una sostituzione
\[
x+3=t \implies x=t-3 \implies \varphi(t)=t-3 \implies \varphi'(t) = 1
\]
Da cui
\[
    \int \dfrac{8}{(x+3)^2 +2} \,dx= 8\int \dfrac{1}{t^2 +2} \,dt = \dfrac{8}{\sqrt{2}} \arctan\left(\dfrac{t}{\sqrt{2}}\right) + c = \dfrac{8}{\sqrt{2}} \arctan\left(\dfrac{x+3}{\sqrt{2}}\right) + c
\]
\begin{esercizio}{}{}
    \textbf{Caso} $\mathbf{\delta P = 1, \delta Q=2, \Delta < 0}$
    \[
    \int \dfrac{3x-10}{x^2-8x+19} \,dx
    \]
\end{esercizio}
Il primo passagio che dobbiamo fare è di riscrivere il denominatore come l'esercizio scorso
\begin{align*}
x^2-8x+19 &= (x+A)^2 + B \\
    &= x^2+2Ax +A^2 + B 
\end{align*}
\[
\begin{cases}
    -8=2A \\
    19=A^2 + B
\end{cases}
\implies
\begin{cases}
    A=-4 \\
    B=3
\end{cases}
\]
Quindi l'integrale lo possiamo riscrivere come
\[
\int \dfrac{3x-10}{x^2-8x+19} \,dx = \int \dfrac{3x-10}{(x-4)^2+3} \,dx
\]
Ora possiamo fare una sostituzione
\[
x-4=t \implies x=t+4 \implies \varphi(t)=t+4 \implies\varphi'(t) = 1
\]
Da cui
\begin{align*}
   \int \dfrac{3x-10}{(x-4)^2+3} \,dx = \int \dfrac{3(t+4)-10}{t^2+3} \,dt &= \int \dfrac{3t+2}{t^2+3} \,dt \\ 
 &= \int \left(\dfrac{3t}{t^2+3} +\dfrac{2}{t^2+3} \right)\,dt \\
  &= \int\dfrac{3t}{t^2+3}\,dt+  \int \dfrac{2}{t^2+3}\,dt
\end{align*}
Notiamo che il primo integrale lo possiamo calcolare con la tecnica del logaritmo
\begin{align*}
3\int\dfrac{t}{t^2+3}\,dt = 3\int\dfrac{1}{2} \cdot \dfrac{2t}{t^2+3}\,dt = \dfrac{3}{2}\log|t^2+3| + c &= \dfrac{3}{2}\log|(x-4)^2+3| + c\\
    &= \dfrac{3}{2}\log|x^2-8x+19| + c
\end{align*}
Mentre il secondo integrale è dell'arcotangente
\[
\int \dfrac{2}{t^2+3}\,dt = \dfrac{2}{\sqrt{3}}\arctan\left(\dfrac{t}{\sqrt{3}}\right) + c = \dfrac{2}{\sqrt{3}}\arctan\left(\dfrac{x-4}{\sqrt{3}}\right) + c
\]
Quindi l'integrale iniziale vale
\[
    \int \dfrac{3x-10}{x^2-8x+19} \,dx = \dfrac{3}{2}\log|x^2-8x+19| + \dfrac{2}{\sqrt{3}}\arctan\left(\dfrac{x-4}{\sqrt{3}}\right) + c
    \]

    \addcontentsline{toc}{subsection}{Definizione di Integrale Generalizzato}
\begin{definizione}{Integrale Generalizzato}{}
    Siano $a\in \mathbb{R}\cup \{-\infty\}$, $b\in \mathbb{R}$, $a<b$, $f:(a,b]\to \mathbb{R}$, integrabile su $[c,b]$ per ogni $c \in (a,b]$. Diciamo che \textbf{$f$ è integrabile in senso generalizzato su $(a,b]$ }se 
    \[
    \exists \lim_{c\to a^+} \int_{c}^{b}f(x)\,dx \in \mathbb{R} \cup \{\pm\infty\}
    \]
    In tal caso scriviamo
    \[
    \int_{a}^{b}f(x)\,dx  = \lim_{c\to a^+} \int_{c}^{b}f(x)\,dx 
    \]
    Analogamente,se $a\in \mathbb{R}$, $b\in \mathbb{R}\cup \{+\infty\}$, $a<b$, $f:[a,b)\to \mathbb{R}$, integrabile su $[a,c]$ per ogni $c \in [a,b)$. Diciamo che \textbf{$f$ è integrabile in senso generalizzato su $[a,b)$ }se 
\[
    \exists \lim_{c\to b^-} \int_{a}^{c}f(x)\,dx \in \mathbb{R} \cup \{\pm\infty\}
    \]
    In tal caso scriviamo
    \[
    \int_{a}^{b}f(x)\,dx  = \lim_{c\to b^-} \int_{a}^{c}f(x)\,dx 
    \]
\end{definizione}
Vediamo l'esempio più classico. Infatti se volessimo calcolare 
\[
\int_{0}^{1}\dfrac{1}{x}\,dx
\]
Non potremmo applicare la definizione di integrale secondo Riemann, dato che la funzione non è limitata nell'intervallo di integrazione $[0,1]$. Quindi questa funzione non è integrabile secondo Riemann, però possiamo controllare se lo è in senso generalizzato. Dato che la funzione è integrabile in $[c,1]$ per ogni $c \in (0,1]$, allora 
\begin{align*}
    \int_{0}^{1}\dfrac{1}{x}\,dx &= \lim_{c\to 0^+} \int_{c}^{1}\dfrac{1}{x}\,dx \\
    &= \lim_{c\to 0^+} \left.\log(x)\right|_{x=c}^{x=1} \\
    &= \lim_{c\to 0^+} \left(\log(1) - \log(c)\right)\\
    &= \lim_{c\to 0^+} -\log(c) = +\infty
\end{align*}
Dato che il limite vale $+\infty$, come con le serie, possiamo dire che l'integrale \textbf{Diverge}.


    \addcontentsline{toc}{subsection}{Definizione di Integrale Convergente e Divergente}
\begin{definizione}{Integrale Convergente e Divergente}{}
     Siano $a\in \mathbb{R}\cup \{-\infty\}$, $b\in \mathbb{R}$, $a<b$, $f:(a,b]\to \mathbb{R}$, integrabile su $[c,b]$ per ogni $c \in (a,b]$. Se $f$ è integrabile in senso generalizzato su $(a,b]$, allora sia
    \[
    c = \int_{a}^{b} f(x)\,dx
    \]
    \begin{itemize}
        \item Se $c \in \mathbb{R}$ allora diciamo che \textbf{l'integrale è Convergente}
        \item Se $c \in\{\pm \infty\}$ allora diciamo che \textbf{l'integrale è Divergente}
    \end{itemize}
\end{definizione}

Vediamo degli integrali fondamnetali, che ci serviranno per confrontarli con le altri integrali. Studiamo il carattere dell'integrale al variare di $\alpha \in \mathbb{R}$ 

\[
\int_{0}^{1}\dfrac{1}{x^\alpha}\,dx
\]
Il caso $\alpha=1$ lo abbiamo già fatto ed abbiamo visto che diverge. Ora quindi dato che $\forall \alpha\in \mathbb{R}$ abbiamo che  la funzione è integrabile in $[c, 1]$ per ogni $c \in (0,1)$, allora possiamo integrarlo in senso generalizzato.
\begin{align*}
    \int_{0}^{1}\dfrac{1}{x^\alpha}\,dx &= \lim_{c\to 0^+} \int_{c}^{1}\dfrac{1}{x^\alpha}\,dx \\
&= \lim_{c\to 0^+} \left.\dfrac{x^{1-\alpha}}{1-\alpha}\right|_{x=c}^{x=1} \\
&= \lim_{c\to 0^+} \left(\dfrac{1}{1-\alpha} - \dfrac{c^{1-\alpha}}{1-\alpha}\right) = \lim_{c\to 0^+} \dfrac{1-c^{1-\alpha}}{1-\alpha}
\end{align*}
Ora il termine $c^{1-\alpha}$ tende a $0$ solo se l'esponente è positivo, mentre diverge a $+\infty$ se l'esponente è negativo, quindi
\[
\lim_{c\to 0^+} \dfrac{1-c^{1-\alpha}}{1-\alpha} = \begin{cases}
    \dfrac{1-0}{1-\alpha} & 1-\alpha > 0 \\
    +\infty & 1-\alpha < 0
\end{cases}
=\begin{cases}
    \dfrac{1}{1-\alpha} & \alpha < 1 \\
    +\infty & \alpha > 1
\end{cases}
\]
Quindi combinando anche il fatto che il caso $\alpha=1$ divergeva, abbiamo che l'integrale iniziale 
\[
\int_{0}^{1}\dfrac{1}{x^\alpha}\,dx = \begin{cases}
    \text{Convergente} & \alpha < 1 \\
    \text{Divergente}  & \alpha \geq 1
\end{cases}
\]
Un altro integrale fondamentale per gli esercizi è
\[
\int_{1}^{+\infty}\dfrac{1}{x^\alpha}\,dx
\]
Con $\alpha\in \mathbb{R}$. Come prima l'integrale non è integrabile secondo Riemann dato che l'intervallo non è limitato, quindi controlliamo se è integrabile in senso generalizzato. Notiamo che $\forall M >1$ la funzione è limitata nell'intervallo $[1,M]$, quindi 
\begin{align*}
   \int_{1}^{+\infty}\dfrac{1}{x^\alpha}\,dx &= \lim_{M\to+\infty}\int_{1}^{M}\dfrac{1}{x^\alpha}\,dx \\[1em]
   &=\begin{cases}
    \displaystyle\lim_{M\to+\infty} \left.\dfrac{x^{1-\alpha}-1}{1-\alpha}\right|_{x=1}^{x=M} & \alpha \ne 1 \\[1.5em]
   \displaystyle \lim_{M\to+\infty} \left.\log(x)\right|_{x=1}^{x=M} & \alpha= 1 \\
   \end{cases} \\[1em]
   &=\begin{cases}
    \displaystyle\lim_{M\to+\infty} \left(\dfrac{M^{1-\alpha}}{1-\alpha} -\dfrac{1}{1-\alpha}\right)& \alpha \ne 1 \\[1em]
   \displaystyle \lim_{M\to+\infty} \log(M)-\log(1) & \alpha= 1 
   \end{cases} =\begin{cases}
    \displaystyle\lim_{M\to+\infty} \dfrac{M^{1-\alpha}-1}{1-\alpha}& \alpha \ne 1 \\[1em]
   \displaystyle \lim_{M\to+\infty} \log(M) & \alpha= 1 
   \end{cases}
\end{align*}
  \newpage
  Il caso $\alpha=1$ come prima diverge, mentre il termine $M^{1-\alpha}$ tende a 0 se l'esponente è negativo e tende a $+\infty$ quando l'esponente è positivo.
  \[
  \begin{cases}
 \displaystyle  \dfrac{-1}{1-\alpha} & 1-\alpha < 0\\
    \displaystyle+\infty& 1-\alpha > 0 \\
   \displaystyle +\infty & \alpha= 1 
   \end{cases} \implies
   \int_{1}^{+\infty}\dfrac{1}{x^\alpha}\,dx = \begin{cases}
    \text{Convergente }& \alpha > 1 \\
     \text{Divergente }& \alpha \leq 1 
\end{cases}
  \]


    \addcontentsline{toc}{subsection}{Proprietà degli Integrali Generalizzati}
  \begin{teorema}{Proprietà degli Integrali Generalizzati}{}
    
    
    \begin{itemize}
        \item Siano $a,b\in \mathbb{R}$, $a<b$, $f:[a,b]\to \mathbb{R}$ limitata, allora se $f$ è integrabile secondo Riemann in $[a,b]$, allora $f$ è integrabile su $[a,b]$ in senso generalizzato, da cui
    \[
    \lim_{c\to b^-} \int_{a}^{c} f(x)\,dx =  \lim_{c\to a^+} \int_{c}^{b} f(x)\,dx =\int_{a}^{b} f(x)\,dx
    \]
    \item Se $f,g:(a,b]\to \mathbb{R}$ integrabili in senso generalizzato  e se entrambi convergono, allora $\forall \alpha, \beta \in\mathbb{R}$ vale
    \[
    \int_{a}^{b} (\alpha f + \beta g)(x)\,dx =   \alpha\int_{a}^{b} f(x)\,dx  +\beta\int_{a}^{b}  g(x)\,dx 
    \]
    Discorso analogo per $[a,b)$.
    \item Se $f:(a,b]\to \mathbb{R}$ integrabile in senso generalizzato su $(c,b]$ per ogni $c \in (a,b)$, allora per ogni $\hat{b} \in (a,b)$ i seguenti integrali hanno lo stesso carattere
    \[\begin{array}{c@{\qquad}@{\qquad}c}
        \displaystyle\int_{a}^{b} f(x)\,dx &\displaystyle\int_{a}^{\hat{b}} f(x)\,dx
    \end{array}\]
    Discorso analogo per $[a,b)$.
    \end{itemize}
  \end{teorema}
  Proviamo a capire meglio il terzo punto. Supponiamo di avere l'integrale
  \[
  \int_{0}^{100} \dfrac{1}{x} \,dx
  \]
  Notiamo che soddisfa tutte le ipotesi del teorema, infatti è integrabile in senso generalizzato in $(c,100]$ e per ogni $c \in (0, 100)$. Pertanto per ogni $\hat{b} \in (0,100)$ tutti gli integrali avranno lo stesso carattere. Facendo un esempio pratico, se prendiamo $\hat{b}_1 = 1$,$\hat{b}_2 = 10$, $\hat{b}_3 = 0.1$ allora i seguenti integrali avranno tutti il seguente carattere
  \[
  \begin{array}{c@{\qquad}c@{\qquad}c@{\qquad}c}
    \displaystyle\int_{0}^{100} \dfrac{1}{x} \,dx &\displaystyle\int_{0}^{1} \dfrac{1}{x} \,dx &\displaystyle\int_{0}^{10} \dfrac{1}{x} \,dx &\displaystyle\int_{0}^{0.1} \dfrac{1}{x} \,dx 
  \end{array}
  \] 
  Questo perchè quello che fa divergere o convergere l'integrale è all'avvicinarsi allo 0, quindi noi possiamo prendere qualsiasi estremo destro (in questo caso) che tanto il carattere sarà sempre lo stesso.

    \addcontentsline{toc}{subsection}{Integrali Generalizzati in Entrambi gli Estremi}
\begin{definizione}{Integrali Generalizzati in Entrambi gli Estremi}{}
    Siano $a\in \mathbb{R}\cup \{-\infty\}$, $b\in \mathbb{R}\cup \{+\infty\}$, $a<b$, $f:(a,b)\to \mathbb{R}$, integrabile su $[c,d]$ per ogni $c, d \in (a,b)$ tali che $c < d$. Diciamo che \textbf{$f$ è integrabile in senso generalizzato su $(a,b)$} se $f$ è integrabile su $(a,c]$ e $[c,b)$ per ogni $c \in (a,b)$, e poniamo
    \[
    \int_{a}^{b} f(x) \,dx = \int_{a}^{c} f(x)\,dx +  \int_{c}^{b} f(x)\,dx
    \]
\end{definizione}
Quindi dato il seguente integrale
\[
\int_{0}^{+\infty} \dfrac{1}{x^2} \,dx
\]
Dato che la funzione non è integrabile nè sull'estremo destro nè quello sinistro possiamo spaccare l'integrale in un $c\in(0,+\infty)$ qualsiasi, per esempio $c=1$, e quindi
\[
\int_{0}^{+\infty} \dfrac{1}{x^2} \,dx = \int_{0}^{1} \dfrac{1}{x^2} \,dx + \int_{1}^{+\infty} \dfrac{1}{x^2} \,dx
\]
Il primo integrale diverge a $+\infty$, mentre il secondo converge (come abbiamo visto prima), e quindi
\[
\int_{0}^{+\infty} \dfrac{1}{x^2} \,dx = +\infty
\]

    \addcontentsline{toc}{subsection}{Integrali Generalizzati con Finite Discontinità}
\begin{definizione}{Integrali Generalizzati con Finite Discontinità}{}
    Siano $a\in \mathbb{R}\cup \{-\infty\}$, $b\in \mathbb{R}\cup \{+\infty\}$, $a<b$, $f:(a,b)\to \mathbb{R}$. Diciamo che \textbf{$f$ è integrabile in senso generalizzato su $(a,b)$} se è integreabile in $(x_{i-1},x_i)$ per ogni $i = 1,...,n$ con $a=x_0 < x_1 < x_2 < ... < x_n = b$ e poniamo
    \[
     \int_{a}^{b} f(x) \,dx = \sum_{i=1}^{n}\int_{x_{i-1}}^{x_1} f(x)\,dx
    \]
\end{definizione}
Quindi per esempio se volessimo calcolare l'integrale
\[
 \int_{0}^{2}\left(\dfrac{1}{\sqrt{|x|}}+\dfrac{1}{\sqrt{|x-1|}}+\dfrac{1}{\sqrt{|x-2|}} \right)\,dx
\]
Notiamo che la nostra funzione ha 3 discontinità: $x=0$, $x=1$ e $x=2$ quindi non è integrabile secondo Riemann, però possiamo spezzare l'integrale in 2 integrali, che sono integrabili in senso generalizzato.
\[
\int_{0}^{2}f(x) \,dx = \int_{0}^{1}f(x)\,dx + \int_{1}^{2}f(x)\,dx 
\]
E quindi possiamo calcolare i singoli integrali, e facendo i conti viene fuori
\[
\int_{0}^{2}f(x) \,dx = \int_{0}^{1}f(x)\,dx + \int_{1}^{2}f(x)\,dx = (2+2\sqrt{2}) + (2\sqrt{2} + 2) = 4\sqrt{2} + 4
\]
\newpage

    \addcontentsline{toc}{subsection}{Criterio della Convergenza Assoluta}
\begin{teorema}{Criterio della Convergenza Assoluta}{}
    Siano $a\in \mathbb{R}\cup \{-\infty\}$, $b\in \mathbb{R}\cup \{+\infty\}$, $a<b$, $f:(a,b)\to \mathbb{R}$ integrabile (anche in senso generalizzato), allora se \textbf{l'integrale converge assolutamente} allora \textbf{converge semplicemente}, e scriviamo 
    \[
    \int_{a}^{b} |f(x)| \,dx \in \mathbb{R} \implies \int_{a}^{b} f(x) \,dx \in \mathbb{R} 
    \] 
\end{teorema}
Il criterio della convergenza assoluta da solo non ci torna molto utile, infatti spesso va usato assieme al criterio del confronto 

    \addcontentsline{toc}{subsection}{Criterio del Confronto Integrale}
\begin{teorema}{Criterio del Confronto Integrale}{}
     Siano $a\in \mathbb{R}\cup \{-\infty\}$, $b\in \mathbb{R}\cup \{+\infty\}$, $a<b$, $f:(a,b)\to \mathbb{R}$ integrabile in $[c,d]$ per ogni $c, d \in (a,b)$. Se vale
     \[
     0 \leq f(x) \leq g(x) \;\;\;\;\; \forall x \in(a,b)
     \]
     Allora  se
     \begin{itemize}
        \centering
        \item $\displaystyle \int_{a}^{b} g(x) \,dx  \text{ Converge} \implies\int_{a}^{b} f(x) \,dx  \text{ Converge}  $
   \item $\displaystyle \int_{a}^{b} f(x) \,dx  \text{ Diverge} \implies\int_{a}^{b} g(x) \,dx  \text{ Diverge}  $
    \end{itemize}
\end{teorema}
Infatti se volessimo capire se il seguente integrale converge o diverge
\[
\int_{1}^{+\infty} \dfrac{\sin(x)}{x^2}\,dx
\]
Dovremmo trovare una funzione che sia maggiore o minore di quella dentro l'integrale. Però non potremmo applicare il criterio del confronto, dato che richiede che le funzioni siano positive. Quindi possiamo usare il criterio della convergenza assoluta per rendere tutto positivo il nostro integrale. E poi applichiamo il criterio del confronto con $\dfrac{1}{x^2}$ dato che $|\sin(x)| \leq 1$ per ogni $x \in \mathbb{R}$. Quindi
\[
\int_{1}^{+\infty} \left|\dfrac{\sin(x)}{x^2}\right|\,dx \leq \int_{1}^{+\infty} \dfrac{1}{x^2}\,dx 
\]
Ma il secondo integrale abbiamo già incontrato, e dato che $2>1$ allora il secondo integrale converge. Ma di conseguenza anche l'integrale con i valori assoluti converge per il criterio del confronto. Che poi per il criterio di convergenza assoluta converge anche l'integrale iniziale
\[
\int_{1}^{+\infty} \dfrac{1}{x^2}\,dx  \text{ Converge} \implies\int_{1}^{+\infty} \left|\dfrac{\sin(x)}{x^2}\right|\,dx  \text{ Converge} \implies \int_{1}^{+\infty} \dfrac{\sin(x)}{x^2}\,dx\text{ Converge}
\]

\newpage

    \addcontentsline{toc}{subsection}{Criterio del Confronto Asintotico}
\begin{teorema}{Criterio del Confronto Asintotico}{}
    Siano $a\in \mathbb{R}\cup \{-\infty\}$, $b\in \mathbb{R}\cup \{+\infty\}$, $a<b$, $f,g:(a,b]\to \mathbb{R}$ integrabili su $[c,b]$ per ogni $c \in (a,b]$. Se
    \[
    \exists \lim_{x\to a^+} \dfrac{f(x)}{g(x)} = l \in \mathbb{R} \cup \{\pm \infty\}
    \]
    Allora se
    \begin{itemize}
        \item $l \in \mathbb{R} \setminus \{0\}$ allora i seguenti integrali hanno lo stesso carattere
        \[
        \begin{array}{c@{\qquad}@{\qquad}c}
            \displaystyle\int_{a}^{b} f(x)\,dx &\displaystyle\int_{a}^{b} g(x)\,dx
        \end{array}
        \]
        \item $l = 0$ allora vale $f(x) \leq \varepsilon g(x)$ per qualche $\varepsilon >0$, e quindi per il criterio del confronto deduciamo che:
        \[
        \int_{a}^{b} g(x)\,dx \text{ Converge } \implies \int_{a}^{b} f(x)\,dx \text{ Converge } 
        \]
        \[
        \int_{a}^{b} f(x)\,dx \text{ Diverge } \implies \int_{a}^{b} g(x)\,dx \text{ Diverge } 
        \]
         \item $l \in \{\pm \infty\}$ allora vale $f(x) \geq M g(x)$ per qualche $M >0$, e quindi per il criterio del confronto deduciamo che:
        \[
        \int_{a}^{b} f(x)\,dx \text{ Converge } \implies \int_{a}^{b} g(x)\,dx \text{ Converge } 
        \]
        \[
        \int_{a}^{b} g(x)\,dx \text{ Diverge } \implies \int_{a}^{b} f(x)\,dx \text{ Diverge } 
        \]
    \end{itemize}
    
\end{teorema}
Vediamo un esercizio dove possiamo applicare questo teorema
\begin{esercizio}{}{}
    Determinare il carattere del seguente integrale al variare di $a \in \mathbb{R}$ 
    \[
    \int_{0}^{\frac{\pi}{2}} \dfrac{\cos(x)}{\sin(x)+x^a} \,dx
    \]
\end{esercizio}
In primis notiamo che la nostra funzione non è definita solo per $x=0$, quindi è integrabile su $[c, \frac{\pi}{2}]$ per ogni $c \in (0,\frac{\pi}{2}]$. Quindi possiamo cercare una funzione asintotica alla nostra. Possiamo notare che il seno con lo sviluppo di Taylor diventa:
\[
\sin(x) + x^a = x+o(x) + x^a
\]
Però se $a>1$ notiamo che diventa un $o(x)$. Mentre per $a<1$ abbiamo che $x+o(x)$ diventa un o-piccolo di $x^a$ dato che $x^a$ è il termine più grossolano. Poi per $a=1$ abbiamo che diventa $x+x = 2x$.

\newpage
Di conseguenza la nostra funzione integranda è asintotica a 
\[
\dfrac{\cos(x)}{\sin(x)+x^a} \sim \dfrac{1}{x+x^a} \sim \begin{cases}
    \dfrac{1}{x} & a >1 \\[1em]
    \dfrac{1}{2x} & a=1 \\[1em]
    \dfrac{1}{x^a} & a <1
\end{cases}
\]
Di conseguenza facendo confronto con la funzione che già conosciamo $\dfrac{1}{x^\alpha}$ scopriamo che il nostro integrale converge per $a<1$ e diverge per $a \geq 1$.

\begin{esercizio}{}{}
     Determinare il carattere del seguente integrale al variare di $a \in \mathbb{R}$ 
    \[
    \int_{2}^{3}\dfrac{x\sin^a(x-2)}{\sqrt{x^2-4}}\,dx
    \]
\end{esercizio}
Notiamo che la nostra funzione ha una discontinuità solo in $x=2$, quindi per comodità, possiamo fare una sostituzione con $t=x-2$
\[
\int_{0}^{1}\dfrac{(t+2)\sin^a(t)}{\sqrt{t^2+2t}}\,dx
\]
In questo modo è centrata sullo zero e ci tornerà più comodo dopo per il confronto con la funzione campione. Ora possiamo fare un paio di scomposizioni e usiamo lo sviluppo del seno ed abbiamo
\[
\dfrac{(t+2)\sin^a(t)}{\sqrt{t^2+2t}} = \dfrac{(t+2)\sin^a(t)}{\sqrt{t+2}\sqrt{t}} \sim \dfrac{(2)\sin^a(t)}{\sqrt{2}\sqrt{t}} \sim  \dfrac{\sqrt{2}(t)^a}{\sqrt{t}} = \dfrac{\sqrt{2}}{t^{\frac{1}{2} - a}}
\]
Per noi sappiamo che la funzione 
\[
\dfrac{\sqrt{2}}{t^{\frac{1}{2} - a}} = \begin{cases}
    \text{Converge } &  \dfrac{1}{2} - a < 1\\[1em]
    \text{Diverge } &  \dfrac{1}{2} - a \geq 1
\end{cases} =\begin{cases}
    \text{Converge } &   a > -\dfrac{1}{2}\\[1em]
    \text{Diverge } &    a \leq -\dfrac{1}{2}
\end{cases}
\]
\begin{esercizio}{}{}
     Determinare il carattere del seguente integrale al variare di $a \in \mathbb{R}$ 
    \[
    \int_{\frac{2}{\pi}}^{+\infty}x^a\left(1-\cos\left(\dfrac{1}{x}\right)\right)\,dx
    \]
\end{esercizio}
Notiamo che per ogni valore di $a$ abbiamo che il dominio della funzione integranda è $x\neq  0$, quindi nell'integrale dobbiamo solo studiare il comportamento per $x\to +\infty$. Notiamo che per equivalenze asintotiche abbiamo
\[
x^a\left(1-\cos\left(\dfrac{1}{x}\right)\right) \sim x^a \left(\dfrac{1}{2x^2}\right) = \dfrac{1}{2x^{2-a}} \;\;\; \text{per } x\to +\infty
\] 
Da cui per la serie campione sappiamo 
\[
\dfrac{1}{2x^{2-a}} =\begin{cases}
    \text{Converge } &   2-a > 1\\
    \text{Diverge } &    2-a \leq 1
\end{cases} = 
\begin{cases}
    \text{Converge } &   a < 1\\
    \text{Diverge } &    a \geq 1
\end{cases}
\]
\begin{esercizio}{}{}
     Determinare il carattere del seguente integrale al variare di $a \in \mathbb{R}$ 
    \[
    \int_{0}^{\frac{\pi}{2}}\dfrac{\sin(x)}{\log(1+\sqrt{x})\left(e^{x^a}-1\right)}\,dx
    \]
\end{esercizio}
Notiamo che il dominio di questa funzione è 
\[
\mathbb{D}(f)=\begin{cases}
    x\geq 0 \\
    1+\sqrt{x} >0 \\
    x \neq 0
\end{cases} \implies
\mathbb{D}(f) = \{x\in \mathbb{R}: x>0\}
\]
Pertanto, dobbiamo solo controllare il comportamento in $x=0$, per $a>0$ notiamo tramite un confronto asintotico che
\[
\dfrac{\sin(x)}{\log(1+\sqrt{x})\left(e^{x^a}-1\right)} \sim  \dfrac{x}{\sqrt{x}\cdot x^a}= \dfrac{1}{x^{a+\frac{1}{2} -1}} = \dfrac{1}{x^{a-\frac{1}{2} }} 
\]
Da cui per confronto con la funzione campione
\[
\dfrac{1}{x^{a-\frac{1}{2} }} = \begin{cases}
    \text{Converge } &   a-\dfrac{1}{2} < 1\\[1em]
    \text{Diverge } &     a<-\dfrac{1}{2} \geq 1
\end{cases} =\begin{cases}
    \text{Converge } &   0<a< \dfrac{3}{2} \\[1em]
    \text{Diverge } &     a\geq \dfrac{3}{2}
\end{cases}
\]
Notiamo invece che per $a=0$ abbiamo 
\[
\dfrac{\sin(x)}{\log(1+\sqrt{x})\left(e^{x^0}-1\right)} = \dfrac{\sin(x)}{\log(1+\sqrt{x})\left(e^{1}-1\right)} \sim \dfrac{x}{\sqrt{x}} = \sqrt{x} = \dfrac{1}{x^{-\frac{1}{2}}}
\]
Che per confronto con la funzione campione sappiamo che converge dato che $-\frac{1}{2}<1$. Ora ci manca da studiare per $a<0$, per comodità definiziamo $b = -a$, in modo tale che possiamo studiare il caso $b>0$. Ora se proviamo a fare il limite per $x\to 0^+$ notiamo che
\[
\lim_{x\to 0^+}\dfrac{\sin(x)}{\log(1+\sqrt{x})\left(e^{x^{-b}}-1\right)} \sim \lim_{x\to 0^+}\dfrac{x}{\sqrt{x}\left(e^{\frac{1}{x^b}}-1\right)} =  \lim_{x\to 0^+}\dfrac{\sqrt{x}}{e^{\frac{1}{x^b}}-1} = \bigl[\dfrac{0}{+\infty}\bigr] = 0
\]
Quindi la nostra funzione origina per $b>0$ (cioè $a<0$), la possiamo estendere 
\[
\tilde{f}(x) = \begin{cases}
    \dfrac{\sin(x)}{\log(1+\sqrt{x})\left(e^{x^{-b}}-1\right)} & x \in (0,+\infty) \\
    0 & x = 0 
\end{cases}
\]
Ma quindi la funzione è continua in $[0, \dfrac{\pi}{2}]$ e quindi è integrabile secondo Riemann, pertanto per $a<0$ è convergente.

\section{Equazioni Differenziali Ordinarie (EDO)}
 \addcontentsline{toc}{subsection}{Definizione di EDO}
\begin{definizione}{Equazioni Differenziali Ordinarie}{}
    Una \textbf{Equazioni Differenziali Ordinarie}, abbreviata come \textbf{EDO}, è una qualsiasi equazione in cui appare sia una funzione (incognita) che una, o più, delle sue derivate. Quindi una EDO è della forma 
    \[
    f(t, u(t), u'(t), u''(t), ..., u^{n}(t)) = 0
    \]
    Dove $f$ è una qualsiasi funzione, $u$ è una funzione in funzione di $t$, e $n\in \mathbb{N}$. Si dice \textbf{ordine di una EDO} il grado più alto della derivata della funzione $u(t)$.  
    
    \textbf{N.B.} quindi nelle EDO non bisogna trovare un valore di $t$, ma una funzione che soddisfa la EDO.
\end{definizione}
Facciamo un esempio di EDO:
\[
u'(t) = u(t)
\] 
In questo caso stiamo cercando le funzioni che, le cue derivate sono uguali alla funzione stessa, e per chi ha un pò di occhio potrà notare che una soluzione può essere $\displaystyle u(t) = e^{t}$, ed infatti la defivata è $\displaystyle u'(t) = e^{t}$, pertanto questa soluzione soddisfa la EDO. 

\textbf{Però non è l'unica!} infatti se guardiamo più attentamente scopriamo che più generalmente una soluzione alla EDO è 
\[
u(t) = ce^t \;\;\;\;\; c \in \mathbb{R}
\]
Per questo quando risolviamo una EDO ci aspettiamo di trovare una famiglia di soluzione, che da qui in poi chiameremo \textbf{Integrale Generale}. Può succedere però che di tutte le soluzioni a noi ce ne interessa una soltanto, e questo lo scopriamo con i problemi di Cauchy.

 \addcontentsline{toc}{subsection}{Definizione Problemi di Cauchy}
 \begin{definizione}{Problemi di Cauchy}{}
    Un \textbf{Problema di Cauchy} è un sistema che contiene una EDO e delle \textbf{condizioni iniziali}. Le condizioni iniziali devono essere di numero $n$, dove $n\in\mathbb{N}$ è il grado della EDO. Quindi un problema di Cauchy è della forma
    \[
    \begin{cases}
        f(t, u(t), u'(t), u''(t), ..., u^{n}(t)) = 0 \\
        u(t_0) = u_0 \\ 
        ... \\ 
        u^{n-1}(t_{n-1}) = u_{n-1} \\ 
    \end{cases}
    \]
\end{definizione}
Rimaendo con l'esempio di prima, un possibile problema di Cauchy è
\[
\begin{cases}
    u'(t) = u(t) \\
    u(0)=2
\end{cases}
\]
Quindi abbiamo visto che la soluzione della EDO è $u(t) = ce^t$, ma dobbiamo trovare il valore di $c\in\mathbb{R}$ in modo tale da soddisfare le condizioni iniziali. Quindi
\[
u(0)=2 \implies ce^0 = 2 \implies c=2
\]
Quindi l'unica soluzione che soddisfa il problema di Cauchy è $u(t) = 2e^t$.
\newpage
 \addcontentsline{toc}{subsection}{EDO Lineari \textit{I°} Ordine}
\begin{teorema}{EDO Lineari \textit{I°} Ordine}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo aperto, $t_0\in I$ $u_0 \in \mathbb{R}$. Siano $a,b\in C^0(I)$, allora il seguente problema di Cauchy
    \[
        \begin{cases}
            u'(t) = a(t)u(t) + b(t) \\
            u(t_0) = u_0
        \end{cases}
    \]
    Ha come soluzione unica
    \[
    u(t) = e^{A(t)-A(t_0)} \left(u_0 +\int_{t_0}^{t}e^{-(A(s)-A(t_0))}b(s)\,ds\right)
    \]
    con $A(t)$ una qualsiasi primitiva di $a(t)$
\end{teorema}
\begin{proof}
    Affinchè sia una soluzione, dobbiamo controllare se verifica l'EDO e le condizioni iniziali, quindi proviamo a vedere se verifica l'EDO
    \begin{align*}
        u'(t) &= \dfrac{d}{dt} \left(e^{A(t)-A(t_0)} \left(u_0 +\int_{t_0}^{t}e^{-(A(s)-A(t_0))}b(s)\,ds\right)\right) \\
&= \dfrac{d}{dt} \left(\mathunderline{red}{e^{A(t)-A(t_0)} }\right)\left(u_0 +\int_{t_0}^{t}e^{-(A(s)-A(t_0))}b(s)\,ds\right) + \\
&+ \left(e^{A(t)-A(t_0)} \right)\dfrac{d}{dt} \left(\mathunderline{blue}{u_0 +\int_{t_0}^{t}e^{-(A(s)-A(t_0))}b(s)\,ds}\right) \\
&= \mathunderline{red}{\left(e^{A(t)-A(t_0)} \right)(A'(t) - 0)}\left(u_0 +\int_{t_0}^{t}e^{-(A(s)-A(t_0))}b(s)\,ds\right) + \\
&+ \left(e^{A(t)-A(t_0)} \right)\left(\mathunderline{blue}{e^{-(A(t)-A(t_0))}b(t)}\right)
\end{align*}
Usiamo la definizione di $u(t)$ e ricordiamo che $A'(t) = a(t)$
\begin{align*}
    &= A'(t)\left(\mathunderline{green}{e^{A(t)-A(t_0)} \left(u_0 +\int_{t_0}^{t}e^{-(A(s)-A(t_0))}b(s)\,ds\right)}\right) + e^{(A(t)-A(t_0))-(A(t)-A(t_0))}b(t) \\
    &= a(t)\mathunderline{green}{u(t)} + e^0 b(t) \\
    &= a(t)u(t) +b(t)
\end{align*}
Quindi notiamo che la soluzione risolve la EDO, ora controlliamo se soddisfa le condizioni iniziali
\begin{align*}
    u(t_0) =&  e^{A(t_0)-A(t_0)} \left(u_0 +\int_{t_0}^{t_0}e^{-(A(s)-A(t_0))}b(s)\,ds\right) \\
=& e^0 \left(u_0 + 0\right) = u_0
\end{align*}
Sono verificate anche le condizioni iniziali, quindi abbiamo dimostrato che la soluzione che si siamo dati risolve quel problema di Cauchy.
\end{proof}

Ora dobbiamo dimostrare che questa soluzione è l'unica a quel problema di Cauchy. Per dimostrarlo dobbiamo prima dimostrare un altro teorema: quello di Gr\"onwall

 \addcontentsline{toc}{subsection}{Lemma di Gr\"onwall}
\begin{teorema}{Lemma di Gr\"onwall}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo aperto, $t_0\in I$, $u\in C^0(I)$ e derivabile su $I$. Siano $a\in C^0(I)$ e una sua primitiva $A\in C^1(I)$ allora 
    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $u'(t) \leq a(t) u(t) \;\;\;\; \forall t \in I\implies u(t) \leq u(t_0)e^{A(t)-A(t_0)} \;\;\;\; \forall t \geq t_0$ 
        \item $u'(t) = a(t) u(t) \;\;\;\; \forall t \in I\implies u(t) = u(t_0)e^{A(t)-A(t_0)} \;\;\;\; \forall t \in I$ 
    \end{enumerate}
\end{teorema}
\begin{proof}
    Definiamo $v(t)=e^{A(t)-A(t_0)} $ per ogni $t \in I$, notiamo che 
    \[
    v(t_0) = e^{A(t_0)-A(t_0)} = e^0 = 1
    \]
    \[
    v'(t) =  e^{A(t_0)-A(t_0)}\left(A'(t) - 0\right) =  e^{A(t_0)-A(t_0)} a(t) = a(t)v(t)
    \]
    Dato che $v$ è un esponenziale, sappiamo che $v(t)>0$ per ogni $t \in I$, allora possiamo definire 
    \[
    f(t) = \dfrac{u(t)}{v(t)} \;\;\;\;\; \forall t \in I
    \]
    Ora calcoliamo la derivata di questa nuova funzione
    \begin{align*}
        f'(t) = \dfrac{u'(t)v(t) - u(t)\mathunderline{blue}{v'(t)}}{v^2(t)} = \dfrac{u'(t)v(t) - u(t)\mathunderline{blue}{a(t)v(t)}}{v^2(t)}
    \end{align*}
    $(i)$ Dato che per ipotesi sappiamo che $u'(t) \leq a(t) u(t)$ allora 
    \begin{align*}
        f'(t) = \dfrac{\mathunderline{red}{u'(t)}v(t) - u(t)a(t)v(t)}{v^2(t)} \implies f'(t) \leq \dfrac{\mathunderline{red}{a(t) u(t)}v(t) - u(t)a(t)v(t)}{v^2(t)}  \implies f'(t) \leq 0
    \end{align*}
    Pertanto se la derivata è negativa per ogni $t \in I$ abbiamo che $f$ è descrescente in $I$. Quindi se prendiamo un punto $t_0 \in I$ abbiamo che
    \[
    f(t) \leq f(t_0) \;\;\;\;\; \forall t \geq t_0
    \]
    Ricordiamo come abbiamo definito $f(t)$ e $v(t)$
    \[
    \dfrac{u(t)}{v(t)} \leq \dfrac{u(t_0)}{\mathunderline{green}{v(t_0)}} \implies \dfrac{u(t)}{v(t)} \leq \dfrac{u(t_0)}{\mathunderline{green}{1}} \implies u(t) \leq u(t_0)v(t) \implies u(t) \leq u(t_0)e^{A(t)-A(t_0)}
    \]
    $(ii)$ per questo caso si fa il ragionamento analogo, infatti si può dedurre che $f'(t) = 0$ per ogni $t \in I$
    Ma allora se la derivata è nulla su $I$, allora preso un $t_0 \in I$ vale
    \[
    f(t) = f(t_0) \;\;\;\;\; \forall t \in I
    \]
    Da cui
    \[
    \dfrac{u(t)}{v(t)}= \dfrac{u(t_0)}{v(t_0)} \implies u(t) = u(t_0)v(t) \implies u(t) = u(t_0)e^{A(t)-A(t_0)}
    \]
\end{proof}
\newpage
Ora abbiamo tutto quello che ci server per dimostrare l'unicità della soluzione che abbiamo dato prima per le EDO lineari di primo ordine.
\begin{proof}
    Supponiamo che esistano due soluzioni alla EDO: $u_1(t), u_2(t)$. Quindi dato che per ipotesi soddisfano l'EDO, allora valgono
    \[
    \begin{array}{c@{\qquad}@{\qquad} c}
        \displaystyle \begin{cases}
            u_1'(t) = a(t)u_1(t) + b(t) \\
            u_1(t_0) = u_0
        \end{cases}  & \begin{cases}
            u_2'(t) = a(t)u_2(t) + b(t)\\
            u_2(t_0) = u_0
        \end{cases}
    \end{array}
    \]
    Definiamo $U(t) = u_1(t) - u_2(t)$, osserviamo che 
    \begin{align*}
    U'(t) &= u_1'(t) - u_2'(t) \\
    &= \left(a(t)u_1(t) + b(t)\right) - \left(a(t)u_2(t) + b(t)\right) \\
    &= a(t)\left(u_1(t) - u_2(t)\right)\\
    &= a(t) U(t)
    \end{align*}

    E che 
    \begin{align*}
        U(t_0) = u_1(t_0) - u_2(t_0) = u_0 - u_0 = 0
    \end{align*}

    Ora possiamo applicare il lemma di Gr\"onwall ad $U(t)$ e scopriamo che
    \[
    U'(t) = a(t) U(t) \implies U(t) = U(t_0) e^{A(t)-A(t_0)} \implies U(t) = 0 e^{A(t)-A(t_0)} \implies U(t) = 0
    \]
    Però ricordiamo che abbiamo definito $U(t) = u_1(t) - u_2(t)$, quindi
    \[
    U(t) = 0 \implies u_1(t) - u_2(t) = 0 \implies u_1(t) = u_2(t) 
    \]
Quindi le soluzione che devono essere uguali, e quindi se troviamo una soluzione alla EDO deve essere unica.
\end{proof}
Abbiamo dimostrato che la soluzione è quella che avevamo definito ed è unica, ma da dove viene fuori? Per capire come mai è proprio quella la formula, prendiamo l'EDO e moltiplichiamo parte per parte per $e^{-A(t)}$, con $A(t)$ una primitiva di $a(t)$
\begin{align*}
u'(t)=a(t)u(t) + b(t) &\implies u'(t)e^{-A(t)} = a(t)u(t)e^{-A(t)} + b(t)e^{-A(t)} \\
&\implies u'(t)e^{-A(t)} - a(t)u(t)e^{-A(t)} = b(t)e^{-A(t)}  
\end{align*}

Ora notiamo che possiamo riscrivere il termine a sinitra come la derivata di un prodotto
\[
u'(t)e^{-A(t)} - a(t)u(t)e^{-A(t)} = b(t)e^{-A(t)} \implies \left(u(t)e^{-A(t)}\right)' = b(t)e^{-A(t)}
\]
Quindi per rimuovere la derivata applichiamo l'integrale parte per parte
\begin{align*}
   \int \left(u(t)e^{-A(t)}\right)' \,dt = \int  b(t)e^{-A(t)}\,dt  &\implies u(t)e^{-A(t)} + c = \int  b(t)e^{-A(t)} \,dt \\
   &\implies u(t) = e^{A(t)}\left(-c + \int  b(t)e^{-A(t)} \,dt \right)
\end{align*}

Poi con le condizioni iniziali abbiamo la formula di che abbiamo verificato prima.

\newpage
\begin{esercizio}{}{}
    Determinare la soluzione al seguente problema di Cauchy
    \[
    \begin{cases}
        u'(t) = -\dfrac{2u(t)}{t} + t^2  \\
        u(1) = 0
    \end{cases}
    \]
\end{esercizio}
In questo esercizio notiamo che $t_0=1$ e $u_0=0$, e che
\[
\begin{array}{c@{\qquad}@{\qquad}c}
    a(t) = \dfrac{-2}{t} & b(t)=t^2
\end{array}
\] 
Quindi troviamo una primitiva di $a(t)$
\[
A(t) = \int a(t) \,dt = \int \dfrac{-2}{t} \,dt =-2\log|t| =-\log(t^2)
\]
Quindi possiamo calcolare la soluzione 
\begin{align*}
    u(t) &= e^{-\log(t^2) - (-\log(1))} \left(0 + \int_{1}^{t}s^2e^{\log(s^2) - (\log(1))}\,ds\right)  = e^{-\log(t^2)}\int_{1}^{t}s^2e^{\log(s^2)}\,ds\\
    &= \dfrac{1}{t^2}\int_{1}^{t}s^2\cdot s^2\,ds = \left.\dfrac{1}{t^2} \cdot \dfrac{s^5}{5}\right|_{s=1}^{s=t} = \dfrac{1}{t^2} \cdot \left(\dfrac{t^5}{5} - \dfrac{1^5}{5}\right) = \dfrac{t^3}{5} -\dfrac{1}{5t^2}
\end{align*}
 \addcontentsline{toc}{subsection}{EDO a Variabili Separabili \textit{I°} Ordine}
\begin{teorema}{EDO a Variabili Separabili \textit{I°} Ordine}{}
    Siano $\varnothing \neq I, J \subseteq \mathbb{R}$ due intervalli aperti. Siano $a\in C^0(I)$, $b \in C^0(J)$. Dati $t_o \in I$, $u_0 \in J$. $\exists I_0 \subseteq I$,$\exists J_0 \subseteq J$ tale che $t_0 \in I_0$ e $u_0 \in J_0$ ed esiste una soluzione al seguente problema di Cauchy
    \[
    \begin{cases}
        u'(t) = a(t)b(u(t)) & \forall t \in I_0\\
        u(t_0) = u_0
    \end{cases}
    \]  

    \begin{itemize}
        \item Se $b(u_0)=0$ allora  la soluzione al problema è
        \[
        u(t) = u_0 \;\;\;\;\; \forall t \in I_0
        \]
        \item  Se $b(u_0)\ne 0$ allora  la soluzione al problema è
        \[
        u(t) = B^{-1}\left(B(u_0) + \int_{t_0}^{t}a(s)\,ds\right)\;\;\; \forall t \in I_0
        \]
        Con $B \in C^1(J_0)$ una primitiva di $\dfrac{1}{b(t)}$ su $J_0$.
    \end{itemize}
    Se $b \in C^1(J)$ allora la soluzione trovata è unica.
\end{teorema}
\begin{proof}
    Il primo caso è molto semplice ed abbastanza intuibile, infatti se $u(t)=u_0$ allora la condizione iniziale è verificata e vale $u'(t)=0$ da cui
    \[
    u'(t) = a(t)b(u(t)) \implies 0 = a(t)b(u_0) \implies 0= a(t)\cdot 0 
    \] 
    Pertanto è verificata. Se invece $b(u_0)\ne 0$ allora $\exists J_0 \subseteq J$ intorno di $u_0$ tale che 
    \begin{equation}\label{eq:edo1}
    \begin{array}{c@{\qquad}@{\qquad}c}
        b(u) > 0 \;\;\;\; \forall u \in J_0 & b(u) < 0 \;\;\;\; \forall u \in J_0
    \end{array}
    \end{equation}
    Ricordiamo che $u:I\to J$, quindi dato che è continua allora esisterà un intorno $I_0\subseteq I$ tale che 
    \[
    u(t) \in J_0 \;\;\;\;\; \forall t \in I_0
    \]
    Con questo scopriamo che 
     \[
    \begin{array}{c@{\qquad}@{\qquad}c}

b(u(t)) > 0 \;\;\;\; \forall t \in I_0 & b(u(t)) < 0 \;\;\;\; \forall t \in I_0

\end{array}
\]
    Ma allora dato che in qualsiasi caso $b(u(t)) \ne 0$ per ogni $t \in I_0$ allora possiamo riscrivere l'EDO come
    \[
     u'(t) = a(t)b(u(t)) \implies \dfrac{u'(t)}{b(u(t))} = a(t)
    \]
    Definiamo $B \in C^1(J_0)$ una primitiva di $\dfrac{1}{b(u)}$, questo perchè se facciamo la derivata di $B(u(t))$ vediamo che
    \[
    \dfrac{d}{dt}\left(B(u(t))\right) = B'(u(t))u'(t) = \dfrac{1}{b(u(t))}\cdot u'(t) =  \dfrac{u'(t)}{b(u(t))}
    \]
    Quindi possiamo riscrivere la EDO come 
    \[
    \dfrac{u'(t)}{b(u(t))} = a(t)\implies \dfrac{d}{dt}\left(B(u(t))\right) = a(t)
    \]
    Per rimuovere la derivata dobbiamo trovare una primitiva di $a(t)$, e per comodità per i conti scegliamo 
    \[
    A(t) = \int_{t_0}^{t}a(s)\,ds
    \]
    Pertanto la Edo diventa (ricordandoci del $+c$)
    \[
    B(u(t)) = c+ \int_{t_0}^{t}a(s)\,ds
    \]
    Per scoprire quanto vale $c$ basta che sostituiamo le condizioni iniziali ($u(t_0)=u_0$)
    \[
    B(u(t_0)) = c+ \int_{t_0}^{t_0}a(s)\,ds \implies B(u_0) = c + 0  \implies c=B(u_0)
    \]
    Ora per isolare la $u(t)$ basta che calcoliamo l'inversa di $B$, ma non sappiamo se $B$ è invertibile. Però ricordiamo che $B'(u)=\frac{1}{b(u)}$ e con (\ref{eq:edo1}) scopriamo che
    \[
    b(u) > 0 \implies  \dfrac{1}{b(u)} > 0 \implies B'(u) > 0 \implies B \text{ è monotona crescente}
    \]
    Ragionamento analogo per $ b(u) < 0$. 
    
    Quindi se $B$ è monotona allora sarà anche invertibile, di conseguenza
    \[
    u(t) = B^{-1}\left(B(u_0)+ \int_{t_0}^{t}a(s)\,ds\right)
    \]
\end{proof}

\begin{esercizio}{}{}
    Risolvere il seguente problema di Cauchy e determinare un intervallo $I$ valido
    \[
    \begin{cases}
        u'(t)\cos(u(t)) = 1 \;\;\;\;\; \forall t \in I\\
        u(0) =\dfrac{9\pi}{4}
    \end{cases}
    \]
\end{esercizio}
Notiamo subito che non è nella forma del teorema, quindi riarrangiamo
\[
u'(t)=\dfrac{1}{cos(u(t))}
\]
Notiamo  che $b(u_0)\ne 0$, quindi rientramo nel secondo caso del teorema, quindi risolviamo
\[
u'(t)\cos(u(t)) = 1 \implies \dfrac{d}{dt}(\sin(u(t))) = 1 \implies \sin(u(t)) = t +c
\] 
Usiamo le condizioni iniziali
\[
\sin(u(0)) = 0 +c \implies \sin\left(\dfrac{9\pi}{4}\right) = c \implies c=\dfrac{\sqrt{2}}{2}
\]
Ora dovremmo invertire il seno (usando l'arcoseno) però dobbiamo stare attenti, infatti anche nei semplici esercizi di trigonometria se avevamo 
\[
\sin(x) = \dfrac{1}{2} \implies x = \dfrac{\pi}{6} +2k\pi 
\]
Ma dobbiamo trovare una sola soluzione (dato che è un problema di Cauchy), e quindi dobbiamo riutilizzare le condizioni iniziali per trovare il valore di $k$
\begin{align*}
   u(t) = \arcsin\left(t+\dfrac{\sqrt{2}}{2}\right) +2k\pi \implies& u(0) = \arcsin\left(0+\dfrac{\sqrt{2}}{2}\right) +2k\pi  \\
 \implies& \dfrac{9\pi}{4} = \dfrac{\pi}{4} +2k\pi  \\
 \implies& \dfrac{8\pi}{4}  =  2k\pi   \implies k=1
\end{align*}
Quindi la soluzione all problema di Cauchy è 
\[
u(t) = \arcsin\left(t+\dfrac{\sqrt{2}}{2}\right) +2\pi 
\]
E per trovare $I$ è necessario trovare il dominio della nostra soluzione, infatti ricordiamo che il dominio dell'arcoseno è 
\[
I: -1< t+\dfrac{\sqrt{2}}{2} < 1 \implies -1-\dfrac{\sqrt{2}}{2}< t < 1-\dfrac{\sqrt{2}}{2}
\]

\begin{esercizio}{}{}
    Determinare una soluzione al seguente problema di Cauchy
    \[
    \begin{cases}
         u'(t) = 3t^2 -1 \\
    u(0) = 1
    \end{cases}
    \]
\end{esercizio}

Questo esercizio è molto semplice, infatti oltre al termine $u'(t)$ non troviamo più la funzione incognita. Pertanto è sufficiente calcolare l'integrale parte per parte
\begin{align*}
    \int u'(t) \, dt &= \int 3t^2 -1  \,dt \\
    u(t) &= \dfrac{3t^3}{3} - t + c = t^3 - t + c
\end{align*}
Ora per determinare $c\in\mathbb{R}$ basta usare la condizione iniziale
\begin{align*}
u(0) &= 0^3 - 0  + c\\
1 &= c
\end{align*}
\begin{esercizio}{}{}
    Determinare la soluzione al seguente problema di Cauchy
    \[
    \begin{cases}
        u'(t)u(t) = t^2 \\
        u(0) = -2
    \end{cases}
    \]
\end{esercizio}

Come prima notiamo che rientriamo nel secondo caso del Teorema, quindi dato che la funzione $u(t)$ è già isolata dalla variabile dipendente $t$, possiamo integrale parte per parte
\[
\int u'(t)u(t) \,dt= \int t^2 \,dt
\]
\[
    \dfrac{u^2(t)}{2} = \dfrac{t^3}{3} + c
\]
Per trovare il valore di $c$  usiamo le condizioni iniziali $t_0=0$
\[
\dfrac{u^2(0)}{2} = \dfrac{0^3}{3} + c \implies   \dfrac{(-2)^2}{2} = 0 + c \implies c= 2
\]
Ora possiamo isolare $u(t)$
\[
\dfrac{u^2(t)}{2} = \dfrac{t^3}{3} + 2 \implies u^2(t) = \dfrac{2t^3}{3} + 4 \implies u(t) = \pm \sqrt{\dfrac{2t^3}{3} + 4} 
\]
Ora però, per via della radice, abbiamo due soluzioni ma solo una è quella che ci interessa. Per capire quale dobbiamo riusare le condizioni iniziali, infatti se poniamo $t=0$
\[
u(0) = \pm \sqrt{\dfrac{2\cdot 0^3}{3} + 4} \implies -2 = \pm \sqrt{0 + 4} \implies -2 = \pm 2
\]
Da questo capiamo che la soluzione che ci serve a noi è quella con il meno, pertanto la soluzione del problema di Cauchy è
\[
u(t) =-\sqrt{\dfrac{2t^3}{3} + 4}
\]
\newpage
 \addcontentsline{toc}{subsection}{EDO Lineari \textit{II°} Ordine a Coefficienti Constanti Omogenea}
\begin{teorema}{EDO Lineari \textit{II°} Ordine a Coefficienti Constanti Omogenea}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo aperto, $a,b,c \in\mathbb{R}$ con $a\ne 0$, consideriamo la seguente EDO
    \[
    au''(t) + bu'(t) + cu(t) = 0
    \]
    Definiamo il polinomio associato
    \[
    P(x) = ax^2 + bx + c
    \]
    Allora 
    \begin{enumerate}[label=(\roman*)]
        \item Se le radici di $P(x)$ sono due distinte: $\lambda_1$,$\lambda_2$ con $\lambda_1 \ne \lambda_2$ allora le soluzioni della EDO sono
        \[
        \begin{array}{c@{\qquad}@{\qquad}c}
            u_1(t) = e^{\lambda_1 t} & u_2(t) = e^{\lambda_2 t}
        \end{array}
        \]
        \item Se le radici di $P(x)$ sono coincidenti: $\lambda$ allora le soluzioni della EDO sono
        \[
        \begin{array}{c@{\qquad}@{\qquad}c}
            u_1(t) = e^{\lambda t} & u_2(t) = te^{\lambda t}
        \end{array}
        \]
        \item Se le radici di $P(x)$ sono due distinte complesse: $\alpha \pm i \beta$ allora le soluzioni della EDO sono
        \[
        \begin{array}{c@{\qquad}@{\qquad}c}
            u_1(t) = e^{\alpha t}\cos(\beta t) & u_2(t) = e^{\alpha t}\sin(\beta t)
        \end{array}
        \]
    \end{enumerate}
\end{teorema}
\begin{proof}
    L'idea fondamentale per risolvere questa tipologia di equazioni è di provare ad indovinare quale sia la soluzione, dato che è impossibile isolare la funzione incognita. Ora per quello che abbiamo visto fino ad adesso l'unica funzione che può tornare comoda può essere la funzione esponenziale, dato che è l'unica funzione che è uguale a se stessa (a patto di costanti moltiplicative). Quindi se supponiamo che $u(t) = e^{x t}$ sia una soluzione, possiamo sostituiamola nella EDO
    \begin{align*}
        a(e^{x t})'' + b(e^{x t})' + c(e^{x t}) &= 0 \\
        a(x^2 e^{x t}) + b(x e^{x t})+ ce^{xt} &= 0 \\
        e^{x t}(ax^2 + b x+ c) = 0
    \end{align*}
    Ora il termine $ e^{xt}$ non può mai essere nullo, quindi possiamo rimuoverlo
    \[
    ax^2 + bx + c = 0
    \]
    E questo è il poliniomio che nel teorema abbiamo definito come "polinomio associato". Ora come ben sappiamo una equazione di secondo grado può avere 3 casi di soluzione: 2 distinte, 2 coincidenti (ovvero una doppia) e 2 soluzioni complesse. 

    $(i)$ Supponiamo quindi che $P(x)$ ha due soluzioni distite $\lambda_1$,$\lambda_2$, allora vuol dire che la soluzione che abbiamo ipotizzato ($e^{xt}$) con $x=\lambda_1$ e con $\lambda_2$ risolvono la EDO, pertanto le soluzioni sono:
     \[
        \begin{array}{c@{\qquad}@{\qquad}c}
            u_1(t) = e^{\lambda_1 t} & u_2(t) = e^{\lambda_2 t}
        \end{array}
        \]
    Quindi ci aspettiamo che per ogni caso di avere due soluzioni, analogamente alla equazione di secondo grado. 
    \newpage
    $(ii)$ Se invece il polinomio associato $P(x)$ ha una sola soluzione, che denominiamo come $\lambda$ allora sicuramente una prima soluzione sarà
    \[
    u_1(t) = e^{\lambda t}
    \]
    Proprio per lo stesso ragionamento di prima. Ora però dobbiamo trovare la seconda soluzione e per farlo dobbiamo fare una osservazione, cioè che la nostra soluzione è doppia, pertanto il polinomio caratteristico lo possiamo scrivere come 
    \[
    P(x) = a(x-\lambda)^2
    \]
    Notiamo che la sua derivata vale
    \[
    P'(x) = 2a(x-\lambda)
    \]
    Da cui capiamo che
    \[
    P'(\lambda) = 2a(\lambda-\lambda) = 0
    \]
    Questo ci torna molto comodo, infatti riscriviamo la EDO nel seguente modo 
    \begin{align*}
        a(e^{x t})'' + b(e^{x t})' + c(e^{x t}) &=  e^{x t}(ax^2 + b x+ c) \\
        &= e^{x t}P(x)
    \end{align*}
    Ora che abbiamo riscritto la EDO, possiamo fare la derivata rispetto ad $x$ ambo i lati
    \begin{align*}
        \dfrac{d}{dx}\left(a(e^{x t})'' + b(e^{x t})' + c(e^{x t})\right) &=  \dfrac{d}{dx}\left(e^{x t}P(x)\right)\\
         a\dfrac{d}{dx}\left((e^{x t})''\right) + b\dfrac{d}{dx}\left((e^{x t})'\right) + c\dfrac{d}{dx}\left((e^{x t})\right) &= te^{x t}P(x) + e^{x t}P'(x) \\
         a(te^{x t})'' + b(te^{x t})' + c(te^{x t}) &= te^{x t}P(x) + e^{x t}P'(x)
    \end{align*}
    Ora a questa nuova informazione che abbiamo, possiamo imporre $x=\lambda$
    \[
    a(te^{\lambda t})'' + b(te^{\lambda t})' + c(te^{\lambda t}) = te^{\lambda t}P(\lambda) + e^{\lambda t}P'(\lambda)
    \]
    Però ricordiamo che sia $P(\lambda) = 0$ che $P'(\lambda) = 0$, pertanto
    \begin{align*}
    a(te^{\lambda t})'' + b(te^{\lambda t})' + c(te^{\lambda t}) &= te^{\lambda t}0 + e^{\lambda t}0 \\
    &= 0
    \end{align*}
    Pertanto dopo questo giochetto con le derivate, abbiamo scoperto che anche la funzione $u(t) =te^{\lambda t}$ rende nulla la EDO, pertanto è una soluzione alla EDO iniziale e quindi la seconda soluzione di questo caso è
    \[
    u_2(t) = te^{\lambda t}
    \]

    $(iii)$ per questo caso la dimostrazione è omessa dato che richiede di sapere le proprietà dei numeri complessi, però per chi li ha visti vi lascio un piccolo stralcio di dimostrazione
   \begin{align*}
    e^{(\alpha + i \beta)t} = e^{\alpha t} e^{i \beta t} = e^{\alpha t} \left(\cos(\beta t) + i\sin(\beta t)\right) \implies& u_1(t) = e^{\alpha t}\cos(\beta t) \\ 
    & u_2(t) = e^{\alpha t}\sin(\beta t)
   \end{align*}
\end{proof}

 \addcontentsline{toc}{subsection}{Integrale Generale delle EDO Lineari}
\begin{teorema}{Integrale Generale delle EDO Lineari}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo aperto,$n\in\mathbb{R}$ ordine della EDO,  $a_i \in\mathbb{R}$ per ogni $i \leq n$ e $a_n\ne 0$, consideriamo la seguente EDO lineare omogenea
    \[
    a_n u^{(n)}(t) + a_{n-1} u^{(n-1)}(t) + ... + a_1 u'(t) = 0
    \]
    E siano $X=\{u_1(t),u_2(t), ..., u_n(t)\}$ l'insieme delle soluzioni semplici, allora l'integrale generale della EDO è 
    \[
    u_0(t) = \sum_{u\in X} c_j u
    \]
    con $c_j$ costanti.
\end{teorema}
\begin{proof}
    Per la dimostrazione facciamo solo per le EDO di secondo ordine, dato che sono quelle con cui avremo a che fare negli esercizi, ma il principio è estendibile a qualsiasi ordine.

    Quindi la EDO sarà della forma
    \[
     au''(t) + bu'(t) + cu(t) = 0
    \]
    Quindi come abbiamo visto prima avremo sempre 2 soluzioni, pertanto in ogni caso avremo $u_1(t)$ e $u_2(t)$. Dato che sono soluzioni alla EDO allora vale 
    \[
    \begin{array}{c@{\qquad}@{\qquad}c}
      \mathunderline{red}{au_1''(t) + bu_1'(t) + cu_1(t) = 0} & \mathunderline{blue}{au_2''(t) + bu_2'(t) + cu_2(t) = 0}  
    \end{array}
    \]
    Ora quindi controlliamo se $u_0(t) = k_1u_1(t) + k_2u_2(t)$ è una soluzione alla EDO
    \begin{align*}
       au_0''(t) + bu_0'(t) + cu_0(t) &=  a(k_1u_1(t) + k_2u_2(t))'' + b(k_1u_1(t) + k_2u_2(t))' + c(k_1u_1(t) + k_2u_2(t)) \\
       &= ak_1u_1''(t) + ak_2u_2''(t) + bk_1u_1'(t) + bk_2u_2'(t) + ck_1u_1(t) + ck_2u_2(t) \\
       &= ak_1u_1''(t)  + bk_1u_1'(t) + ck_1u_1(t) + ak_2u_2''(t) + bk_2u_2'(t) + ck_2u_2(t) \\
       &= k_1(\mathunderline{red}{au_1''(t) + bu_1'(t) + cu_1(t)}) + k_2(\mathunderline{blue}{au_2''(t) + bu_2'(t) + cu_2(t)}) \\
       &= k_1\cdot  0 + k_2 \cdot 0 = 0 
    \end{align*}
    Quindi notiamo che risolve l'EDO.
\end{proof}

\begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
    u''(t)-3u'(t) + 2u(t) = 0
    \]
\end{esercizio}
Notiamo che abbiamo una EDO lineare del secondo ordine omogenea, pertanto troviamo il polinomio associato: $P(x) =x^2-3x+2$, le cui radici sono $x=2$ e $x=1$, pertanto le soluzioni semplici saranno
\[
\begin{array}{c@{\qquad}@{\qquad}c}
u_1(t) = e^{1t} & u_2(t)=e^{2t}
\end{array}
    \]
    Pertanto l'integrale generale sarà
    \[
    u_0(t) = c_1u_1(t) + c_2u_2(t) = c_1 e^{t} + c_2 e^{2t}  
    \]
\newpage
    \begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
    u''(t)+6u'(t) + 9u(t) = 0
    \]
\end{esercizio}

Notiamo che siamo nello stesso caso dell'esercizio precedente, quindi cerchiamo il polinio associato  e le sue radici
\[
P(x) = x^2 +6x+9 \implies x=3
\]
Notiamo però che in questo esercizio le radici sono coincidenti, cioò ne abbiamo una sola, pertanto siamo nel secondo caso del teorema e le soluzioni saranno 
\[
\begin{array}{c@{\qquad}@{\qquad}c}
u_1(t) = e^{-3t} & u_2(t)=te^{-3t}
\end{array}
    \]
    Di conseguenza l'integrale generale sarà
    \[
    u_0(t) = c_1 e^{-3t} + c_2 te^{-3t}  
    \]



\begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
    u''(t)-4u'(t) + 13u(t) = 0
    \]
\end{esercizio}
Cerchiamo il polinomio associato 
\[
P(x) = x^2 -4x+13 \implies x=\dfrac{-(-4)}{2\cdot 1} \pm \dfrac{\sqrt{(-4)^2-4(1)(13)}}{2\cdot 1} = 2 \pm 3i
\]
Quindi le soluzioni semplici sono
\[
\begin{array}{c@{\qquad}@{\qquad}c}
u_1(t) = e^{2t}\cos(3t) & u_2(t)=e^{2t}\sin(3t)
\end{array}
    \]
    L'integrale generale sarà
    \[
    u_0(t) = c_1 e^{2t}\cos(3t) + c_2 e^{2t}\sin(3t) =e^{2t}( c_1 \cos(3t) + c_2 \sin(3t))
    \]

  \begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
    u''(t)  = -25u(t)
    \]
\end{esercizio}  
Riarrangiamo meglio la nostra EDO come siamo abituati e troviamo le radici del polinomio associato
\[
u''(t)  +25u(t) = 0 \implies x^2 +25= 0 \implies x = 0 \pm 5i
\]
Quindi le soluzioni semplici sono
\[
\begin{array}{c@{\qquad\qquad}c}
    \begin{aligned}
        u_1(t) &= e^{0t}\cos(5t) \\
               &= \cos(5t)
    \end{aligned}
    &
    \begin{aligned}
        u_2(t) &= e^{0t}\sin(5t) \\
               &= \sin(5t)
    \end{aligned}
\end{array}
\]
Quindi l'integrale generale è 
\[
u_0(t)= c_1\cos(5t) + c_2\sin(5t)
\]
\newpage
 \addcontentsline{toc}{subsection}{EDO lineari \textit{II°} ordine  non Omogenee}
\begin{teorema}{EDO lineari \textit{II°} ordine  non Omogenee}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo aperto, $a,b,c \in\mathbb{R}$ con $a\ne 0$, $f:I \to \mathbb{R}$ consideriamo la seguente EDO
    \[
    au''(t) + bu'(t)+cu(t) = f(t)
    \]
    Allora l'integrale generale sarà della forma 
    \[
    u(t) = u_p(t) + u_o(t)
    \]
    Dove $u_p(t)$ è una soluzione (che poi chiameremo \textbf{soluzione particolare}) alla nostra EDO, mentre $u_o(t)$ è la soluzione dell'EDO omogenea associata.
\end{teorema}
Sostanzialmente questo teorema ci dice che non appena abbiamo trovato una soluzione alla nostra EDO (che sarà $u_p(t)$) dovremo sempre aggiungere anche la soluzione della EDO omogenea associata.
\begin{proof}
    Supponiamo di avere sue soluzioni alla EDO originale, che chiamiamo $u(t)$ e $u_p(t)$. Pertanto varrà
    \[
\begin{array}{c@{\qquad}@{\qquad}c}
\mathunderline{red}{au''(t) + bu'(t)+cu(t) = f(t)} & \mathunderline{blue}{au_p''(t) + bu_p'(t)+cu_p(t) = f(t)}
\end{array}
    \]
    Definiamo una nuova funzione $u_o(t) = u(t) - u_p(t)$ e notiamo che questa nuova funzione soddisfa l'EDO omogena associata, infatti
    \begin{align*}
        au_o''(t) + bu_o'(t)+cu_o(t) &= a(u(t) - u_p(t))'' + b(u(t) - u_p(t))'+c(u(t) - u_p(t)) \\
        &= au''(t) - au_p''(t) + bu'(t) - bu_p'(t)+cu(t) - cu_p(t) \\
        &=  \mathunderline{red}{au''(t)  + bu'(t)+cu(t) } - (\mathunderline{blue}{au_p''(t)+bu_p'(t) + cu_p(t) })\\
        &= f(t) - f(t) = 0
    \end{align*}
    Pertanto una qualsiasi soluzione alla EDO iniziale $u(t)$ dovrà sempre contenere anche la soluzione della equazione omogenea, oltre ad una soluzione che risolve la nostra EDO originale
    \[
    u_o(t) = u(t) - u_p(t) \implies   u(t) =  u_p(t)+ u_o(t)
    \]
\end{proof}
Quindi per esempio se dobbiamo determinare l'integrale generale della seguente EDO
\[
u''(t) - 6u'(t) + 9u(t) = (1+2t)e^{3t}
\]
Allora la prima cosa che dobbiamo fare è determinare la soluzione della EDO omogenea associata, quindi in questo caso
\[
u''(t) - 6u'(t) + 9u(t) = 0 
\]
Ora la risolviamo come prima, per questo esercizio notiamo che abbiamo una soluzione doppia ($x=3$) e quindi le soluzioni saranno $u_1(t) = e^{3t}$ , $u_2(t)=te^{3t}$,
quindi l'integrale generale dell' omogenea è 
\[
u_o(t) = c_1 e^{3t}+ c_2te^{3t}
\]
Ma quindi la soluzione della EDO originale (cioè quella NON omogenea) sarà della forma:
\[
u(t) = u_p(t) + \mathunderline{green}{u_o(t)} = u_p(t) + \mathunderline{green}{c_1 e^{3t}+ c_2te^{3t}}
\]
Ora capiamo come determinare la soluzione particolare.

\newpage

 \addcontentsline{toc}{subsection}{Trovare $u_p(t)$: Metodo Per Somiglianza}
\begin{teorema}{Trovare $u_p(t)$: Metodo Per Somiglianza}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo aperto, $a,b,c \in\mathbb{R}$ con $a\ne 0$, $f:I \to \mathbb{R}$ consideriamo la seguente EDO
    \[
    au''(t) + bu'(t)+cu(t) = f(t)
    \]
    Se $f(t)$ è della forma 
    \[
    f(t) = Q(t) e^{\alpha t}
    \]
    con $Q(t)$ un qualsiasi polinomio di grado $n\in\mathbb{N}$ e $\alpha\in\mathbb{R}$, allora la soluzione particolare sarà della forma
    \[
    u_p(t) = S(t)e^{\alpha t} t^r
    \]
    Con $S(t)$ un polinomio generico di grado $n$, e $r\in \{0,1,2\}$ è la moltiplicità di $\alpha$ rispetto a $P(x)$, dove $P(x)$ è il poliniomo associato alla EDO.
\end{teorema}

Vediamo un esercizio per capirci meglio
\begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
    u''(t) - 6u'(t) + 9u(t) = (1+2t)e^{3t}
    \]
\end{esercizio}
Come dicevamo prima l'integrale generale sarà della forma
\[
u(t) = u_p(t) + c_1 e^{3t}+ c_2te^{3t}
\]
Quindi ora dobbiamo trovare $u_p(t)$.Guardando $f(t)=(1+2t)e^{3t}$  notiamo  $\alpha =3$ e il grado del polinomio è $n=1$ dato che $Q(t)=1+2t$, in più, il polinomio associato alla EDO, $P(x)=x^2-6x+9$ lo possiamo riscrivere come $P(x)=(x-3)^2$, ma dato che $\alpha=3$ abbiamo che la molteplicità di $\alpha$ rispetto a $P(x)$ è doppia, da cui $r=2$. Pertanto la  soluzione particolare è della forma
\[
u_p(t) = (At+B)e^{3t}t^2
\]
Ora però dobbiamo trovare $A,B \in \mathbb{R}$ dato che la tecnica richiede un polinomio generico. Per trovare i valori di $A,B$ dobbiamo sostituire la soluzione particolare nella EDO, prima però per comodità calcoliamoci a parte le derivate di $u_p(t)$. Per comodità imponiamo $S(t) =At^3+Bt^2$ e questo lo faremo spesso nei casi in cui $r\in \{1,2\}$ dato che agevola molto i conti
\begin{align*}
    u_p(t) &= (At+B)e^{3t}t^2 = (At^3+Bt^2)e^{3t} =  Se^{3t}\\
\\
    u_p'(t) &= S'e^{3t} + S\left(e^{3t}\right)' \\
        &= S'e^{3t} + 3Se^{3t}\\
        &= e^{3t}(S' + 3S)\\
\\
    u_p''(t) &= \left(e^{3t}\right)'(S' + 3S)+e^{3t}(S' + 3S)'\\
    &= 3 e^{3t}(S' + 3S) + e^{3t}(S'' + 3S') \\
    &= e^{3t}(S''+6S'+ 9S)
\end{align*}
\newpage
Ora sistutiamo queste derivate nella EDO
\begin{align*}
    \overbrace{e^{3t}(S''+6S'+ 9S)}^{u''(t)}  - 6\overbrace{e^{3t}(S' + 3S)}^{u'(t)}   + 9\overbrace{Se^{3t}}^{u(t)} =t\sin(t)
\end{align*}
Rimuoviamo $e^{3t}$ ambo i lati dato che non può mai essere nullo
\begin{align*}
S''+\mathunderline{red}{6S'}+ \mathunderline{blue}{9S}- \mathunderline{red}{6S'} -\mathunderline{blue}{18S}   + \mathunderline{blue}{9S} &=1+2t \\
S'' &=1+2t
\end{align*}
Ricordiamo che $S(t) = At^3+Bt^2$
\begin{align*}
(At^3+Bt^2)'' &=1+2t \\
6At+2B &=1+2t 
\end{align*}
Da cui
\[
\begin{cases}
    6A= 2 \\
    2B = 1
\end{cases}
=
\begin{cases}
    A =\frac{1}{3}  \\
    B = \frac{1}{2}
\end{cases}
\]
Pertanto la soluzione particolare è 
\[
u_p(t) = \left(\dfrac{t^3}{3}+\dfrac{t^2}{2}\right)e^{3t}
\]
Quindi l'integrale generale della EDO è
\[
u(t) = \left(\dfrac{t^3}{3}+\dfrac{t^2}{2}\right)e^{3t} + c_1 e^{3t}+ c_2te^{3t}
\]
Non vi preoccupare se vi sembra difficile o comunque articolato, tra poco vederemo un modo molto più semplice e veloce. Ma comunque gli esercizi possono essere anche molto più semplici, vediamo tipo
\begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
    u''(t) - 7u'(t) + 12u(t) = 5+8t^2
    \]
\end{esercizio}
Risolviamo quindi prima la EDO omogenea, quindi il polinomio associato è $P(x)=x^2-7x+12$ le cui radici sono $x=3$ e $x=4$, quindi le soluzioni sono $u_1(t)=e^{3t}$ e $u_2(t)=e^{4t}$. Ora cerchiamo una soluzione particolare, e notiamo che $n=2$, e poi che $\alpha=0$ e notiamo che $\alpha$ non è uno zero di $P(x)$, quindi dobbiamo cercare una soluzione particolare del tipo
\[
u_p(t) = (At^2+Bt+C)e^{0t} t^0=At^2+Bt+C
\]
Ora possiamo sostituirla nella EDO
\begin{align*}
    (At^2+Bt+C)'' - 7(At^2+Bt+C)' + 12(At^2+Bt+C) &= 5+8t^2\\
    2A - 7(2At + B) + 12At^2+12Bt+12C &=  \\
    (2A -7B +12C) +t(-14A+12B) +12At^2 &= 5+0t + 8t^2
\end{align*}
Imponiamo il seguente sistema
\[
\begin{cases}
    12A = 8 \\
    -14A+12B = 0 \\
    2A -7B +12C= 5
\end{cases}=
\begin{cases}
    A = \dfrac{2}{3} \\[1em]
    B = \dfrac{7}{9} \\[1em]
    C= \dfrac{41}{54}
\end{cases}
\]
Quindi l'integrale generale della EDO è 
\[
u(t) = \mathunderline{red}{u_p(t)}+ \mathunderline{blue}{u_o(t)} =\mathunderline{red}{\dfrac{2}{3}t^2 +\dfrac{7}{9}t+\dfrac{41}{54}} +\mathunderline{blue}{c_1e^{3t}+c_2e^{4t}}
\]
\begin{teorema}{Trovare $u_p(t)$: Metodo Per Somiglianza Generalizzato}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo aperto, $a,b,c \in\mathbb{R}$ con $a\ne 0$, $f:I \to \mathbb{R}$ consideriamo la seguente EDO
    \[
    au''(t) + bu'(t)+cu(t) = f(t)
    \]
    Se $f(t)$ è di una delle seguenti forme 
    \[
    \begin{array}{c@{\qquad}c@{\qquad}c}
        Q_1(t) e^{\alpha t}\cos(\beta t)  & e^{\alpha t}\left[Q_1(t)\cos(\beta t) + Q_2(t)\sin(\beta t)\right] & Q_2(t) e^{\alpha t}\sin(\beta t)
    \end{array}
    \]
    con $Q_1(t)$ e $Q_2(t)$  qualsiasi polinomi di grado rispettivamente $n_1,n_2\in\mathbb{N}$ e $\alpha,\beta\in\mathbb{R}$, allora la soluzione particolare sarà della forma
    \[
    u_p(t) = e^{\alpha t}\left[S_1(t)\cos(\beta t) + S_2(t)\sin(\beta t)\right]t^r
    \]
    Con $S_1(t)$ e $S_2(t)$  due polinomi generici di grado $\max\{n_1,n_2\}$ (chiaramente se c'è solo $Q_1(t)$ o solo $Q_2(t)$ sarà il grado di quel polinomio), e $r=1$  se  $\alpha\pm i\beta$ sono le radici di $P(x)$, dove $P(x)$ è il poliniomo associato alla EDO, altrimenti $r=0$.
\end{teorema}
Vediamo un esempio pratico
\begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
    u''(t)-4u'(t)+29u(t) = t\sin(t)
    \]
\end{esercizio}
Come al solito risolviamo l'EDO omogenea associata
\[
u''(t)-4u'(t)+29u(t)=0 \implies x^2-4x+29=0 \implies x_{1/2}= 2 \pm 5i
\]
Da cui la soluzione omogenea è
\[
u_o(t) = e^{2t}\left(c_1\cos(5t) + c_2\sin(5t)\right)
\]
\newpage
Ora dobbiamo cercare la $u_p(t)$ e notiamo che $f(t)$ è della forma: $Q(t)e^{\alpha t}\sin(\beta t)$, con $n=1$, $\alpha=0$ e $\beta=1$. In più notiamo che $\alpha\pm i\beta = 0 \pm 1i$ non è una soluzione del polinomio associato $P(x)=x^2+-4x+29$, pertanto $r=0$. Quindi la soluzione particolare sarà della forma 
\begin{align*}
    u_p(t) &= e^{0 t}\left((At+B)\cos(1t) + (Ct+D)\sin(1t)\right)t^0 \\
    &= (At+B)\cos(x) + (Ct+D)\sin(t)
\end{align*}

Quindi come prima dobbiamo calcolare tutte le derivate:
\begin{align*}
    u_p(t) &= (At+B)\cos(t) + (Ct+D)\sin(t) \\
    \\
    u'_p(t) &= (A)\cos(t) +(At+B)(-\sin(t)) + (C)\sin(t) + (Ct+D)\cos(t) \\
    &= (Ct + D + A)\cos(t) + (-At -B + C)\sin(t) \\
    \\
    u_p''(t) &= (C)\cos(t) + (Ct + D + A)(-\sin(t)) + (-A)\sin(t) +(-At -B + C)\cos(t)  \\
    &= (-At -B + 2C)\cos(t) + (-Ct -D -2A)\sin(t)
\end{align*}
Per fare meno confusione, facciamo la somma prima solamente dei termini con il $\sin(t)$ e poi faccimo con solo in $\cos(t)$. 
\begin{align*}
    \overbrace{(-Ct -D -2A)\sin(t)}^{u''(t)}  - 4\overbrace{(-At -B + C)\sin(t)}^{u'(t)}   + 29\overbrace{(Ct+D)\sin(t)}^{u(t)} &=t\sin(t) \\
    [(4A+28C)t-2A+4B-4C+28D]\sin(t) &= [1t+0]\sin(t)
\end{align*}
Da questo bisogna che
\[
\begin{cases}
    4A+28C = 1 \\
    -2A+4B-4C+28D = 0
\end{cases}
\]
Ora facciamo lo stesso per i termini con il coseno (e ricordiamo che il coseno non c'è nell'espressione $f(t)$, pertanto questo risultato sarà uguale a 0)
\begin{align*}
    \overbrace{(-At -B + 2C)\cos(t)}^{u''(t)}  - 4\overbrace{(Ct + D + A)\cos(t)}^{u'(t)}   + 29\overbrace{(At+B)\cos(t)}^{u(t)} &=0 \\
    [(28A-4C)t+28B+2C-4D-4A]\cos(t)&=[0t+0]\cos(t)
\end{align*}
Da questo ne evince che (unendo con il sistema precedente)
\[
\begin{cases}
    28A-4C = 0 \\
    28B+2C-4D-4A = 0 \\
    4A+28C = 1 \\
    -2A+4B-4C+28D = 0
\end{cases}
= \begin{cases}
    A = \frac{1}{200}\\
    B = -\frac{1}{1000} \\
    C = \frac{7}{200} \\
    D = \frac{11}{2000}
\end{cases}
\]
Con questo scopriamo che la soluzione della EDO iniziale è
\[
u(t) = \overbrace{\left(\frac{1}{200}t -\frac{1}{1000}\right)\cos(t) + \left(\frac{7}{200}t +\frac{11}{2000}\right)\sin(t)}^{u_p(t)} + \overbrace{e^{2t}\left(c_1\cos(5t) + c_2\sin(5t)\right)}^{u_o(t)}
\]
\newpage
 \addcontentsline{toc}{subsection}{Trovare $u_p(t)$:    Variazione delle Costanti}
\begin{teorema}{Trovare $u_p(t)$: Variazione delle Costanti}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo aperto, $b,c \in\mathbb{R}$, $f:I \to \mathbb{R}$ consideriamo la seguente EDO
    \[
    u''(t) + bu'(t)+cu(t) = f(t)
    \]
    Siano $u_1(t)$ e $u_2(t)$ le soluzioni semplici della EDO omogenea, allora se troviamo $k_1, k_2 \in C^1(I)$ tali che
    \[
    \begin{cases}
        k'_1(t)u_1(t) + k'_2(t)u_2(t) = 0 \\
        k'_1(t)u_1'(t) + k'_2(t)u_2'(t) = f(t)
    \end{cases}
    \]
    Allora la soluzione particolare della EDO iniziale è 
    \[
    u_p(t) = k_1(t)u_1(t) + k_2(t)u_2(t)
    \]
\end{teorema}
\begin{proof}
    Ricordiamo che $u_1(t)$ e $u_2(t)$  sono soluzioni della EDO omogenea, pertanto valgono 
    
    \begin{equation}
    \begin{array}{c@{\qquad}@{\qquad}c}\label{eq:diff2}
        u_1''(t) + bu_1'(t)+cu_1(t)=0 & u_2''(t) + bu_2'(t)+cu_2(t)=0 
    \end{array}
\end{equation}

    Supponiamo che la soluzione particolare sia 
    \[
    u_p(t) = k_1(t)u_1(t) + k_2(t)u_2(t)
    \]
    Pertanto verifichiamo se risolve la EDO, calcoliamo prima le sue derivate
    \begin{align*}
         u_p(t) &= k_1(t)u_1(t) + k_2(t)u_2(t) \\
         \\
         u_p'(t) &= k_1'(t)u_1(t) +k_1(t)u_1'(t)+ k_2'(t)u_2(t) + k_2(t)u_2'(t)
    \end{align*}
    Per facilitarci i conti, supponiamo che 
    \begin{equation}\label{eq:diff1}
        k_1'(t)u_1(t) + k_2'(t)u_2(t) = 0
    \end{equation}
    In modo tale che 
    \begin{align*}
         u_p'(t) &= k_1(t)u_1'(t)+  k_2(t)u_2'(t) \\
         \\
         u_p''(t) &= k_1'(t)u_1'(t) + k_1(t)u_1''(t)+  k_2'(t)u_2'(t)+  k_2(t)u_2''(t)
    \end{align*}
    Ora sostituiamolo nella EDO per vedere se la verifica (per comodità ometterò la $t$)
    \begin{align*}
        \overbrace{\mathunderline{red}{k_1'u_1'} + \mathunderline{blue}{k_1u_1''}+  \mathunderline{red}{k_2'u_2'}+  \mathunderline{blue}{k_2u_2''}}^{u_p''(t)} + b(\overbrace{\mathunderline{green}{k_1u_1'+  k_2u_2'} }^{u_p'(t)})  + c( \overbrace{\mathunderline{pink}{k_1u_1 + k_2u_2}}^{u_p(t)}) &= f(t) \\
        \mathunderline{red}{k_1'u_1' +  k_2'u_2'} +\mathunderline{blue}{k_1u_1''} +\mathunderline{green}{bk_1u_1'} + \mathunderline{pink}{ck_1u_1}  +\mathunderline{blue}{k_2u_2''} +\mathunderline{green}{bk_2u_2'} + \mathunderline{pink}{ck_2u_2}&=  \\
        k_1'u_1' +  k_2'u_2' +k_1(u_1'' + bu_1'+cu_1) + k_2(u_2'' + bu_2'+cu_2) &=
    \end{align*}
    Ricordiamo (\ref{eq:diff2}) 
    \begin{align}
        k_1'u_1' +  k_2'u_2' +k_1\cdot 0 + k_2\cdot 0 &=f(t) \notag \\
        k_1'u_1' +  k_2'u_2' &= f(t)\label{eq:diff3}
    \end{align}
    Pertanto la nostra soluzione particolare risolve la EDO solo se valgono (\ref{eq:diff1}) e (\ref{eq:diff3})
    \[
    \begin{cases}
        k'_1(t)u_1(t) + k'_2(t)u_2(t) = 0 \\
        k'_1(t)u_1'(t) + k'_2(t)u_2'(t) = f(t)
    \end{cases}
    \]
\end{proof}
Questo metodo quindi ci permette di cercare delle soluzioni nei casi in cui la funzione $f(t)$ non assomiglia ad una delle forme del metodo con somiglianza, ad esempio
\begin{esercizio}{}{}
    Determinare l'integrale generelare della seguente EDO
    \[
    u''(t) + u(t) = \dfrac{1}{\cos(t)} \;\;\;\;\; \forall t \in \left(0, \dfrac{\pi}{4}\right)
    \]
\end{esercizio}
Ad ogni modo prima risolviamo la EDO omogenea associata 
\[
u''(t) + u(t) = 0 \implies x^2+1 = 0 \implies x_{1/2} = 0 \pm 1i
\]
Da cui le soluzioni semplici sono 
\[
\begin{array}{c@{\qquad\qquad}c}
    \begin{aligned}
        u_1(t) &= e^{0t}\cos(1t) \\
               &= \cos(t)
    \end{aligned}
    &
    \begin{aligned}
        u_2(t) &= e^{0t}\sin(1t) \\
               &= \sin(t)
    \end{aligned}
\end{array}
\]
Ora notiamo che la funzione $f(t)=\frac{1}{\cos(t)}$ non è riconducibile a nessuna delle forme del metodo con somiglianza, pertanto l'unico metodo che conosciamo è quello della variazione delle costanti. Quindi dobbiamo cercare due funzioni $k_1, k_2 \in \left(0, \frac{\pi}{4}\right)$ tali che
\[
    \begin{cases}
        k'_1(t)\cos(t) + k'_2(t)\sin(t) = 0 \\
        -k'_1(t)\sin(t) + k'_2(t)\cos(t) = \dfrac{1}{\cos(t)}
    \end{cases}
    \]
    Ora nella prima equazione possiamo isolare $k'_1$ e nella seconda moltiplicare ambo i menbri per $\cos(t)$ e questo possiamo farlo dato che l'EDO è definita per $t \in \left(0, \frac{\pi}{4}\right)$ e in quell'intervallo $\cos(t)$ non si annulla mai.
    \begin{align*}
    &\begin{cases}
        -k'_1\cos(t)   = k'_2\sin(t)\\
        -k'_1\cos(t)\sin(t) + k'_2\cos^2(t) = 1
    \end{cases}\implies 
    \begin{cases}
        -k'_1\cos(t)   = \mathunderline{red}{k'_2\sin(t)}\\
        \mathunderline{red}{k'_2\sin(t)}\sin(t) + k'_2\cos^2(t) = 1
    \end{cases}\\
    & \implies 
    \begin{cases}
        k'_1\cos(t)   = -k'_2\sin(t)\\
        k'_2(\sin^2(t) + \cos^2(t)) = 1
    \end{cases}
    \implies
    \begin{cases}
        k'_1\cos(t)   = -1\sin(t)\\
        k'_2 = 1
    \end{cases}\\
    &\implies 
    \begin{cases}
        k'_1(t)  = -\dfrac{\sin(t)}{\cos(t) }\\
        k'_2(t)=1
    \end{cases}
    \implies
    \begin{cases}
        k_1(t)  = \log|\cos(t)|\\
        k_2(t)=t
    \end{cases}
    \end{align*}

    Quindi la soluzione particolare alla EDO è 
    \[
    u_p(t) = \overbrace{\log|\cos(t)|}^{k_1}\cos(t) + \overbrace{t}^{k_2}\sin(t)
    \]
    Di conseguenza l'integrale generale è 
    \[
    u(t) = \log|\cos(t)|\cos(t)+t\sin(t) +c_1\cos(t) + c_2\sin(t)
    \]
    Questo metodo però può essere usato anche per risolvere EDO che rientrano nella forma del metodo con somiglianza, proviamo a rifare un esercizio che abbiamo già fatto, ma utilizzando questo nuovo metodo
\begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
    u''(t) - 6u'(t) + 9u(t) = (1+2t)e^{3t}
    \]
\end{esercizio}
Le soluzioni semplici dell'omogenea le avevamo già calcolate e sono
\[
\begin{array}{c@{\qquad\qquad}c}
    u_1(t) = e^{3t} &u_2(t) = te^{3t}
\end{array}
\]
Quindi dobbiamo cercare $k_1, k_2$ tali che 
\[
    \begin{cases}
        k'_1e^{3t} + k'_2te^{3t}= 0 \\
        k'_13e^{3t} + k'_2(1e^{3t}+ t\cdot3e^{3t}) = (1+2t)e^{3t}
    \end{cases}
    \]
In entrambe le equazioni possiamo dividere per $e^{3t}$ che non può mai essere nulla
\begin{align*}
    &\begin{cases}
        k'_1 + k'_2t= 0 \\
        3k'_1 + k'_2(1+ 3t) = 1+2t
    \end{cases}
    \implies
    \begin{cases}
        k'_1  = -k'_2t\\
        -3tk'_2 + k'_2+ 3tk'_2 = 1+2t
    \end{cases}\\
    &
    \implies
    \begin{cases}
        k'_1  = -k'_2t\\
        k'_2= 1+2t
    \end{cases}\implies
    \begin{cases}
        k'_1(t)  = -1t-2t^2\\
        k'_2(t)= 1+2t
    \end{cases}
    \implies
    \begin{cases}
        k_1(t)  = \frac{-t^2}{2}-\frac{2t^3}{3}\\
        k_2(t)= t+t^2
    \end{cases}
\end{align*}
Quindi la soluzione particolare è 
\begin{align*}
u_p(t) &= \left(\frac{-t^2}{2}-\frac{2t^3}{3}\right)e^{3t} + \left(t+t^2\right)te^{3t} \\
&= \left(\frac{-t^2}{2}-\frac{2t^3}{3} + t^2+t^3\right)e^{3t} \\
&= \left(\frac{t^2}{2}+\frac{t^3}{3}\right)e^{3t}
\end{align*}
Di conseguenza l'integrale generale sarà dato da
\[
u(t) = \left(\frac{t^2}{2}+\frac{t^3}{3}\right)e^{3t} + c_1e^{3t} + c_2te^{3t}
\]
Come avevamo già trovato. Però da notale che con il metodo della somigliaza abbiamo dovuto calcolare due derivate (non tanto semplici) e abbiamo dovuto risolvere un sistema lineare, mentre con questo metodo abbiamo risolto un semplice sistema lineare, rendendo lo svolgimento molto più semplice che prima.

\newpage
 \addcontentsline{toc}{subsection}{Principio di Sovrapposizione}
\begin{teorema}{Principio di Sovrapposizione}{}
     Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo aperto, $a,b,c \in\mathbb{R}$ con $a\ne 0$, $f:I \to \mathbb{R}$ consideriamo la seguente EDO
    \[
    au''(t) + bu'(t)+cu(t) = f_1(t)+ f_2(t) + ... + f_n(t)
    \]
    Sia $u_1(t)$ la soluzione particolare alla EDO: $  au''(t) + bu'(t)+cu(t) = f_1(t)$, $u_2(t)$ la soluzione particolare della EDO $  au''(t) + bu'(t)+cu(t) = f_2(t)$, e così via fino ad $u_n(t)$.
    Allora la soluzione particolare della EDO originale è
    \[
    u_p(t) = u_1(t) + u_2(t) + ...+ u_n(t)
    \]
\end{teorema}
\begin{proof}
    
Controlliamo se la soluzione particolare risolve la EDO 
\begin{align*}
    a(u_1 + u_2 + ...+ u_n)'' +b(u_1 + u_2 + ...+ u_n)' + c(u_1 + u_2 + ...+ u_n) &= f_1+ f_2+ ...  +f_n\\
    au_1''+au_2'' + ... + au_n'' +  bu_1'+bu_2' + ... + bu_n' +cu_1+cu_2 + ... + cu_n &= \\
   \mathunderline{red}{ au_1''+ bu_1'+cu_1} +  \mathunderline{blue}{au_2''+ bu_2'+cu_2}  + ... +  \mathunderline{green}{au_n''+ bu_n'+cu_n} &= \\
   \mathunderline{red}{f_1}+ \mathunderline{blue}{f_2}+ ...  +\mathunderline{green}{f_n}&= f_1+ f_2+ ...  +f_n
\end{align*}
\end{proof}
\begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
     u''(t)-4u(t) = e^t + \sin(t)
    \]
\end{esercizio}
Come al solito cerchiamo prima la soluzione omogenea
\[
 u''(t)-4u(t)=0 \implies x^2-4 =0 \implies x_{1/2 } = \pm 2 \implies u_1(t) = e^{2t} \;\;\;\; u_2(t)=e^{-2t}
\]
Ora dobbiamo cercare la soluzione particolare, ma dato che la nostra EDO è formata da una somma, possiamo "spezzare" la equazione studiando singolaremente le soluzioni delle seguenti EDO
\[
\begin{array}{c@{\qquad}@{\qquad}c}
    u''(t)-4u(t) = e^t  & u''(t)-4u(t) = \sin(t)
\end{array}
\]
Entrambe le risolviamo con il metodo di somiglianza, quindi nella prima EDO la soluzione particolare è della forma: $u_1(t)=Ae^{t}$, quindi
\[
(Ae^{t})''-4(Ae^{t}) = e^t \implies Ae^{t}-4Ae^{t} = e^{t} \implies -3A = 1 \implies A=\frac{-1}{3}
\]
Mentre la seconda soluzione è della forma: $u_2(t)=B\sin(t)$
\[
(B\sin(t))'' -4(B\sin(t)) = \sin(t) \implies -B\sin(t) - 4B\sin(t)=\sin(t) \implies B=\frac{-1}{5}
\]
Quindi la soluzione alla EDO originale è 
\[
u(t) = \overbrace{c_1e^{2t} + c_2e^{-2t}}^{u_o(t)} - \overbrace{\dfrac{e^{t}}{3}}^{u_1(t)} - \overbrace{\dfrac{\sin(t)}{5}}^{u_2(t)}
\]
\newpage
 \addcontentsline{toc}{subsection}{EDO di Ordine Superiore al \textit{II}°}
\begin{teorema}{EDO di Ordine Superiore al \textit{II}°}{}
     Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo aperto, $f:I\to\mathbb{R}$, $n,k \in \mathbb{N}$ con $k\leq n$ consideriamo la seguente EDO
    \[
    f(t, u^{(k)}, u^{(k+1)}, u^{(k+2)},...,  u^{(n)}) = 0
    \]
    Definiamo 
    \[
    v(t) = u^{(k)}(t)
    \]
    In modo tale che la EDO diventa
    \[
    f(t, v, v^{(1)}, v^{(2)},..., v^{(n-k)}) = 0
    \]
    Trovata poi la soluzione a questa EDO, per avere la soluzione alla EDO originale basta integrare $k$ volte.
    \[
     u(t) = \underbrace{\int \int ... \int}_{k\;\text{volte}} v(t) \,dt
    \]
\end{teorema}
Vediamo un esempio per affinare la tecnica

\begin{esercizio}{}{}
    Determinare l'integrale generale della seguente EDO
    \[
     u'''(t) = 36u'(t) +  e^{t}
    \]
\end{esercizio}
Notiamo che possiamo applicare la tecnica facendo la seguente sostituzione
\[
v(t) = u'(t)
\]
In modo tale che la nuova EDO sia
\[
v''(t) = 36v(t) + e^{t}
\]
Questa la sappiamo risolvere, cerchiamo la soluzione omogenea
\[
v''(t) -36v(t) = 0  \implies x^2-36= 0 \implies x_{1/2} =\pm 6 \implies v_1(t) = e^{6t} \;\;\;\; v_2(t) = e^{-6t}
\]
Ora cechiamo la soluzione particolare della forma $v_p(t) = Ae^{t}$
\begin{align*}
(Ae^{t})'' - 36(Ae^{t}) = e^{t} \implies -35Ae^{t} &= 1e^{t} \implies A=\frac{-1}{35}
\end{align*}
 Quindi la soluzione della nuova EDO è
\[
v(t) = -\frac{e^{t}}{35} + c_1e^{6t} + c_2e^{-6t}
\]
Ora per trovare la soluzione originale basta integrale una sola volta dato che $v=u'$
\[
u(t) = \int v(t) \,dt = \int \left(-\frac{e^{t}}{35} + c_1e^{6t} + c_2e^{-6t} \right)\,dt = -\frac{e^{t}}{35} + \frac{c_1e^{6t}}{6} + \frac{c_2e^{-6t}}{-6} + c_3
\]
Ora ricordiamo che le costanti $c_1$,$c_2$ sono arbitrarie, quindi se moltiplicate o divise per un'altra costante, otteniamo sempre un'altra costante, quindi la soluzione la possiamo semplificare come
\[
u(t) =  -\frac{e^{t}}{35} + c_1e^{6t} + c_2e^{-6t} + c_3
\]
    \end{document}

