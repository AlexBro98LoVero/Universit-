\documentclass{rapport}
\usepackage{lipsum}
\usepackage{gensymb}
\usepackage{float}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsthm} 
\usepackage{amssymb}
\usepackage{cancel}

\usepackage{enumitem}
\usepackage[most, theorems]{tcolorbox}
\usepackage{xcolor}
\usepackage[italian]{babel} % lingua italiana

\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\usepackage{tikz}
\newcommand*\circled[2][]{%
  \tikz[baseline=(char.base)]{
    \node[shape=circle,draw=#1,inner sep=1pt] (char) {$\displaystyle #2$};
  }%
}



\newtcbtheorem{teorema}{Teorema}{
  colback=blue!5!white,
  colframe=blue!75!black,
  enhanced,
  fonttitle=\bfseries,
}{theo}

\newtcbtheorem{definizione}{Definizione}{
    colback=green!5!white, 
    colframe=green!75!black, 
    enhanced,
    fonttitle=\bfseries,
}{def}

\newtcbtheorem{esercizio}{Esercizio}{
    colback=red!5!white, 
    colframe=red!75!black, 
   enhanced,
  fonttitle=\bfseries,
}{es}

\newtcbtheorem{corollario}{Corollario}{
    colback=pink!5!white, 
    colframe=pink!75!black, 
    enhanced,
  fonttitle=\bfseries,
}{corr}

\newtcbtheorem{esempio}{Esempio}{
    colback=purple!5!white, 
    colframe=purple!75!black, 
    enhanced,
  fonttitle=\bfseries,
}{esem}

% \newtheorem{theorem}{Teorema}
% \newtheorem{proposition}[theorem]{Proposizione}
% \newtheorem{corollary}[theorem]{Corollario}
% \newtheorem{lemma}[theorem]{Lemma}

% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definizione}
% \newtheorem{axiom}[theorem]{Assioma}
% \newtheorem{example}[theorem]{Esempio}

% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}
% \newtheorem{note}[theorem]{Note}
\def\mathunderline#1#2{\color{#1}\underline{{\color{black}#2}}\color{black}}
\newcommand{\mathrect}[2]{%
  \fcolorbox{#1}{white}{\strut$\displaystyle #2$}%
}

\title{DataBase} %title of the file

\begin{document}

%----------- Report information ---------

\logo{logos/logo.jpg}
\uni{\textbf{Università degli Studi di Padova}}
\ttitle{Formulario di Teoremi e relative Dimostrazioni} %title of the file
\subject{Analisi 1} % Subject name
\topic{Formulario di Teoremi e relative Dimostrazioni} % Topic name

\students{Alex Gasparini} % information related to the students

%----------- Init -------------------
        
\buildmargins % display margins
\buildcover % create the front cover of the document
\toc % creates the table of contents

%------------ Report body ----------------



\section{Principio d'Induzione}
\addcontentsline{toc}{subsection}{Teorema del binomio di Newton}
\begin{teorema}{del binomio di Newton}{}
\begin{equation*}
    (a+b)^n = \sum_{k=0}^{n}\binom{n}{k}a^{n-k}b^{k} \quad \forall n \in \mathbb{N}_0
\end{equation*}

\end{teorema} 

\begin{proof} 
Facciamo una dimostrazione per induzione. Partiamo con la base induttiva, con $n_0 = 0$:

\begin{align*}
    (a+b)^0 &= \sum_{k=0}^{0}\binom{0}{k}a^{0-k}b^{k} \\
    1 &= \binom{0}{0}a^{0-0}b^{0} \\
    1 &= 1 \cdot 1 \cdot 1 \\
    1 &= 1
\end{align*}



La base induttiva è stata verificata. Ora passiamo al passo, quindi supponiamo che $P(n)$ sia vero, e proviamo a vedere se è vero $P(n+1)$

\begin{equation*}
    (a+b)^{n+1} = \sum_{k=0}^{n+1}\binom{n}{k}a^{n+1-k}b^{k}    
\end{equation*}

Per proseguire con la dimostrazione lasceremo in alterato il termine di destra e andremo a modificare quello di sinistra.

\begin{equation*}
    (a+b)^{n+1} =(a+b)^{n} \cdot(a+b) 
\end{equation*}

Ora possiamo sostituire $(a+b)^{n}$ con $P(n)$ visto che è $P(n)$ è vera (dato che è una nostra ipotesi)

\begin{align}
    (a+b)^{n+1} &= \left( \sum_{k=0}^{n}\binom{n}{k}a^{n-k}b^{k}\right) \cdot(a+b)   \\
     &= \mathunderline{blue}{a \cdot \left(\sum_{k=0}^{n}\binom{n}{k}a^{n-k}b^{k} \right)}+\mathunderline{red}{b\cdot \left(\sum_{k=0}^{n}\binom{n}{k}a^{n-k}b^{k}\right)} \label{eq:bin1}
\end{align}


Per comodità andiamo a d analizzare singolarmente le due sommatorie, prima quella in blu e poi quella in rosso.

\begin{align*}
\mathunderline{blue} {a \cdot \left( \sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^{k} \right)} &= \sum_{k=0}^{n} a \cdot \binom{n}{k} a^{n-k} b^{k} \\
 &= \sum_{k=0}^{n}\binom{n}{k} a^{n+1-k} b^{k}
\end{align*}



\newpage
Ora analiziamo la parte rossa


\begin{align*}
\mathunderline{red} {b \cdot \left( \sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^{k} \right)} &= \sum_{k=0}^{n} b \cdot \binom{n}{k} a^{n-k} b^{k} \\
 &= \sum_{k=0}^{n}\binom{n}{k} a^{n-k} b^{k+1}
\end{align*}

ora facciamo una sostituzione $h = k+1$

\begin{align*}
\mathunderline{red} {\sum_{k=0}^{n}\binom{n}{k} a^{n-k} b^{k+1}} &= \sum_{h=1}^{n+1}\binom{n}{h-1} a^{n-(h-1)} b^{h} \\
 &= \sum_{h=1}^{n+1}\binom{n}{h-1} a^{n-h+1} b^{h}
\end{align*}

Visto che gli indici nelle sommatorie sono muti, sostituiamo $h$ con $k$, in modo che tutte le sommatorie sono rispetto a $k$

\begin{align*}
\mathunderline{red} {\sum_{h=1}^{n+1}\binom{n}{h-1} a^{n-h+1} b^{h}} = \sum_{k=1}^{n+1}\binom{n}{k-1} a^{n+1-k} b^{k}
\end{align*}

Ricomposiamo le due sommatorie ritornando al punto \eqref{eq:bin1}


\begin{align}
    (a+b)^{n+1} &= \mathunderline{blue}{\sum_{k=0}^{n}\binom{n}{k} a^{n+1-k} b^{k}}+\mathunderline{red}{\sum_{k=1}^{n+1}\binom{n}{k-1} a^{n+1-k}b^{k}} \label{eq:bin2}
\end{align}
 Per unire le due sommatorie devono avere gli stessi indici, quindi rimoviamo gli indici "in più", nella prima togliamo il termine con indice $k=0$, in modo che entrambe partano con $k=1$, e nella seconda rimuoviamo il termine $k=n+1$ in modo che entrambe finiscano con il termine $k=n$

 \begin{align*}
    \mathunderline{blue}{\sum_{k=0}^{n}\binom{n}{k} a^{n+1-k} b^{k}} &= \binom{n}{0} a^{n+1-0} b^{0} + \sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k} \\
    &= 1\cdot a^{n+1} \cdot 1 + \sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k}\\
    &= a^{n+1} + \sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k}
\end{align*}

\newpage
 \begin{align*}
    \mathunderline{red}{\sum_{k=1}^{n+1}\binom{n}{k-1} a^{n+1-k}b^{k}} &= \binom{n}{(n+1)-1} a^{n+1 -(n+1)}b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}  \\
    &= \binom{n}{n} a^{0}b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k} \\
    &= 1 \cdot 1 \cdot b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}\\
    &= b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}
\end{align*}

 Riscriviamo il termine \eqref{eq:bin2}


\begin{align*}
    (a+b)^{n+1} &= \mathunderline{blue}{a^{n+1} + \sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k}}+\mathunderline{red}{b^{n+1} + \sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}} \\
    &= \mathunderline{blue}{a^{n+1}} + \mathunderline{red}{b^{n+1}} + \mathunderline{blue}{\sum_{k=1}^{n}\binom{n}{k} a^{n+1-k} b^{k}} + \mathunderline{red}{\sum_{k=1}^{n}\binom{n}{k-1} a^{n+1-k}b^{k}}  \\
    &=a^{n+1} + b^{n+1} + \sum_{k=1}^{n} \left(\binom{n}{k} a^{n+1-k} b^{k} +\binom{n}{k-1} a^{n+1-k}b^{k}\right)\\
    &=a^{n+1} + b^{n+1} + \sum_{k=1}^{n} a^{n+1-k} b^{k} \cdot\left( \binom{n}{k} +\binom{n}{k-1} \right)\\
    &=a^{n+1} + b^{n+1} + \sum_{k=1}^{n} a^{n+1-k} b^{k} \cdot\binom{n+1}{k} \\
    &=a^{n+1} + b^{n+1} + \sum_{k=1}^{n} \binom{n+1}{k}a^{n+1-k} b^{k} 
\end{align*}
 Siamo quasi alla fine ma notiamo che rispetto a $P(n+1)$ gli indici sono sbagliati, infatti $P(n+1)$ parte con indice $k=0$ e termina con $k=n+1$, mentre la sommatoria che abbiamo appena trovato parte da $k=1$ e termina con $k=n$. Proviamo a vedere cosa sarebbero i termini $k=0$ e $k=n+1$ (quelli che a noi mancano)


\begin{align*}
    \binom{n+1}{0}a^{n+1-0} b^{0}  &= 1 \cdot a^{n+1} \cdot 1 = \mathunderline{blue}{a^{n+1}} \qquad (k=0)  \\ 
    \binom{n+1}{n+1}a^{n+1-(n+1)} b^{n+1}  &= 1 \cdot 1 \cdot b^{n+1} = \mathunderline{red}{b^{n+1}} \qquad (k=n+1)  
\end{align*}
 \newpage
Vediamo che i termini che ci mancano ($a^{n+1}$ e $b^{n+1}$) in realtà ce li abbiamo fuori dalla sommatoria, quindi possiamo "portarli dentro" alla sommatoria sistemando gli indici

\begin{align*}
    (a+b)^{n+1} &= a^{n+1} + b^{n+1} + \sum_{k=1}^{n} \binom{n+1}{k}a^{n+1-k} b^{k}  \\
    &= \sum_{k=0}^{n+1} \binom{n+1}{k}a^{n+1-k} b^{k}
\end{align*}

In questa maniera siamo riusciti a dimostrare il passo indutttivo, visto che siamo partiti da $P(n)$ e siamo riusciti a dimostrare che $P(n+1)$. Pertanto, visto che sia il passo induttivo che la base induttiva sono verificati, allora il teorema è dimostrato. 
\end{proof}


\addcontentsline{toc}{subsection}{Teorema dell'Irrazionalità di $\sqrt{2}$}
\begin{teorema}{Irrazionalità di $\sqrt{2}$}{}
    

    \begin{equation*}
        \sqrt{2} \notin \mathbb{Q}
    \end{equation*}
\end{teorema}
    

\begin{proof}
    La dimostrazione sarà fatta per assurdo, quindi partiamo supponendo che $\sqrt{2} \in \mathbb{Q}$, pertanto $\sqrt{2}$ lo possiamo scrivere come:

    \begin{equation*}
        \sqrt{2} = \frac{p}{q} \qquad p,q \in \mathbb{Z} \setminus\{0\}
    \end{equation*}

    Supponiamo anche che il $mcd(p, q) = 1$ (Massimo Comun Divisore), altrimenti $p$ e $q$ sarebbero semplificabili ulteriormente. Attenzione perchè questo punto sarà fondamentale per la dimostrazione.

    Per semplicità, eleviamo tutto al quadrato

    \begin{align}
        2 &= \frac{p^2}{q^2} \\
        2q^2 &= p^2 \label{eq:rad2_1}
    \end{align}

    Ora notiamo che $p^2$ è un multiplo di $2$, perciò $p^2$ è un numero pari. di conseguenza anche $p$ è un numero pari dato che solamente un il prodotto di due numeri pari da un numero pari. Allora possiamo scrivere $p$ come 

    \begin{equation*}
        p = 2k \qquad k \in \mathbb{Z}         
    \end{equation*}

    Quindi andiamo a sostituirlo nell'equazione \eqref{eq:rad2_1}

    \begin{align*}
        2q^2 &= (2k)^2  \\
        2q^2 &= 4k^2 \\
        q^2 &= 2k^2
    \end{align*}

    Dopo le varie semplificazioni notiamo che anche $q^2$ è divisivile per $2$, e come abbiamo dedotto per $p$, allora anche $q$ è divisibile per 2. Però sia $p$ che $q$ sono divisibili per $2$, questo implica che $mcd(p,q) \ge 2$. Cosa assurda, visto che avevamo imposto che $mcd(p,q) = 1$, pertanto sono sbagliate le tesi: ovvero che $\sqrt{2} \in \mathbb{Q}$, e se questa affermazione è sbagliata allora per forza $\sqrt{2} \notin \mathbb{Q}$. 
\end{proof}


\newpage

\section{Insiemistica}

\begin{definizione}{Insieme Limitato}{}
\addcontentsline{toc}{subsection}{Definizione di Insieme Limitato}
Dato $\varnothing \ne A \subseteq \mathbb{R} $ è detto
\begin{itemize}
    \item \textbf{Superiorimente limitato} se  $\exists M \in \mathbb{R} : M \geq a \;\; \forall a \in A $
    \item \textbf{Inferiormente limitato} se  $\exists m \in \mathbb{R} : m \leq a \;\; \forall a \in A $
\end{itemize}

L'insieme $\{M: M \geq a \;\; \forall a \in A \}\label{eq:mag} $ è detto \textbf{Insieme dei maggioranti di $A$}.


L'insieme $\{m: m \leq a \;\; \forall a \in A \}$ è detto \textbf{Insieme dei minoranti di $A$}

\end{definizione}




\begin{definizione} {Massimo e Minimo}{}

\addcontentsline{toc}{subsection}{Definizione di Massimo e Minimo}
\noindent

\begin{itemize}
    \item Un maggiorante $M$ di $ A \subseteq \mathbb{R} $ è detto \textbf{massimo} se $M \in A$
    \begin{equation}\label{eq:max}
       M=  max(A) = (M \geq a \;\; \forall a \in A)  \wedge M \in A
    \end{equation}
    \item Un minorante $m$ di $ A \subseteq \mathbb{R} $ è detto \textbf{minimo} se $m \in A$
    \begin{equation} \label{eq:min1}
       m=  min(A) = (m \leq a \;\; \forall a \in A)  \wedge m \in A
    \end{equation}
\end{itemize}



    
\end{definizione}



\begin{teorema}{Unicità dei Massimi e Minimi}{}
\addcontentsline{toc}{subsection}{Teorema dell'unicità dei Massimi e Minimi}
    Dato $\varnothing \ne A \subseteq \mathbb{R}$ se ammette un massimo o un minimo, essi sono \textbf{unici} 
\end{teorema}

\begin{proof}
    Supponiamo che ci siano due massimi $M_1, M_2$ con $M_1 \neq M_2$. Visto che $M_1$ è un massimo allora, per definizione di massimo (\ref{eq:max}), deve essere $M_1 \in A$. Poi visto che $M_2$ è un massimo, di conseguenza è anche un maggiorante allora, per definizione di maggiorante,  deve valere la seguente affermazione

    \begin{equation*}
         M_2 \geq a \;\; \forall a \in A
    \end{equation*}

    Dato che $M_1 \in A$ possiamo dire che 
    
    \begin{equation} \label{eq:max1}
         M_2 \geq M_1
    \end{equation}


    Ora rifacciamo il seguente ragionamento ma al contrario. Dato che $M_2$ è massimo allora deve essere $M_2 \in A$. Visto che $M_1$ è un massimo deve anche essere un maggiorante, e per tanto vale

    \begin{equation*}
         M_1 \geq a \;\; \forall a \in A
    \end{equation*}

    Dato che $M_2 \in A$ possiamo dire che 
    
    \begin{equation} \label{eq:max2}
         M_1 \geq M_2
    \end{equation}


    Ora combinando le informazioni (\ref{eq:max1}) e (\ref{eq:max2}) deve valere 

    \begin{equation*}
        (M_2 \geq M_1) \wedge(M_1 \geq M_2) \Rightarrow M_1 = M_2
    \end{equation*}

    Dato che una delle ipotesi era che $M_1 \neq M_2$ abbiamo raggiunto un assurdo, per tanto il massimo deve essere unico. La dimostrazione per il minimo è analoga. 
\end{proof}



\begin{definizione}{Estremi Superiore e Inferiore}{}
 \addcontentsline{toc}{subsection}{Definizione Estremi Superiori e Inferiore}
    Dato $\varnothing \ne A \subseteq \mathbb{R} $ definiamo 
    \begin{itemize}
        \item \textbf{estremo superiore} di $A$ come il minore dei maggioranti.  
        \begin{equation}
            sup(A) = min\{M: M \geq a \;\; \forall a \in A \}
        \end{equation}

        \item \textbf{estremo inferiore} di $A$ come il maggiore dei minoranti.  
        \begin{equation*}
            inf(A) = max\{m: m \leq a \;\; \forall a \in A \}
        \end{equation*}
    \end{itemize}
\end{definizione}


\begin{teorema}{Relazione Massimi/Minimi e Estremi}{}
    Dato $\varnothing \ne A \subseteq \mathbb{R}$
\addcontentsline{toc}{subsection}{Relazione Massimi/Minimi e Estremi}
    \begin{itemize}
        \item Se esiste $max(A)$, allora coincide con $sup(A)$

        \begin{equation*}
            M = max(A) \Rightarrow M = sup(A)
        \end{equation*}

         \item Se esiste $min(A)$, allora coincide con $inf(A)$

        \begin{equation*}
            m = min(A) \Rightarrow m = inf(A)
        \end{equation*}
        
    \end{itemize}
\end{teorema}

\begin{proof}
    Supponiamo che esista $M_1 = max(A)$ allora sappiamo che è un maggiorante, e di conseguenza appartiene all'insieme dei maggioranti ($N$)
    
    \begin{equation} \label{eq:min3}
        M_1\in N = \{M:M\geq a \;\; \forall a \in A\}
    \end{equation}
    
    e sappiamo anche che $M_1 \in A$, visto che è il massimo. 
    
    Se prendiamo un numero $u \in N$ abbiamo  che 

    \begin{equation*}
        u\geq a \;\; \forall a \in A \;\; \forall u\in N
    \end{equation*}

    Visto che $M_1 \in A$ allora deve valere 
    
    \begin{equation*}
        u\geq M_1 \;\;  \forall u\in N
    \end{equation*}

    Per questo deduciamo che $M_1$ è minorante di $N$, ma nel punto (\ref{eq:min3}) avevamo detto che $M_1 \in N$, di conseguenza 

    \begin{equation*}
        M_1 = min(N)
    \end{equation*}

    Che per definizione è anche l'estremo superiore, quindi

    \begin{equation*}
        M_1 = sup(A)
    \end{equation*}

    La dimostrazione del minimo è analoga.
\end{proof}

\textbf{N.B.} che $max(A) \Rightarrow sup(A)$ e che $sup(A) \not\Rightarrow max(A) $. Per vederlo basta farsi degli esempi, come $A = \{x \in \mathbb{R} ^+ : x^2 < 2\}$ si vede che esiste un estremo superiore ($sup(A) = \sqrt{2}$) mentre non esiste il massimo.



\begin{teorema}{Caratterizzazione degli Estremi}{}

\addcontentsline{toc}{subsection}{Caratterizzazione degli Estremi}
Dato $\varnothing \ne A \subseteq \mathbb{R} $ se esiste $sup(A)$ oppure $inf(A)$ allora possiamo definirli anche come:


\begin{equation*}
    S= \sup(A)= \begin{cases}
S \in \{M:M\geq a \;\; \forall a \in A \} \\
\forall\varepsilon > 0  \;\; \exists a \in A : a > S-\varepsilon 
\end{cases}
\end{equation*}


  
\begin{equation*}
    s= \inf(A)= \begin{cases}
s \in \{m:m\leq a \;\; \forall a \in A \} \\
\forall\varepsilon > 0  \;\; \exists a \in A : a < s+\varepsilon 
\end{cases}
\end{equation*}


\end{teorema}
    

\addcontentsline{toc}{subsection}{Completezza di $\mathbb{R}$ \textit{I}° forma}

\begin{teorema}{Completezza di $\mathbb{R}$ \textit{I°} forma}{}

Dati $A, B \subseteq \mathbb{R}$ tali che $a \leq b \;\;\forall a\in A \;\;\forall b\in B$

allora $\exists c \in \mathbb{R}$, detto \textbf{elemento separatore} tale che 

\begin{equation*}
    a \leq c \leq  b \;\;\forall a\in A \;\;\forall b\in B
\end{equation*}
    
\end{teorema}

\addcontentsline{toc}{subsection}{Completezza di $\mathbb{R}$ \textit{II}° forma}
\begin{teorema}{Completezza di $\mathbb{R}$ \textit{II°} forma}{}
Dato $\varnothing \ne A \subseteq \mathbb{R}$ se è superiormente/inferiormente limitato allora ammette un estremo superiore/inferiore.

\end{teorema}

\begin{proof}
    Dato che $A$ è superiormente limitato allora esiste l'insieme dei maggiornati $N = \{M: M \in \mathbb{R} : M \geq a \;\; \forall a \in A\}$. Visto che tutti gli elementi dell'inisieme dei maggioranti è maggiore di tutti gli elementi di $A$ ($n \geq a \;\;\forall a\in A \;\;\forall n\in N$) possiamo applicare il Teorema di completezza di $\mathbb{R}$ \textit{I°} forma


    \begin{equation*}
        \exists c \in \mathbb{R} : \;\; \mathunderline{blue}{a \leq }c \mathunderline{red}{\leq  n }\;\;\forall a\in A \;\;\forall n\in N
    \end{equation*}

    Visto che $\mathunderline{blue}{a \leq c} \;\;\forall a\in A$ vuol dire che $c$ è un maggiorante di $A$ e di conseguenza $c\in N$ (l'insieme dei maggioranti). 

    Dato che $\mathunderline{red}{c \leq  n } \;\;\forall n\in N$ allora $c$ è un minorante di $N$. Quindi visto che $c \in N$, per definizione di minimo (\ref{eq:min1}) possiamo dire che 

    \begin{equation*}
        c = min(N)
    \end{equation*}

    Questo coincide con la definizione di \textbf{estremo superiore}, quindi

    \begin{equation*}
        c = sup(A)
    \end{equation*}

    Quindi abbiamo dimostrato che esiste un estremo superiore. 

    La dimostrazione don l'estremo inferiore è analoga.
\end{proof}

\newpage


\addcontentsline{toc}{subsection}{Definizione di Intorno}
\begin{definizione}{Intorno}
    Sia $r \in \mathbb{R} \;\cup \; \{\pm\infty\}$ allora

\vspace{-0.35cm}
    \begin{itemize}
        \item Se $r \in \mathbb{R}$ diciamo \textbf{intorno} un qualsiasi intervallo aperto della forma 
        \vspace{-0.35cm}
        \begin{equation*}
            (r-\varepsilon, r+\varepsilon) \;\; \epsilon >0
        \end{equation*}

        \item Se $r = +\infty$ diciamo \textbf{intorno} un qualsiasi intervallo aperto della forma 
        \vspace{-0.35cm}
        \begin{equation*}
            (M, +\infty) \;\; M \in \mathbb{R} \;\cup \; \{-\infty\}
        \end{equation*}

        \item Se $r = -\infty$ diciamo \textbf{intorno} un qualsiasi intervallo aperto della forma 
        \vspace{-0.35cm}
        \begin{equation*}
            (-\infty, M) \;\; M \in \mathbb{R} \;\cup \; \{+\infty\}
        \end{equation*}
    \end{itemize}
\end{definizione}



\begin{teorema}{Intersezione degli introni}{}
\addcontentsline{toc}{subsection}{Teorema di Intersezione degli Intorni}
\noindent

    Sia $I_1, I_2 \subseteq \mathbb{R}$ intorni di $x_0$, allora anche $I = I_1 \;\cap\; I_2$ è intorno di $x_0$.
\end{teorema}

\begin{teorema}{Separazione degli introni}{}
\addcontentsline{toc}{subsection}{Teorema di Separazione degli introni}
\noindent

    Sia $r_1, r_2 \in \mathbb{R} \;\cup \; \{\pm\infty\}$ allora $\exists I_1$ intorno di $r_1$ e $\exists I_2$ intorno di $r_2$ tali che $I_1 \;\cap\; I_2 = \varnothing$ 
\end{teorema}




    \addcontentsline{toc}{subsection}{Definizione di punto di Accumulazione}
\begin{definizione}{Definizione di punto di Accumulazione}
Sia $r\in \mathbb{R} \cup  \{\pm\infty\}$ è detto \textbf{punto di accumulazione} di $\varnothing \ne A \subseteq \mathbb{R}$ se
\vspace{-0.25cm}
    \begin{itemize}
        \item caso $r \in \mathbb{R}$: 
        \vspace{-0.35cm}
        \begin{center}
        
            $\forall I\subseteq \mathbb{R}$ intorno di $r\;\;\;A \cap(I \setminus \{r\}) \neq \varnothing$  
        \end{center}
    \vspace{-0.35cm}
        \item caso $r \in \{\pm\infty\}$: 
        \vspace{-0.35cm}
        \begin{center}
        
            $\forall I\subseteq \mathbb{R}$ intorno di $r\;\;\;A \cap I \neq \varnothing$  
        \end{center}
    \end{itemize}
\end{definizione}



\addcontentsline{toc}{subsection}{Definizione di punto di Accumulazione Destro/Sinistro}
\begin{definizione}{Accumulazione Destro/Sinistro}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $r\in \mathbb{R}$ è detto 

    \begin{itemize}
        \item \textbf{Punto di accumulazione destro} se
    \vspace{-0.20cm}
        \[
        \forall\varepsilon>0\;\;\; (r,r+\varepsilon) \cap A\neq \varnothing
        \]


        \item \textbf{Punto di accumulazione sinistro} se
\vspace{-0.20cm}
        \[
        \forall\varepsilon>0\;\;\; (r-\varepsilon,r) \cap A \neq \varnothing
        \]
    \end{itemize}

    \textbf{N.B.} i simboli $\pm \infty$ non ha senso definirli punti di accumulazione destro/sinistro
\end{definizione}

\newpage
\begin{esercizio}{}{}
    Sia $r\in \mathbb{R}$ e $\varnothing \ne A \subseteq \mathbb{R}$, $r$ è punto di accumulazione destro o sinistro di $A$ se e solo se $r$ è punto di accumulazione di $A$
\end{esercizio}

\begin{proof}
    Visto che questa è una doppia implicazione dobbiamo controllare che l'implicazione sia vera da entrambi i lati. 
    
    ($\implies$)Partiamo dimostrando che se $r$ è punto di accumulazione destro (o sinistro) di $A$ allora $r$ è punto di accumulazione di $A$. Partiamo con la definizione di punto di accumulazione destro.

    \[
    \forall\varepsilon>0\;\;\; (r,r+\varepsilon) \cap A \neq \varnothing
    \]

    Ora dobbiamo controllare se è vera la definizione di punto di accumulazione, per farla riscriviamola 

    \begin{center}
        $\forall I\subseteq \mathbb{R}$ intorno di $r\;\;\;A \cap(I \setminus \{r\}) \neq \varnothing$  
    \end{center}
    \[
        \forall \varepsilon>0 \;\;\; ((r-\varepsilon, r+\varepsilon) \setminus \{r\} ) \cap A \neq \varnothing
    \]


    Con questa riscrittura si nota che 


    \[
    (r,r+\varepsilon) \subseteq (r-\varepsilon, r+\varepsilon) \setminus \{r\}
    \]

    Di conseguenza per ogni insieme che troviamo per il punto di accumulazione destro, posso trovare un intervallo sul punto di accumulazione, di conseguenza se $r$ è un punto di accumulazione destro deve anche essere un punto di accumulazione in $A$. Ragionamento analogo per il punto di accumulazione sinistro


    ($\impliedby$)Per dimostrare che se $r$ è punto di accumulazione allora deve essere punto di accumulazione destro o sinistro, ragioniamo per assurdo: quindi supponiamo che $r$ non è ne punto di accumulaione destro ne sinistro. Allora sappiamo

    \[
        \exists\varepsilon_1>0,\exists\varepsilon_2>0 :(r-\varepsilon_1, r) \cap A = \varnothing \wedge (r, r + \varepsilon_2) \cap A = \varnothing 
    \]


    Ma se prendiamo $\varepsilon = min(\varepsilon_1,\varepsilon_2)$ allora 

    \[
        \exists\varepsilon>0 :  (r-\varepsilon_,  r + \varepsilon_) \cap A = \varnothing
    \]


    Questo non è altro che la definizione di punto isolato, ovvero la negazione di punto di accumulazione. Quindi abbiamo scoperto che 

    \begin{center}
        $r$ non è punto acc. dx e $r$ non è punto acc. sx $\Rightarrow$ $r$ non è punto di acc.
    \end{center}

    Usando le proprietà dell'implicazione $(P \Rightarrow Q ) \Leftrightarrow (\overline{Q}
 \Rightarrow \overline{P})$

    \begin{center}
        $r$ è punto di acc.$\Rightarrow$ $r$ è punto acc. dx opure $r$ è punto acc. sx 
    \end{center}

    
\end{proof}






\newpage





\section{Limiti}

\begin{definizione}{Definizione di Limite}{}

\addcontentsline{toc}{subsection}{Definizione di Limite}
    Dato $\varnothing \neq A \subseteq \mathbb{R}$, $x_0\in \mathbb{R} \cup  \{\pm\infty\}$ punto di accumulazione per $A$, $f:A\rightarrow\mathbb{R}$. Diciamo che $l  \in \mathbb{R} \;\cup \; \{\pm\infty\}$ è \textbf{limite di $f$ per $x\rightarrow x_0$} se

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I \subseteq \mathbb{R}$ intorno di $x_0$ tale che \\ $f(x) \in U\;\;\; \forall x \in A \cap (I \;\setminus \{x_0\})$
    \end{center}
\end{definizione}



\begin{teorema}{Unicità del limite}{}

\addcontentsline{toc}{subsection}{Teorema di Unicità del Limite}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $x_0\in \mathbb{R}\cup \{\pm\infty\}$ punto di accumulazione per $A$, $f:A\rightarrow\mathbb{R}$. Se esiste il limite $f$ per $x\rightarrow x_0$ allora è unico. E lo si indica con:

    \begin{equation}
        \lim_{x\to x_0} f(x) = l
    \end{equation}
\end{teorema}

\begin{proof}
    Supponiamo che il limite esista ed abbiamo due valori: $l_1, l_2 \in \mathbb{R}\cup \{\pm\infty\}$ con $l_1 \neq l_2$. Visto che per ipotesi $l_1 \neq l_2$ allora per il teorema di separazione abbiamo che $\exists U_1 \subseteq \mathbb{R}$ intorno di $l_1$ e $\exists U_2 \subseteq \mathbb{R}$ intorno di $l_2$ tali che:

    \begin{equation}\label{eq:unicita1}
         U_1 \cap U_2 = \varnothing
    \end{equation}

     Applicando la definizione di limite, $\exists I_1, I_2$ intorni di $x_0$ tali che:

    \begin{center}
        $\forall U_1$ intorno di $l_1 \;\;\;\;\; f(x) \in U_1 \;\;\; \forall x \in A \cap (I_1 \setminus \{x_0\})$
    \end{center}

    
    \begin{center}
        $\forall U_2$ intorno di $l_2 \;\;\;\;\; f(x) \in U_2 \;\;\; \forall x \in A \cap (I_2 \setminus \{x_0\})$
    \end{center}


     Usando il teorema di intersezione degli intorni, $\exists I_3 =I_1 \cap I_2$ intorno di $x_0$, e visto che $I_3 \subseteq I_1$, anche in esso varrà la proprietà del limite $l_1$. Contemporaneamente varrà anche la proprietà del limite $l_2$ visto che $I_3 \subseteq I_2$, per tanto

     \begin{center}
        $\forall U_1 \;\forall U_2  \;\;\;\;\; f(x) \in U_1  \wedge f(x) \in U_2 \;\;\; \forall x \in A \cap (I_3 \setminus \{x_0\})$
    \end{center}


    la congiungiole logica la possiamo riscrivere come congiunzione insiemistica

    \begin{center}
        $\forall U_1 \;\forall U_2  \;\;\;\;\; f(x) \in U_1  \cap U_2 \;\;\; \forall x \in A \cap (I_3 \setminus \{x_0\})$
    \end{center}

    Però questo necessita che $\forall U_1 \;\forall U_2 \;\;\; U_1  \cap U_2 \neq \varnothing$, perchè altrimenti il limite non esisterebbe. Ma all'inizio con l'equazione (\ref{eq:unicita1}) sappiamo che esistono almeno un $U_1$ e un $U_2$ che rendono l'intersezione vuoto. Per tanto è un assurdo e le ipotesi erano sbagliate. Di conseguenza il limite deve essere unico e non può assumere più di un valore. 

\end{proof}


\newpage

\begin{esercizio}{}{}
\addcontentsline{toc}{subsection}{Esercizi Dimostrazione Limite}
    \begin{equation*}
        \lim_{x\to x_0} 2x+3 = 2x_0 +3
    \end{equation*}    
\end{esercizio}

\begin{proof}
    Proviamo a dimostrarlo con la definizione. $f(x)=2x+3$ e, dato che è un polinomio, il suo dominio sarà $A = \mathbb{R}$. l'intorno di $l = 2x_0 +3$ sarà $U = (2x_0 +3-\varepsilon, 2x_0 +3 + \varepsilon)$, e un intorno di $x_0$ sarà $I =(x_0-\delta, x_0+ \delta)$. Di conseguenza con la definizione di limite sarà


    \begin{equation*}
        \forall U \;\;\;\; f(x) \in U \;\;\; \forall x \in  I \setminus\{x_0\}
    \end{equation*}

    Con i dati dell'esercizio

    \begin{equation*}
        \forall \varepsilon>0 \;\;\;\; 2x+3 \in (2x_0 +3-\varepsilon, 2x_0 +3 + \varepsilon) \;\;\; \forall x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\}
    \end{equation*}

    Riscriviamo il termine $2x+3 \in (2x_0 +3-\varepsilon, 2x_0 +3 + \varepsilon)$

    \begin{center}
         $2x_0 +3-\varepsilon <2x+3 < 2x_0 +3 + \varepsilon $\\
         $2x_0-\varepsilon <2x < 2x_0 + \varepsilon$ \\
         $x_0-\frac{\varepsilon }{2} <x < x_0 + \frac{\varepsilon }{2}$\\
    \end{center}


    Vediamo come partendo dalla prima porzione abbiamo trovato un intorno su cui deve stare $x$, pertanto affinchè sia vero la definizione di limite possiamo scegliere 

    \begin{equation*}
        \delta \leq \frac{\varepsilon}{2}
    \end{equation*}
\end{proof}


\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} c = c \;\;\; \forall c \in \mathbb{R}
    \end{equation*} 
\end{esercizio}


\begin{proof}
    Usiamo la definizione di limite 

\[
\forall \varepsilon>0 \;\;\;\; c \in (c-\varepsilon, c + \varepsilon) \;\;\; \forall x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\}
\]
    Scrivendola in un'altra maniera 
\[
\forall \varepsilon>0 \;\;\;\; x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\} \Rightarrow c \in (c-\varepsilon, c + \varepsilon)
\]

Però il conseguente della nostra implicazione è sempre vero, perchè $c$ sarà sempre nell'intevallo $(c-\varepsilon, c + \varepsilon)$ per qualsiasi valore di $\varepsilon$ positivo. Quindi volendo riscrivere la proposizione


    \[
\forall \varepsilon>0 \;\;\;\; x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\} \Rightarrow Vero
\]

Una implicazione è sempre vera quando implica vero (vedi tabella di verità dell'implicazione). Quindi la nostra proposizione è sempre vera, e di conseguenza il limite è verificato.
\end{proof}

\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} x^2 = x_0^2
    \end{equation*}
\end{esercizio}

\begin{proof}
    Con la definizione di limite abbiamo

    \begin{equation*}
        \forall \varepsilon>0 \;\;\;\; x^2 \in (x_0^2 -\varepsilon, x_0^2 + \varepsilon) \;\;\; \forall x \in  (x_0-\delta, x_0+ \delta) \setminus\{x_0\}
    \end{equation*}

    Utilizzando le proprietà della funzione modulo possiamo scriverlo anche come 

    \begin{equation*}
        \forall \varepsilon>0 \;\;\;\; |x^2 -x_0^2| < \varepsilon \; \Leftarrow \; |x - x_0| < \delta
    \end{equation*}

    Riscriviamo il primo termine

    
        \[|x^2 -x_0^2| < \varepsilon\] 
        \[|(x-x_0)(x+x_0)| < \varepsilon\]
        \[|x-x_0||x+x_0| < \varepsilon\] 
    


    Ora dobbiamo capire quanto vale $|x+x_0| $in modo da non avere più la variabile $x$. Per farlo scegliamo $\delta<1$

        
    
        \[|x+x_0| < \delta <1\]
        \[|x+x_0|<1\]
    

    con questo scopriamo che 
    
        \[|x+x_0| = |x-x_0+2x_0| \leq \mathunderline{red}{|x-x_0|} + 2|x_0| < \mathunderline{red}{1} + 2|x_0|\]
        \[|x+x_0| < 1+2|x_0|\]
    
    
    Moltiplicando per $|x-x_0|$, ricordandoci anche che $|x-x_0|<\delta$
    
    \begin{center}
        \[|x-x_0||x+x_0| < |x-x_0|(1 + 2|x_0|)\]
        \[|x-x_0||x+x_0| < \mathunderline{red}{|x-x_0|}(1 + 2|x_0|) < \mathunderline{red}{\delta}(1 + 2|x_0|)\]
        \[|x-x_0||x+x_0| < \delta(1 + 2|x_0|)\]
    \end{center}

    Partendo da $|x-x_0|<\delta$ siamo riusciti a capire che $|x^2-x_0^2|<\delta(1 + 2|x_0|)$, quindi se per verificare il limite bisogna che $|x^2-x_0^2|<\varepsilon$ è necessario imporre 

    
        \[\delta(1 + 2|x_0|) < \varepsilon  \]
        \[\delta < \frac{\varepsilon}{1 + 2|x_0|}\]
    
    Visto che abbiamo imposto $\delta<1$, aggiustiamo la definizione

    \[\delta < min \left( 1,\frac{\varepsilon}{1 + 2|x_0|}\right) \]
    
\end{proof}


\newpage

\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} \sqrt{x} = \sqrt{x_0}  \;\;\; \forall x_0\geq 0
    \end{equation*}
\end{esercizio}

\begin{proof}
    Usiamo la definizione di limite

    \[
    \forall \varepsilon > 0 \;\;\; |\sqrt{x} - \sqrt{x_0}|<\epsilon \;\;\; |x-x_0| < \delta 
    \]


    Partiamo analizzando il termine $|x-x_0| < \delta$


    \[
    |x-x_0| < \delta
    \]

    \[
    |\sqrt{x}-\sqrt{x_0}|\cdot|\sqrt{x}+\sqrt{x_0}| < \delta
    \]

    \[
    |\sqrt{x}-\sqrt{x_0}| < \frac{\delta}{|\sqrt{x}+\sqrt{x_0}|}
    \]

    Ora possiamo sfruttare la seguente espressione

    \[
    |\sqrt{x}+\sqrt{x_0}| \geq |\sqrt{x_0}|
    \]

    \[
    \frac{1}{|\sqrt{x}+\sqrt{x_0}|} \leq \frac{1}{|\sqrt{x_0}|}
    \]

    Di conseguenza

    \[
    |\sqrt{x}-\sqrt{x_0}| < \frac{\delta}{|\sqrt{x}+\sqrt{x_0}|} \leq \frac{\delta}{\sqrt{x_0}}
    \]

    \[
    |\sqrt{x}-\sqrt{x_0}| <  \frac{\delta}{\sqrt{x_0}}
    \]


    Affinchè il limite sia verificato è necessiario che 
    
    \[
    \frac{\delta}{\sqrt{x_0}} < \varepsilon
    \]

    \[
    \delta < \sqrt{x_0}\varepsilon
    \]


    \textbf{N.B.} per tutto l'esercio abbiamo potuto scrivere $\sqrt{x_0}$ non controllando se $x_0$ fosse non negativo perchè il dominio di $f(x) = \sqrt{x}$ è $\mathbb{R}^+_0$ e di conseguenza qualsiasi punto $x<0$ non è punto di accumulazione, dato che esiste almeno un intorno di un numero negativo che intersecato con il dominio ($\mathbb{R}^+_0$) dà insieme vuoto. E per questo il limite lo possiamo fare solo con valori di $x\geq0$ e possiamo scrivere $\sqrt{x_0}$ senza alcun problema.

\end{proof}
\newpage
\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} x^n = x_0^n \;\;\;\; \forall n\in\mathbb{N}
    \end{equation*}
\end{esercizio}

\begin{proof}
    Con la definizione di limite abbiamo
    
        \[|x^n -x_0^n| < \varepsilon\] 
        \[\left|(x-x_0)\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right| < \varepsilon\]
        \[|x-x_0|\left|\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right| < \varepsilon\] 
    


    Ora dobbiamo capire quanto vale il secondo termine in modo da non avere più la variabile $x$. 

        
    
        \[|x+x_0| < \delta <1\]
        \[|x+x_0|<1\]
    
    con questo scopriamo che 
    
        \[|x| = |x-x_0+x_0| \leq \mathunderline{red}{|x-x_0|} + |x_0| < \mathunderline{red}{1} + |x_0|\]
        \[|x| < 1+|x_0|\]
    
    
    Di conseguenza

    \[\left|\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right| \leq \sum_{k=0}^{n-1} |x^{n-1-k}x_0^k| = \sum_{k=0}^{n-1} |\mathunderline{red}{x^{n-1-k}}||x_0^k| \leq \sum_{k=0}^{n-1} |(\mathunderline{red}{1+|x_0|})^{n-1-k}||x_0^k|\] 


    Semplificando un po troviamo che 

    \[\left|\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right| \leq (1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k\]


    
    Moltiplicando per $|x-x_0|$, ricordandoci anche che $|x-x_0|<\delta$

        \[\mathunderline{blue}{|x-x_0|\left|\left(\sum_{k=0}^{n-1} x^{n-1-k}x_0^k\right)\right|}  < |x-x_0|(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k\]
        \[\mathunderline{blue}{|x^n-x_0^n|}< \mathunderline{red}{|x-x_0|}(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k < \mathunderline{red}{\delta}(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k\]
        \[|x^n-x_0^n| < \delta(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k\]
        


    Per trovare quanto vale $\varepsilon$ basta fare 

    
        \[\delta(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k < \varepsilon  \]
        \[\delta < \frac{\varepsilon}{(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k}\]
    
    Visto che abbiamo imposto $\delta<1$, aggiustiamo la definizione

    \[\delta < min \left( 1,\frac{\varepsilon}{(1+|x_0|)^{n-1}\sum_{k=0}^{n-1} \left(\frac{|x_0|}{1+|x_0|}\right)^k}\right) \]
    
\end{proof}

\begin{esercizio}{}{}
    \begin{equation*}
        \lim_{x\to x_0} sin(x) = sin(x_0)
    \end{equation*}
\end{esercizio}


\begin{proof}
    Dalla definizione di limite

    \[
    |sin(x)-sin(x_0)| < \varepsilon
    \]

    Usando le formule di Prostaferesi ($sin(\alpha) - sin(\beta) = 2cos\left(\frac{\alpha+\beta}{2}\right)sin\left(\frac{\alpha-\beta}{2}\right)$)


    \[
        |sin(x)-sin(x_0)| =     \left|2cos\left(\frac{x+x_0}{2}\right)sin\left(\frac{x-x_0}{2}\right)\right|
    \]

    Ora ricordiamo che per definizione delle funzioni trigonometriche, vale sempre la seguente proposizione

    \[
    |cos(x)| \leq 1  \;\;\;\;\;\;\;\;\;\;\;\; |sin(x)|\leq 1
    \]

    Quindi usiamo questa proprietà del coseno per diventare

     \[
        \left|2\mathunderline{red}{cos\left(\frac{x+x_0}{2}\right)}sin\left(\frac{x-x_0}{2}\right)\right| \leq 2\left|\mathunderline{red}{1}\cdot sin\left(\frac{x-x_0}{2}\right) \right|
    \]

    Poi ricordiamo anche che per la funzione seno vale la seguente relazione

    \[
    |sin(x)| \leq |x|
    \]

    E che quindi nella nostra dimostrazione possiamo usarla 

    \[
    2\left|sin\left(\frac{x-x_0}{2}\right) \right| \leq 2\left|\frac{x-x_0}{2}\right| = |x-x_0|
    \]


    Riscrivendo le informazioni trovate finora sappiamo che

    \[
        |sin(x)-sin(x_0)| \leq |x-x_0|
    \]

    Per la definizione di limite sappiamo che $|x-x_0| < \delta$

    \[
        |sin(x)-sin(x_0)| \leq |x-x_0| <\delta
    \]


    Ora se dobbiamo trovare il $\varepsilon$ basta impore

    \[
        |sin(x)-sin(x_0)| \leq |x-x_0| <\delta < \varepsilon
    \]

    \[\delta < \varepsilon\]


    Per il coseno la dimostrazione è analoga.
\end{proof}
    
\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} a^x = 1 \;\;\;\;\;a \in [1,+\infty)
    \]
\end{esercizio}

\begin{proof}
    Per dimostrare questo limite dobbiamo trovare un un qualche $\delta>0$ tale che

    \[
    \forall \varepsilon>0 \;\;\; |x-0| < \delta \Rightarrow |a^x-1| < \epsilon
    \]

    \[
        \forall \varepsilon>0 \;\;\; -\delta <x < \delta \Rightarrow 1-\epsilon < a^x < 1+\epsilon
    \]

    Quindi iniziamo partendo dalla disuguaglianza di Bernulli

    \begin{equation}
        (1+\varepsilon)^n \geq 1+n\varepsilon \;\;\; \forall n \in \mathbb{N}_0 \; \forall \varepsilon \geq 0
    \end{equation}

    Ora decidiamo che $a < 1+n\varepsilon$ e che quindi $\forall n>\frac{a-1}{\varepsilon}$ vale

    \[
    (1+\varepsilon)^n \geq 1+n\varepsilon > a
    \]

    \[
    (1+\varepsilon)^n  > a
    \]

    \[
        1+\varepsilon  > a^{\frac{1}{n}}
    \]


    Ora quindi sappiamo che per qualsiasi valore di $n>\frac{a-1}{\varepsilon}$ vale la relazione $a^{\frac{1}{n}} < 1+\varepsilon$, quindi se trovo per quali valori di $x$ vale la relazione $a^x<a^{\frac{1}{n}}$ posso imporre $a^x < a^{\frac{1}{n}} < 1+\varepsilon $ che vuol dire che abbiamo dimostrato la prima parte. 


    \[
    a^x < a^{\frac{1}{n}}
    \]

    Per monotonia della funzione $f(x) = a^x$ allora  vale

    \[
    x < \frac{1}{n}
    \]

    Quindi per dimostrare il limite basta scegliere un $\delta <\frac{1}{n}$ in modo tale che l'espressione $a^x <1+\varepsilon$ sia valida.
\newpage
    Per dimostrare la porzione $1-\varepsilon < a^x$ dobbiamo ricorrere alla formula 

    \begin{equation} \label{eq:bern}
        1-\varepsilon < \frac{1}{1+\varepsilon}
    \end{equation}
    


    Dai ragionamenti di prima sappiamo che $(1+\varepsilon)^n > a$, quindi vale anche

    \[
    \left(\frac{1}{1+\varepsilon}\right)^n < \frac{1}{a^n}
    \]

    Quindi se eleviamo tutto alla $n$ l'equazione (\ref{eq:bern}) ricaviamo

     \[
    (1-\varepsilon )^n<\left(\frac{1}{1+\varepsilon}\right)^n < \frac{1}{a}
    \]
    
    \textbf{N.B.} per non avere problemi di segno dobbiamo imporre $1-\varepsilon >0 \Rightarrow \varepsilon < 1$.
    
    \[
    (1-\varepsilon )^n< \frac{1}{a}
    \]

     \[
    1-\varepsilon < a^{-\frac{1}{n}}
    \]

    Quindi come per la prima parte della dimostrazione ora basta scegliere delle $x$ per cui $a^{-\frac{1}{n} }< a^x$, che per monotonia come prima rimane

    \[
    -\frac{1}{n} < x
    \]
    
    Per confermare la dimostrazione possiamo scegliere un $\delta$ tale che 

    \[
    -\frac{1}{n} < -\delta
    \]

    
    \[
    \delta < \frac{1}{n} 
    \]

    Quindi sce scegliamo un $\delta < \frac{1}{n}$ anche l'espressione $1-\varepsilon < a^x$ sarà verificata. Pertanto per verificare il limite basta scegliere 

    \[
    \delta = min\left(1, \frac{\varepsilon}{a-1}\right)
    \]
    
\end{proof}


\begin{esercizio}{}{}
    

    \[
        \lim_{x\to 0} a^x = 1 \;\;\;\;\;a \in(0,1)
    \]
\end{esercizio}
\begin{proof}
    Per dimostrare questo limite possiamo usare uno stratagemma per evitare di fare tutta la dimostrazione classica. Perchè con l'esercizio precendente abbiamo dimostrato con la base $a\geq 1$, quindi cerchiamo di ricondurli a quel limite. Per farlo usiamo le regole delle potenze infatti 

    \[
        0<a<1 \Rightarrow \frac{1}{a} > 1
    \]

    Quindi il limite lo possiamo riscrivere come


    \vspace{-0.30cm}
    \[
        \lim_{x\to 0} a^x = \lim_{x\to 0} \left( \frac{1}{a}\right)^{-x}
    \]

    In questa maniera la base è maggiore di 1, di conseguenza è uguale al limite dell'esercizio precedente da quel punto di vista. Quello che cambia è che all'esponente abbiamo $-x$ e non più $x$, però non è troppo un problema, infatti se $x\to 0$ allora anche $-x\to 0$, quindi l'esponente si avvicina lo stesso allo $0$, di conseguenza il limite sarà lo stesso, di prima e possiamo usare quello (che abbiamo già dimostrato) per dimostrare questo senza la dimostrazione rigorosa.

    \vspace{-0.35cm}
    \[
        \lim_{x\to 0} \left( \frac{1}{a}\right)^{-x} = \lim_{x\to 0} \left( \frac{1}{a}\right)^{x} = 1
    \]

    \textbf{N.B.} il passaggio dove diciamo che se $x\to 0$ allora $-x\to 0$ non è dimostrato in maniera rigorosa, infatti per questo passaggio serve il teorema del cambio di variabile che vedremo più avanti, ma intuitivamente ha senso che se $x\to 0$ allora $-x\to 0$.
\end{proof}



    Ora vediamo un caso particolare, che come vedremo non ha soluzione per come abbiamo definito il limite. 
  \begin{esercizio}{}{}  

    \begin{equation*}
        \lim_{x\to 0} \frac{1}{x} \neq +\infty
    \end{equation*}
\end{esercizio}
   \begin{proof}
       
    Proviamo usando la definizione di limite.
\vspace{-0.10cm}
    \[\forall M > 0 \; \exists \delta > 0 : f(x) \in (M, +\infty) \; \forall x \in (x_0-\delta, x_0+\delta)\setminus\{x_0\}
    \]
    \[\forall M > 0 \; \exists \delta > 0 : \frac{1}{x} \in (M, +\infty) \; \forall x \in (0-\delta, 0+\delta)\setminus\{0\}
    \]

    Partiamo analizzando $\frac{1}{x} \in (M, +\infty)$

    \[
    \frac{1}{x} \in (M, +\infty) \Leftrightarrow \frac{1}{x} > M 
    \]

    Ricordiamo che $M>0$, quindi anche $\frac{1}{x} >0 \Leftrightarrow x>0$. Ora possiamo fare il reciproco di entrambi i  membri (visto che sono entrampi positivi)

    \[
    \frac{1}{x} > M > 0 \Leftrightarrow  0 < x < \frac{1}{M}
    \]

    Quindi fino ad ora abbiamo capito che $f(x) \in (M, +\infty)$ è uguale a dire $0 < x < \frac{1}{M}$, però nella definizione di limite abbiamo che $\forall x \in (-\delta, \delta)\setminus\{0\}$ però questo è impossibile, perchè per qualsiasi valore di $\delta$ l'intevallo comprenderà anche numeri negativi (dato che l'intervallo è $(-\delta, \delta)$ ma ciò va in contraddizione con quanto abbiamo trovato prima (ovvero che $0<x<\frac{1}{M}$. Infatti non c'è nessun valore di $\delta>0$ che valida la seguente affermazione.

    \[
    (-\delta, \delta)\setminus\{0\} \nsubseteq (0, \frac{1}{M})
    \]

    Pertanto il limite è sbagliato. Il ragionamento con $-\infty$ è analogo.

    Per poter calcolare questo limite ci serve la nozione di limite destro e limite sinistro.
    
    \end{proof}



\begin{definizione}{Limite Destro e Sinistro}{}
    
\addcontentsline{toc}{subsection}{Definizione di Limite Destro e Sinistro}

    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f:A\to \mathbb{R}$ 

    \begin{itemize}
        \item Sia $x_0$ punto di accumulazione destro, allora definiamo limite destro di $f(x)$ come 

        \[
        \lim_{x\to x_0^+}f(x) = l
        \]

        E la sua caratterizzazione sarà

        \begin{center}
            $\lim_{x\to x_0^+}f(x) = l \Leftrightarrow $ $\forall U \subseteq \mathbb{R}$ intorno di $l$ $\exists I \subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \cap (x_0,+\infty) \;\;\; f(x) \in U$
        \end{center}


         \item Sia $x_0$ punto di accumulazione sinistro, allora definiamo limite sinistro di $f(x)$ come 

        \[
        \lim_{x\to x_0^-}f(x) = l
        \]

        E la sua caratterizzazione sarà

        \begin{center}
            $\lim_{x\to x_0^-}f(x) = l \Leftrightarrow $ $\forall U \subseteq \mathbb{R}$ intorno di $l$ $\exists I \subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \cap (-\infty,x_0) \;\;\; f(x) \in U$
        \end{center}
    \end{itemize}
\end{definizione}

    Ora proviamo a risolvere il limite di prima con il limite destro.

\begin{esercizio}{}{}

    \begin{equation*}
        \lim_{x\to 0^+} \frac{1}{x} = +\infty
    \end{equation*}

\end{esercizio}

\begin{proof}
    Usiamo la definizione di limite destro

    \[
    \forall M>0 \;\;\exists \delta >0 : \; \frac{1}{x} \in (M, +\infty ) \;\; \forall x \in (0-\delta, 0+\delta) \setminus \{0\} \cap (0, +\infty)  
    \]

    Riscriviamo meglio l'ultimo termine

    \[
    (-\delta, +\delta) \setminus \{0\} \cap (0, +\infty) = (0, \delta)
    \]

    Ora possiamo fare gli stessi ragionamenti di prima 

    \[
    \frac{1}{x} \in (M, +\infty ) \Leftrightarrow \frac{1}{x} > M 
    \]

    Visto che $M>0 $ allora anche $x>0$, per lo stesso ragionamento di prima

    \[
    \frac{1}{x} > M  \Leftrightarrow 0<x <\frac{1}{M}
    \]

    Ora però il risultato è diverso da prima infatti, dopo le semplificazione, la nostra condizione del limite sarà $\forall x \in (0, \delta) \Rightarrow x \in (0, \frac{1}{M})$. Ora affinchè questa proposizione sia vera basta prendere

\vspace{-0.50cm}
    \[
        \delta \leq \frac{1}{M}
    \]

    E quindi ora il limite è verificato. Per il limite $\lim_{x\to 0^-} \frac{1}{x} = -\infty$ il ragionamento è analogo.
\end{proof}


\begin{esercizio}{}{}
    \[
    \lim_{x\to 0^-} \frac{1}{x^2} = +\infty
    \]
\end{esercizio}

\begin{proof}
    Usiamo la definizione 

    \[
        \forall M>0 \;\;\exists \delta >0 : \; \frac{1}{x^2} \in (M, +\infty ) \;\; \forall x \in (0-\delta, 0+\delta) \setminus \{0\} \cap (-\infty, 0)  
    \]

    Riscrivendo meglio la definizione

    \begin{equation}\label{eq:sx}
       \forall \delta >0 \;\;\; x \in (-\delta, 0) \Rightarrow \frac{1}{x^2}  \in (M, +\infty)        
    \end{equation}
     


    Riscriviamo il secondo termine

    \[ 
    \frac{1}{x^2}  \in (M, +\infty) \Leftrightarrow \frac{1}{x^2} > M
    \]

    Ora non abbiamo nessun problema riguardante il segno visto che $x^2>0\;\;\forall x \neq 0$, quindi possiamo invertire la disequazione
    \vspace{-0.30cm}
    \[
    x^2 < \frac{1}{M}
    \]

    Visto che, sia $x^2$ che $\frac{1}{M}$ sono positivi possiamo fare la radice quadrata ambo i membri 

    \[
    \sqrt{x^2} < \sqrt{\frac{1}{M}}
    \]
    
    \[
    |x| < \frac{1}{\sqrt{M}}
    \]

    \[
   - \frac{1}{\sqrt{M}}< x < \frac{1}{\sqrt{M}}
    \]

    Riscrivendo l'espressione (\ref{eq:sx}) 

    \[
    \forall \delta >0 \;\;\; x \in (-\delta, 0) \Rightarrow x \in \left(- \frac{1}{\sqrt{M}},  \frac{1}{\sqrt{M}}\right) 
    \]

    Questa implicazione è vera quando vale

    \[
    -\delta\geq - \frac{1}{\sqrt{M}}
    \]

    \[
    \delta \leq \frac{1}{\sqrt{M}}
    \]
    
\end{proof}

\begin{teorema}{Relazione Limite con limite Destro e Sinistro}{}
\addcontentsline{toc}{subsection}{Relazione Limite con limite Destro e Sinistro}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 \in \mathbb{R}$ punto di accumulazione di $f(x)$ e $l \in \mathbb{R} \cup \{\pm \infty\}$ allora


    \begin{equation*}
        \lim_{x\to x_0} f(x) = l \Leftrightarrow \lim_{x\to x_0^-} f(x) = \lim_{x\to x_0^+} f(x) = l
    \end{equation*}
\end{teorema}


\begin{proof}
    Visto che è una doppia implicazione dovremmo controllare entrambe le direzione

    ($\implies$) quindi, usando la definizione di limite, sappiamo che 

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \setminus \{x_0\} \; \Rightarrow \; f(x) \in U$
    \end{center}


    Se quindi sappiamo che l'affermazione è vera (per ipotesi) $\forall x \in A \cap I \setminus \{x_0\}$ allora varrà anche per un qualunque sottoinsieme, quindi la proposizione sarà vera anche per l'insieme 
    $A \cap I \cap (x_0, +\infty)$ e anche  per $A \cap I \cap (-\infty, x_0)$, che sono gli insiemi compresi nella definizione di limite destro e limite sinistro. Pertanto saranno valide anche le definizioni di limite destro e sinistro e quindi è verificata l'implicazione.


    ($\impliedby$) Se quindi esiste il limite destro e sinistro sappiamo che

    \begin{itemize}
        \item per limite destro

        \begin{center}
            $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I_1\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I_1 \cap (x_0, +\infty) \; \Rightarrow \; f(x) \in U$
        \end{center}

        \item per limite sinistro

        \begin{center}
            $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I_2\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I_2 \cap (-\infty,x_0 ) \; \Rightarrow \; f(x) \in U$
        \end{center}
    \end{itemize}

    Se noi ora, per il teorema di intersezione degli intorni, possiamo trovare un $I = I_1 \cap I_2$ intorno di $x_0$. tale che vale
\begin{center}
    $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \cap ((-\infty,x_0 ) \cup (x_0, +\infty)) \; \Rightarrow \; f(x) \in U$

\end{center}
    
    che scrivendo meglio

\begin{center}
    $\forall U \subseteq \mathbb{R}$ intorno di $l$, $\exists I\subseteq \mathbb{R}$ intorno di $x_0$ tale che $\forall x \in A \cap I \setminus \{x_0 \} \; \Rightarrow \; f(x) \in U$

\end{center}

    Che valida la definizione di $\displaystyle\lim_{x\to x_0} f(x) = l$.
\end{proof}
\newpage
\begin{esercizio}{}{}

    Sia $f(x)=\begin{cases}
        1 & \text{se } x<0 \\ 
        200 & \text{se } x=0 \\
         1 & \text{se } x>0 \\
    \end{cases}$ Allora 

    \begin{equation*}
        \lim_{x\to 0} f(x) = 1
    \end{equation*}
    
\end{esercizio}


\begin{proof}
    Partiamo analizzando il limite sinistro 

    \[
    \lim_{x\to 0^-} f(x)
    \]


    Ora nell'intervallo $(-\infty, 0)$ la funzione assume sempre il valore $1$, quindi possiamo sostiture la funzione nel limite nel suo valore

    \[
    \lim_{x\to 0^-} f(x)=\lim_{x\to 0^-} 1 = 1
    \]

    Ora facciamo lo stesso ragionamendo con il limite destro, e visto che anche nell'intervallo $(0, +\infty)$ assume sempre il valore $1$ possiamo calcolare il limite destro

    \[
    \lim_{x\to 0^+} f(x)=\lim_{x\to 0^+} 1 = 1
    \]

    Ora, dato che il limite destro e sinistro esistono e sono uguali, per il teorema visto prima sappiamo che esiste anche il limite

     \[
    \lim_{x\to 0} f(x)= 1
    \]

    \textbf{N.B.} questo è un esempio lampante per capire che il limite studia "ciò che è attorno" ad un punto di una funzione, e al limite "non tiene conto" di cosa fa la funzione nel punto effettivo, come in questo esempio anche se $f(0) = 200$ non influenza il valore del limite. 
\end{proof}

\newpage

\begin{teorema}{Limite del Valore Assoluto di una Funzione}{}

\addcontentsline{toc}{subsection}{Limite del Valore Assoluto di una Funzione}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 \in \mathbb{R}$ punto di accumulazione di $f(x)$ e $l \in \mathbb{R}$ allora

    \begin{equation*}
        \lim_{x\to x_0} f(x) = l \implies \lim_{x\to x_0} |f(x)| = |l|
    \end{equation*}
\end{teorema}


\begin{proof}
    Inizialmente iniziamo a studiare per $l>0$, quindi per ipotesi sappiamo che 

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $l$ $\exists I\subseteq \mathbb{R}$ intorno di $x_0 :
        x \in I \implies f(x) \in U$
    \end{center}

    E dobbiamo trovare che

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $|l|$ $\exists I\subseteq \mathbb{R}$ intorno di $x_0 :
        x \in I \implies |f(x)| \in U$
    \end{center}

    Partiamo dalla prima proposizione, e riscriviamo meglio la prima parte

    \begin{center}
        $\forall U \subseteq \mathbb{R}$ intorno di $l$ 
    \end{center}

    \[
     \forall \varepsilon > 0 \;\;\; |f(x)-l| < \varepsilon
    \]

    \[
        l-\varepsilon < f(x) < l+\varepsilon
    \]

    Notiamo che se scegliamo $\varepsilon<l$ allora $l-\varepsilon > 0$ e quindi

    \[
       0 < l-\varepsilon < f(x) < l+\varepsilon
    \]

    Quindi ora tutti i termini sono positivi allora, per la seguente proprietà del valore assoluto $a >0 \implies a = |a|$ possiamo sostituire il termine $f(x)$ con $|f(x)|$

    \[
       0 < l-\varepsilon < |f(x)| < l+\varepsilon
    \]

    Ora riprendiamo il la condizione che abbiamo imposto $\varepsilon < l$, noi sappiamo, per definizione di limite, che $\varepsilon > 0$ quindi $0<\varepsilon < l$ di conseguenza $l>0$, quindi abbiamo scoperto che che anche $l$ è positivo e che quindi possiamo usare la stessa proprietà di che abbiamo usato per $|f(x)|$ per mettere il modulo

    \[
       |l|-\varepsilon < |f(x)| < |l|+\varepsilon
    \]

    Con questo siamo riusciti a verificare il limite perchè $|f(x)|$ è in un introno di $|l|$. Ora però ci manca da controllare i casi con $l<0$, e per evitare di usare la dimostrazione classica di limite usiamo uno stratagemma

    \[
    \lim_{x\to x_0} f(x) = l < 0 \iff \lim_{x\to x_0} -f(x) = -l > 0 
    \]

    Ora visto che $l<0$ allora $-l > 0$ e di conseguenza possiamo usare il teorema che abbiamo appena verificato (e possiamo applicarlo proprio perchè $-l > 0$)
    
    \[
    \lim_{x\to x_0} -f(x) = -l  \implies \lim_{x\to x_0} |-f(x)|  = |-l| \implies \lim_{x\to x_0} |f(x)|  = |l|
    \]
    
    E quindi anche per $l<0$ il risultato rimane lo stesso e quindi il limite è verificato $\forall l \neq 0$. 
\end{proof}

\addcontentsline{toc}{subsection}{Limite del Valore Assoluto di una Funzione (caso $l=0$)}

\begin{teorema}{Limite del Valore Assoluto di una Funzione (caso $l=0$)}{}

    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 \in \mathbb{R}$ punto di accumulazione di $f(x)$ e $l \in \mathbb{R}$ allora

    \begin{equation*}
        \lim_{x\to x_0} f(x) = 0 \iff \lim_{x\to x_0} |f(x)| = 0
    \end{equation*}
\end{teorema}

\begin{proof}
    Visto che c'è una dobbia implicazione controlliamo entrambi i sensi

    ($\implies$) Sappiamo che $\forall \varepsilon >0$ $\exists\delta>0$ tale che

    \[
    f(x) \in (-\varepsilon, \varepsilon) \;\;\;\; \forall x \in (x_0-\delta,x_0+\delta)
    \]

    Riscrivendo meglio il primo termine

    \[
    -\varepsilon < f(x) <  \varepsilon
    \]

    Usiamo le proprietà dei valori assoluti

    \[
    -\varepsilon < f(x) <  \varepsilon \iff 0\leq|f(x)| < \varepsilon
    \]

    Quindi ora sappiamo che il limite è verificato per $0\leq |f(x)| < \varepsilon$ ma quindi possiamo "allargare" l'intervallo e varrà comunque la proprietà e quindi

    \[
    0\leq |f(x)| < \varepsilon \implies -\varepsilon < |f(x)| < \varepsilon
    \]

    E di conseguenza è verificato il limite $\lim_{x\to x_0} |f(x)| = 0$.

    ($\impliedby$) Sappiamo che $\forall \varepsilon >0$ $\exists\delta>0$ tale che

    \[
    |f(x)| \in (-\varepsilon, \varepsilon) \;\;\;\; \forall x \in (x_0-\delta,x_0+\delta)
    \]

    Riscrivendo meglio il primo termine

    \[
    -\varepsilon < |f(x)| <  \varepsilon
    \]

    Possiamo togliere la parte $-\varepsilon$ perchè il valore assoluto è sempre positivo

    \[
    |f(x)| <  \varepsilon \iff -\varepsilon < f(x) <  \varepsilon
    \]

     e quindi è verificato il limite $\displaystyle\lim_{x\to x_0} f(x) = 0$.
\end{proof}

\textbf{N.B.} faccendo un riassunto dei due teoremi appena fatti sappiamo che 

\[
\lim_{x\to x_0} f(x) = l \implies \lim_{x\to x_0} |f(x)| = |l|
\]\[
\lim_{x\to x_0} f(x) = l \iff \lim_{x\to x_0} |f(x)| = 0
\]

E vedendo bene notiamo che se $\displaystyle\lim_{x\to x_0} |f(x)| = |l|$ allora non possiamo dire nulla su $\displaystyle\lim_{x\to x_0} f(x) = l $. Vediamo un esempio.

\newpage
\begin{esercizio}{}{}
    Sia $f(x)=\begin{cases}
        1 & \text{se }x\leq 0 \\
        -1 & \text{se }x> 0
    \end{cases}$  

\end{esercizio}
possiamo notare che 
    \[
    |f(x)|=\begin{cases}
        |1| & \text{se }x\leq 0 \\
        |-1| & \text{se }x> 0
    \end{cases} = \begin{cases}
        1 & \text{se }x\leq 0 \\
        1& \text{se }x> 0
    \end{cases} \iff  |f(x)| = 1
    \]

    E che quindi 

    \[
    \lim_{x\to 0} |f(x)| = \lim_{x\to 0} 1 = 1
    \]

    Ora proviamo a vedere $\lim_{x\to 0} f(x)$, e visto che è una funzione definita a tratti facciamo il limite destro e sinistro. Partiamo con quello sinistro e vediamo che la funzione nell'intervallo $(-\infty, 0) $ assume il valore $1$ quindi

    \[
     \lim_{x\to 0^-} f(x) = \lim_{x\to 0^-} 1 = 1
    \]

    Con il limite destro e la nostra funzione nell'intervallo $(0, +\infty)$ assume il valore $-1$ e quindi

    \[
     \lim_{x\to 0^+} f(x) = \lim_{x\to 0^-} -1 = -1
    \]

    Notiamo che 
    \[
     \lim_{x\to 0^-} f(x) \ne \lim_{x\to 0^+} f(x) \implies\nexists\lim_{x\to 0} f(x)
    \]
    


\begin{teorema}{ Permanenza del Segno}{}

\addcontentsline{toc}{subsection}{Teorema della Permanenza del Segno}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f:A\to\mathbb{R}$, $x_0$ punto di accumulazione di $f(x)$ e $l=\lim_{x\to x_0} f(x)$ allora 

    \begin{itemize}
        \item Se $l>0$ allora $f(x) >0$ definitivamente per $x\to x_0$
        \item Se $l<0$ allora $f(x) <0$ definitivamente per $x\to x_0$
    \end{itemize}
\end{teorema}

\begin{proof}
    Facciamo la dimostrazione per $l>0$, gli altri casi sono analoghi.

    Per ipotesi sappiamo che il limite esiste, e pertanto 

    \begin{center}
        $\forall \varepsilon>0$ $\exists I$ intorno di $x_0$ tale che $f(x) \in (l-\varepsilon, l+\varepsilon) \;\;\; \forall x \in A\cap (I \setminus \{x_0\}) $
    \end{center}

    Se scelgo $\varepsilon < l$ avrò che 

    \[
    \varepsilon < l \implies l-\varepsilon >0 \implies 0 < l-\varepsilon < f(x)
    \]

    di conseguenza

    \[
    f(x) > 0 \;\;\; \forall x \in A\cap (I \setminus \{x_0\})
    \]

    
\end{proof}
\newpage

\addcontentsline{toc}{subsection}{Limiti e Relazioni d'Ordine \textit{I}}
\begin{teorema}{limiti e relazioni d'ordine \textit{I}}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $l_1=\lim_{x\to x_0} f(x)$ e   $l_2=\lim_{x\to x_0} g(x)$

  
   \begin{center}
       $l_1 < l_2 \implies f(x) < g(x)$ definitivamente per $x\to x_0$
   \end{center}
    

\end{teorema}

\begin{proof}
    Dato che $l_1 < l_2$ sappiamo che $l_1 \ne l_2$ e quindi per il teorema di separazione degli intorni $\exists U_1$ intorno di $l_1$ e $\exists U_2$ intorno di $l_2$ tali che 

    \begin{equation}\label{eq:not}
        U_1 \cap U_2 = \varnothing
    \end{equation}

    Ora per definizione di limite sappiamo 

    \begin{itemize}
        \item $\forall U_1$ intorno di $l_1$ $\exists I_1$ intorno di $x_0$ tale che $f(x) \in U_1 \;\;\; \forall x \in A\cap (I_1 \setminus \{x_0\}) $

        \item $\forall U_2$ intorno di $l_2$ $\exists I_2$ intorno di $x_0$ tale che $g(x) \in U_2 \;\;\; \forall x \in A\cap (I_2 \setminus \{x_0\}) $
    \end{itemize}


    Ora se dato che abbiamo $I_1$ e $I_2$ intorni di $x_0$, per il teorema di intersezione sappiamo 

   \begin{center}
       $\exists I = I_1 \cap I_2$ intorno di $x_0$
   \end{center}

   E quindi nell'intorno $I$ varrà
    
    \begin{equation}\label{eq:fg}
        f(x) \in U_1 \land g(x) \in U_2 \;\;\; \forall x \in A\cap (I \setminus \{x_0\}) 
    \end{equation}

    riscriviamo l'equazione (\ref{eq:not})

    \[
        (l_1 - \varepsilon_1,l_1 + \varepsilon_1) \cap (l_2 - \varepsilon_2,l_2 + \varepsilon_2)  = \varnothing 
    \] 

    Dato che per ipotesia sappiamo $l_1 < l_2$, cioò può accedere soltanto se 

    \[
    \mathunderline{red}{l_1 + \varepsilon_1 < l_2 - \varepsilon_2}
    \]

\vspace{-0.35cm}
    Ora usiamo questa informazione e combiniamola con la formula (\ref{eq:fg})
    
    \[
    f(x) \in (l_1 - \varepsilon_1,l_1 + \varepsilon_1)
    \land g(x) \in (l_2 - \varepsilon_2,l_2 + \varepsilon_2)
    \]
    \[
    \mathunderline{blue}{l_1 - \varepsilon_1 <f(x) < l_1 + \varepsilon_1} \land \mathunderline{blue}{l_2 - \varepsilon_2 <g(x)< l_2 + \varepsilon_2}
    \]
    \vspace{-0.35cm}
    \[
    \mathunderline{blue}{f(x) < l_1} +\mathunderline{red}{ \varepsilon_1 < l_2} - \mathunderline{blue}{\varepsilon_2 <g(x)}
    \]
    \vspace{-0.35cm}
    \[
    f(x) < g(x) \;\;\;\forall x \in A\cap (I \setminus \{x_0\}) 
    \]
\end{proof}

\newpage

\begin{teorema}{limiti e relazioni d'ordine \textit{II}}{}

\addcontentsline{toc}{subsection}{Limiti e Relazioni d'Ordine \textit{II}}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $l_1=\lim_{x\to x_0} f(x)$ e   $l_2=\lim_{x\to x_0} g(x)$

  
   \begin{center}
       $f(x) \leq g(x)$ definitivamente per $x\to x_0 \implies l_1 \leq l_2 $
   \end{center}
    

\end{teorema}

\begin{proof}
    La dimostrazione segue per assurdo, quindi supponiamo che $l_1 > l_2$, allora per il teorema della relazione d'ordine I sappiamo che 

    \begin{center}
       $l_1 > l_2 \implies f(x) > g(x)$ definitivamente per $x\to x_0$
   \end{center}

   Ma ciò va in contraddizione con le ipotesi iniziali $f(x) < g(x)$ pertanto è impossibile che $l_1> l_2$ e di conseguenza è vero che $l_1 \leq l_2$. 
\end{proof}

\begin{esercizio}{}{}
    \textbf{N.B.} se $f(x) < g(x)$ non possiamo dire con certezza nulla su $l_1 < l_2$. Vediamo un esempio. Sia $f(x) = 0$ e $g(x) = x^2$. Noi sappiamo che 

    \[
    f(x) < g(x) \;\;\; \forall x \in \mathbb{R} \setminus \{0\}
    \]

    Ma i limiti per $x\to 0$ fanno $\displaystyle\lim_{x\to 0} f(x) =\lim_{x\to 0} g(x) = 0$.
\end{esercizio}


\begin{teorema}{Due Carabinieri}{}

\addcontentsline{toc}{subsection}{Teorema dei due Carabinieri}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g,h:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $\lim_{x\to x_0} f(x)=\lim_{x\to x_0} h(x) = l$.

    
    \[
    f(x) \leq g(x) \leq h(x) \implies \lim_{x\to x_0} g(x) = l
    \]
\end{teorema}

\begin{proof}
    Visto che $f(x)$ e $h(x)$ hanno limite, sappiamo che

    \begin{itemize} 
    \centering
        \item $\forall U$ intorno di $l$ $\exists I_1$ intorno di $x_0$ tale che $f(x) \in U \;\;\; \forall x \in A\cap (I_1 \setminus \{x_0\}) $

        \item $\forall U$ intorno di $l$ $\exists I_2$ intorno di $x_0$ tale che $h(x) \in U \;\;\; \forall x \in A\cap (I_2 \setminus \{x_0\}) $
    \end{itemize}

    Ora per il teorema di intersezione degli intorni sappiamo che

    \begin{center}
        $\exists I = I_1 \cap I_2$ intorno di $x_0$
    \end{center}

    In $I$ vale 

    \[
    f(x) \in U \land h(x) \in U \;\;\; \forall x \in A\cap (I \setminus \{x_0\})
    \]

    Pertanto sappiamo che 

    \[
    \mathunderline{red}{l - \varepsilon <f(x)} < l + \varepsilon \land  - \varepsilon <\mathunderline{red}{h(x) < l + \varepsilon}
    \]
\newpage
    Combinando questa informazione con le ipotesi ($\mathunderline{blue}{f(x) \leq g(x) \leq h(x)}$ definitivamente per $x\to x_0$)

    \[
    \mathunderline{red}{l - \varepsilon <f}(\mathunderline{blue}{x)\leq g(x) \leq h }(\mathunderline{red}{x) < l + \varepsilon}
    \]

    Di conseguenza 

    \[
    l - \varepsilon < g(x) < l + \varepsilon\;\;\; \forall x \in A\cap (I \setminus \{x_0\})
    \]

    E questà è la definizione di limite, quindi questo implica che 

    \[
    \exists \lim_{x\to x_0} g(x) =  l
    \]
    
\end{proof}


\begin{corollario}{Teorema dei carabinieri II}{}
Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $f(x) \leq g(x)$ definitivamente per $x\to x_0$. Allora


    \begin{itemize}
    \centering
        \item se $\displaystyle  \lim_{x\to x_0} f(x) = +\infty \implies \lim_{x\to x_0} g(x) = +\infty$
        \item se $\displaystyle  \lim_{x\to x_0} g(x) = -\infty \implies \lim_{x\to x_0} f(x) = -\infty$
    \end{itemize}

\end{corollario}

\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{\sin(x)}{x} = 1
    \]
\end{esercizio}

\begin{proof}
Iniziamo disegnando una circonferenza unitaria e notiamo che

\begin{center}
    

\begin{tikzpicture}
  \begin{axis}[
  xmin=0, xmax=1.3,
ymin=0, ymax=1.3,
axis equal,
axis lines=middle,
enlargelimits=false,
clip=false,
axis line style={-stealth, thick},
 width=8cm
]


    % --- Circonferenza unitaria ---
    \addplot[
      domain=0:90,
      samples=200,
      black,
       thick
    ] ({cos(x)}, {sin(x)});

    % --- Raggio ---
    \addplot[gray, ultra thick] coordinates {(0,0) (1, {tan(40)})};


    % --- Arco dell’angolo (sulla circonferenza) ---
    \addplot[blue, ultra thick, domain=0:40, samples=100]
      ({cos(x)}, {sin(x)});

    % --- Seno (rosso, verticale) ---
    \addplot[red, ultra thick] coordinates {({cos(40)}, 0) ({cos(40)}, {sin(40)})};



    \addplot[green, ultra thick] coordinates {(1, 0) (1, {tan(40)})};

    % --- Etichette ---
    \node[blue] at (axis cs:{cos(40*0.7)+0.01}, {sin(40*0.7)+0.1}) {$x$};
    \node[red] at (axis cs:{cos(40)-0.15},{sin(40)/2}) {$\sin(x)$};
    \node[green!50!black, right] at (axis cs:1,0.84) {$\tan(x)$};

  \end{axis}
\end{tikzpicture}
\end{center}

Dal grafico possiamo notare che in un intorno di 0 abbiamo che

\[
\sin(x) \leq x \le \tan(x)
\]
 \newpage
Ora possiamo  dividere tutto per $\sin(x)$

\[
\frac{\sin(x)}{\sin(x)} \leq \frac{x}{\sin(x)} \le \frac{\tan(x)}{\sin(x)}
\]

\[
1 \leq \frac{x}{\sin(x)} \le \frac{1}{\cos(x)}
\]

inveriamo tutti i membri (e anche i segni delle disequazioni)

\[
1 \geq \frac{\sin(x)}{x} \ge \cos(x)
\]


Ora vediamo che $\displaystyle\lim_{x\to 0} 1 = 1$,$\displaystyle\lim_{x\to 0} \cos(x) = \cos(0)=  1$, e visto che le due funzioni estreme tendono entrambe a $1$ e la funzione $\frac{\sin(x)}{x} $ è compresa tra le altre due funzioni definitivamente per $x\to x_0$ allora per il teorema dei carabinieri abbiamoche 

\[
\lim_{x\to 0} \frac{\sin(x)}{x} = 1
\]

\end{proof}
\vspace{-0.50cm}
\begin{teorema}{Algebra dei Limiti Finiti}{}
\addcontentsline{toc}{subsection}{Algebra dei Limiti Finiti}
    Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$ e $\displaystyle\lim_{x\to x_0} f(x)=l_1\in\mathbb{R}$ e $ \displaystyle\lim_{x\to x_0} g(x) = l_2 \in \mathbb{R}$. Allora

\begin{enumerate}[label=(\roman*)]

    \centering
    \item $\displaystyle\lim_{x\to x_0} [f(x)+g(x)] = l_1+l_2$
    \item $\displaystyle \lim_{x\to x_0} [f(x)\cdot g(x)] = l_1\cdot l_2$
    \item $\displaystyle\lim_{x\to x_0} \frac{f(x)}{g(x)} = \frac{l_1}{l_2}$ (se $l_2 \ne 0$)
\end{enumerate}

\end{teorema}


\begin{proof}
    $(i)$ Partiamo scrivendo le definizioni di limite come sappiamo 
    \begin{itemize} 
    \centering
        \item $\forall \varepsilon>0$ $\exists I_1$ intorno di $x_0$ tale che $f(x) \in (l_1-\varepsilon, l_1+\varepsilon) \;\;\; \forall x \in A\cap (I_1 \setminus \{x_0\}) $

        \item $\forall \varepsilon>0$ $\exists I_2$ intorno di $x_0$ tale che $g(x) \in (l_2-\varepsilon, l_2+\varepsilon) \;\;\; \forall x \in A\cap (I_2 \setminus \{x_0\}) $
    \end{itemize}

    Quindi per il teorema di intersezione trovo un $I= I_1 \cap I_2$ intorno di $x_0$ tale che

    \[
    \forall \varepsilon>0\;\; f(x) \in (l_1-\varepsilon, l_1+\varepsilon) \land g(x) \in (l_2-\varepsilon, l_2+\varepsilon)
    \]
    \[
    \mathunderline{red}{l_1-\varepsilon <f(x)< l_1+\varepsilon} \;\;\;\;\; \mathunderline{blue}{l_2-\varepsilon<g(x)< l_2+\varepsilon}
    \]
    \[
    \mathunderline{red}{l_1-\varepsilon} +\mathunderline{blue}{l_2-\varepsilon}<\mathunderline{red}{f(x)}+\mathunderline{blue}{g(x)}< \mathunderline{red}{l_1+\varepsilon}+\mathunderline{blue}{l_2+\varepsilon}
    \]
    \[
    (l_1 +l_2)-2\varepsilon<f(x)+g(x)< (l_1+l_2)+2\varepsilon
    \]

E notiamo che $f(x)+g(x)$ è in un intorno di $l_1+l_2$ e che quindi il limite è verificato.

\textbf{N.B.} anche se c'è scritto $2\varepsilon$ e non solamente $\varepsilon$ va bene lo stesso, anche perchè l'espressione all'interno della definizione di limite è $\forall \varepsilon>0$ e quindi anche se moltiplico $\varepsilon$ per una qualsiasi costante, potrò rappresentare qualunque intorno.

\newpage

$(ii)$ Partiamo analizzando il seguente modulo, e compensando il termine $l_1\cdot g(x)$


\[
    |f(x)\cdot g(x) - l_1\cdot l_2| = |f(x)\cdot g(x)-\mathunderline{red}{l_1\cdot g(x)} + \mathunderline{red}{l_1\cdot g(x)} - l_1\cdot l_2| \]

    Ora raccogliamo alcuni termini
\[
|f(x)\cdot \mathunderline{red}{g(x)}-l_1\cdot \mathunderline{red}{g(x)} + \mathunderline{blue}{l_1}\cdot g(x) - \mathunderline{blue}{l_1}\cdot l_2| = |(f(x)-l_1)\cdot \mathunderline{red}{g(x)} + \mathunderline{blue}{l_1}\cdot (g(x)-l_2)|
\]

Applichiamo la disuguaglianza triangolare


\begin{align*}
    |(f(x)-l_1)\cdot g(x) + l_1\cdot (g(x)-l_2)| &\leq |(f(x)-l_1)\cdot g(x)| + |l_1\cdot (g(x)-l_2)| \\
    &=|(f(x)-l_1)|\cdot |g(x)| + |l_1|\cdot |(g(x)-l_2)| 
\end{align*}

Ora dalle definizioni di limite sappiamo che $|f(x)-l_1| < \varepsilon$ e  $|g(x)-l_2| < \varepsilon$

\[
\mathunderline{red}{|(f(x)-l_1)|}\cdot |g(x)| + |l_1|\cdot \mathunderline{blue}{|(g(x)-l_2)|} < \mathunderline{red}{\varepsilon}\cdot |g(x)| + |l_1|\cdot\mathunderline{blue}{\varepsilon} = \varepsilon\cdot(|g(x)| + |l_1|)
\]

Per valutare la quanto vale $|g(x)|$ facciamo qualche sistemazione algebrica

\begin{align*}
    |g(x)| &= |g(x) - l_2 + l_2|\\
    &\leq | g(x) - l_2| + |l_2|\\
    &< \varepsilon + |l_2|
\end{align*}

Con questo possiamo semplificare

\[
\varepsilon\cdot (|g(x)| + |l_1|)< \varepsilon\cdot ((\varepsilon+|l_2|) + |l_1|)
\]

Per semplificare possiamo scegliere $\varepsilon<1$

\[
\varepsilon\cdot ((1+|l_2|) + |l_1|) = \varepsilon\cdot (1+|l_2| + |l_1|)
\]

Facendo un po' di ordine vediamo che 

\[
|f(x)\cdot g(x) - l_1\cdot l_2| < \varepsilon\cdot (1+|l_2| + |l_1|)
\]


Di Conseguenza abbiamo trovato che $|f(x)\cdot g(x) - l_1\cdot l_2|$ è sempre minore di $\varepsilon$, appatto di qualche costante proporzionale. Infatti $1+|l_2| + |l_1|$ è sempre maggiore di $1$ e quindi il limite è verificato.

\newpage

\textit{($iii$)} Per verificare questo limite è necessario verificare che 

\begin{equation}\label{eq:divLim1}
\lim_{x\to x_0} \frac{1}{g(x)} = \frac{1}{l_2}
\end{equation}

Perchè se fosse vero potremmo usare il teorema del prodotto perchè

\begin{equation}\label{eq:divLim2}
    \lim_{x\to x_0} \frac{f(x)}{g(x)} = \lim_{x\to x_0}\left[  f(x)\cdot \frac{1}{g(x)}\right] = l_1 \cdot \frac{1}{l_2} = \frac{l_1}{l_2}
\end{equation}


Quindi proviamo a verificare (\ref{eq:divLim1}) con la definizione di limite

\[
    \left| \frac{1}{g(x)} - \frac{1}{l_2} \right| = \left| \frac{l_2 - g(x)}{g(x)\cdot l_2}\right| = \frac{|g(x) - l_2|}{|g(x)||l_2|}
\]


Visto che il limite $\displaystyle\lim_{x\to x_0} g(x) = l_2$ è verificato per ipotesi, allora sappiamo $\exists I_1$ intorno di $x_0$ tale che $|g(x) - l_2|< \varepsilon$ $\forall x \in I$,  e quindi 

\[
\forall x \in I_1 \;\;\; \frac{|g(x) - l_2|}{|g(x)||l_2|} < \frac{\varepsilon}{|g(x)||l_2|}
\]

Ora per capire quanto vale $|g(x)|$ dobbiamo dividere i casi con $l_2 >0$ e $l_2 <0$, noi ora vedremo la dimostrazione per $l_2 >0$, l'altro caso è analogo.

Quindi sfruttando il teorema della permanenza del segno noi sappiamo che $exists I_2$ intorno di $x_0$ tale che $g(x)>0 $ $\forall x \in I_2$, di conseguenza sarà vero anche che $\forall x \in I_2$  $g(x) > \frac{l_2}{2}$, e quindi anche $\frac{1}{g(x)} < \frac{2}{|l_2|}$.

Possiamo usare il teorema di intersezione degli intorno per trovare $I_3 = I_1 \cap I_2$ intorno di $x_0$ tale che 


\[
\forall x \in I_3 \;\;\; \frac{|g(x) - l_2|}{|g(x)||l_2|} < \frac{\varepsilon}{|g(x)||l_2|} = \frac{\varepsilon}{|l_2|} \cdot \frac{1}{|g(x)|} < \frac{\varepsilon}{|l_2|} \cdot \frac{2}{|l_2|} = \frac{2\varepsilon}{|l_2|^2}
\]

E che quindi 
\[
\left| \frac{1}{g(x)} - \frac{1}{l_2} \right|  < \frac{2\varepsilon}{|l_2|^2}
\]

Per lo stesso ragionamento fatto prima nel prodotto, abbiamo trovato che il limite è minore di $\varepsilon$ appatto di una costante moltiplicativa.

Quindi abbiamo trovato un intevallo $I_3$ che verifica il limite $\displaystyle\lim_{x\to x_0} \frac{1}{g(x)} = \frac{1}{l_2}$ e che quindi usando anche il teorema del prodotto, si verifica il teorema della divisione, come visto nel punto (\ref{eq:divLim2}). Chiaramente visto che $g(x)$ è a denominatore è necessario che $g(x) \ne 0$ definitivamente per $x\to x_0$.
\end{proof}

\newpage 


\addcontentsline{toc}{subsection}{Algebra dei Limiti Infiniti (Forme Determinate)}
\begin{teorema}{Algebra dei Limiti Infiniti (Forme Determinate)}{}
        Siano $\varnothing \ne A \subseteq \mathbb{R}$, $f,g:A\to\mathbb{R}$, $x_0$ punto di accumulazione in $A$. Allora


    \begin{enumerate}[label=(\roman*)]

        \centering
        \item 
        Se $\displaystyle\lim_{x\to x_0} f(x) = \color{red}\pm \color{black}\infty$ e $g(x)$ è definitivamente limitata per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} [f(x)\pm g(x)] = \color{red}\pm \color{black}\infty\]

    
        \item Se $\displaystyle\lim_{x\to x_0} f(x) = 0$ e $g(x)$ è definitivamente limitata per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} [f(x)\cdot g(x)] = 0]\]

        \item Se $\displaystyle\lim_{x\to x_0} f(x) = \color{red}\pm \color{black}\infty$ e $\exists c > 0 : g(x) \leq c$ definitivamente per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} [f(x)\cdot g(x)] = \color{red}\pm \color{black}\infty\]
    
        \item Se $\displaystyle\lim_{x\to x_0} f(x) = \color{red}\pm \color{black}\infty$ e $\exists c > 0 : 0 < g(x) \leq c$ definitivamente per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} \frac{f(x)}{g(x)} = \color{red}\pm \color{black}\infty\]
    
        \item Se $\displaystyle\lim_{x\to x_0} f(x) = \pm \infty$ e $g(x)$ è definitivamente limitata per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} \frac{g(x)}{f(x)} = 0\]

        \item Se $\displaystyle\lim_{x\to x_0} f(x) = 0$, $f(x)$ è positiva definitivamente per $x\to x_0$ e $\exists c > 0 : g(x) > c$ definitivamente per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} \frac{g(x)}{f(x)} = +\infty\]
        
        \item Se $\displaystyle\lim_{x\to x_0} f(x) = 0$, $f(x)$ è negativa definitivamente per $x\to x_0$ e $\exists c > 0 : g(x) > c$ definitivamente per $x\to x_0$ allora
        \[\displaystyle\lim_{x\to x_0} \frac{g(x)}{f(x)} = -\infty\]
    \end{enumerate}


    
\end{teorema}

\newpage

\addcontentsline{toc}{subsection}{Esercizi sull'Algebra dei Limiti Infiniti}
\begin{esercizio}{}{}
    \[
        \lim_{x \to +\infty} x^2 + \sin(x) = +\infty
   \]
\end{esercizio}

\begin{proof}
    Vediamo che 
    \[
    \lim_{x \to +\infty} x^2 = +\infty
    \]

    In più $|\sin(x)| \leq 1$ $\forall x \in \mathbb{R}$ e quindi lo è anche definitivamente per $x\to +\infty$, Quindi come nella casistica $(i)$ dell'algebra dei limiti finiti abbiamo che 
\[\lim_{x \to +\infty} x^2 + \sin(x) = +\infty\]

\end{proof}


\begin{esercizio}{}{}
    \[
        \lim_{x \to +\infty} x(\sin(x)+2) = +\infty
   \]
\end{esercizio}

\begin{proof}
    Notiamo che 
    \[
    \lim_{x \to +\infty} x = +\infty
    \]

    In più $0< \sin(x) +2 \leq 3$ $\forall x \in \mathbb{R}$ e quindi lo è anche definitivamente per $x\to +\infty$, Quindi come nella casistica $(iv)$ dell'algebra dei limiti finiti abbiamo che 
\[\lim_{x \to +\infty} x(\sin(x)+2) = +\infty\]

\end{proof}


\begin{esercizio}{}{}
    \[
        \lim_{x \to -2} \frac{x}{(x+2)^2} = +\infty
   \]
\end{esercizio} 



\begin{proof}
    Se proviamo a calcolare il limite notiamo che il denominatore tende a 0, mentre il numeratore tende a -2 quindi sembra di essere nella casistica ($vi$), controlliamo se le ipotesi sono verificate. 

    In primis il teorema richiede che $f(x)$ sia positivo definitivamente per $x\to -2$, è questo è verificato sempre, infatti $(x+2)^2 > 0 \implies \forall x \ne -2 $. In più il numeratore ($x$) è limitato definitivamente per $x\to -2$, pertanto il teorema è applicabile e quindi 
    \[
        \lim_{x \to -2} \frac{x}{(x+2)^2} = +\infty
   \]

    \textbf{N.B.} Se il limite fosse stato $\displaystyle\lim_{x\to -2}\frac{x}{x+2}$ il teorema non sarebbe applicabile, perchè $x+2$ non è positiva definitivamente per $x\to -2$, perchè un qualsiasi intorno dalla parte sinistra sarebbe negativo e invece la parte destra sarebbe positiva. Pertanto non si può applicare il teorema ($vi$). Per risolverlo è necessario calcolare o il limite destro o sinistro, infatti in quei intorni ($x+2$) è positivo definitivamente. Quindi $\displaystyle\lim_{x\to -2^-}\frac{x}{x+2} = +\infty$ e $\displaystyle\lim_{x\to -2^+}\frac{x}{x+2} = -\infty$. E da questo notiamo che $\nexists \displaystyle\lim_{x\to -2}\frac{x}{x+2}$ perchè il limite destro e sinistro sono diversi.
\end{proof}


\addcontentsline{toc}{subsection}{Forme Indeterminate}
\begin{definizione}{Forme Indeterminate}
    SSi dicono \textbf{Forme Indeterminate} tutti i limiti che hanno come risultato 

   \[
\begin{array}{@{\qquad}c@{\qquad}c@{\qquad}c@{\qquad}}
\bigl[\dfrac{\infty}{\infty}\bigr] & \bigl[\dfrac{0}{0}\bigr] & \bigl[\infty \cdot 0\bigr] \\[6pt]
\bigl[+\infty - \infty\bigr] & \bigl[\infty^{0}\bigr] & \bigl[1^{0}\bigr]
\end{array}
\]


    E il risultato effettivo del limite non si può determinare subito, ma sono necessarie altre operazioni.

    \textbf{N.B.} Pertanto due limiti che hanno inizialmente la stessa forma indeterminata posso avere limiti diversi, vediamo degli esempi.
\end{definizione}



\addcontentsline{toc}{subsection}{Primi Esercizi sulle Forme Indeterminate}

\begin{esercizio}{}{}
    \[
    \lim_{x\to +\infty} \frac{2x^2+3x-1}{x^2 - 1}
    \]
\end{esercizio}


\begin{proof}
    Se proviamo a calcolare il limite vediamo che il numeratore tende a $+\infty$ e lo stesso si può dire per il denominatore. Quindi caschiamo nella forma indeterminata del tipo $\bigl[\dfrac{\infty}{\infty}\bigr]$. Pertanto dobbiamo fare delle manipolazioni, proviamo raccogliendo il grado maggiore ($x^2$) al numeratore e lo stesso facciamo anche a demonimatore
    \[
    \lim_{x\to +\infty} \frac{2x^2+3x-1}{x^2 - 1} = \lim_{x\to +\infty} \frac{x^2\left(2+\frac{3}{x}-\frac{1}{x^2}\right)}{x^2\left(1-\frac{1}{x^2}\right)}
    \]

    Notiamo che il termine $x^2$ si può semplificare

    \[
     \lim_{x\to +\infty} \frac{\cancel{x^2}\left(2+\frac{3}{x}-\frac{1}{x^2}\right)}{\cancel{x^2}\left(1-\frac{1}{x^2}\right)} = \lim_{x\to +\infty} \frac{2+\frac{3}{x}-\frac{1}{x^2}}{1-\frac{1}{x^2}}
    \]

    Ora possiamo calcolare il limite infatti i termini $\frac{3}{x}$, $\frac{1}{x^2}$ tendono a 0 quando $x\to \infty$ (questo grazie alle forme determinate) e quindi 

    \[
        \lim_{x\to +\infty}
        \frac{2+\circled[red!75!black]{\frac{3}{x}}-\circled[red!75!black]{\frac{1}{x^2}}}{1-\circled[red!75!black]{\frac{1}{x^2}}} = \frac{2 + 0 - 0}{1 - 0} = 2
        \]


    Quindi 

     \[
    \lim_{x\to +\infty} \frac{2x^2+3x-1}{x^2 - 1} = 2
    \]

    Quindi noi siamo partiti con una forma indeterminata e siamo arrivati a una soluzione che è 2. Ora vediamo che un altro limite sempre con la stessa forma indeterminata, ma avremo un altro risultato.
\end{proof}

\newpage

\begin{esercizio}{}{}
    \[
    \lim_{x\to +\infty} \frac{x^3+5x}{x^2 +7x - 1}
    \]
\end{esercizio}

\begin{proof}
    Notiamo subito che esce la stessa forma indeterminata: $\bigl[\dfrac{\infty}{\infty}\bigr]$ e quindi proviamo a fare la stessa tecnica di prima

    \[
    \lim_{x\to +\infty} \frac{x^3+5x}{x^2 +7x - 1} = \frac{x^3\left(1 + \frac{5}{x^2}\right)}{x^2\left(1 + \frac{7}{x}-\frac{1}{x^2}\right)} = \frac{x\left(1 + \frac{5}{x^2}\right)}{1 + \frac{7}{x}-\frac{1}{x^2}} 
    \]

    Ora come prima i termini con la $x$ a denominatore tendono a 0, però a numeratore è rimasto una $x$ che tende a $+\infty$ quindi il numeratore, per la proprietà ($iii$) delle forme determinate, tende a $+\infty$, il denominatore invece tende a 1, e quindi per la proprietà ($iv$) il limite tende a $+\infty$.
\[
    \lim_{x\to +\infty} \frac{x^3+5x}{x^2 +7x - 1} = +\infty
    \]

    \textbf{N.B.} inizialmente anche questo limite era della forma $\bigl[\dfrac{\infty}{\infty}\bigr]$ ma abbiamo avuto un risultato diverso da prima, e quindi quando ci troviamo davanti una forma indetermnata sappiamo che dobbiamo rimaneggiare i termini.
\end{proof}


\begin{esercizio}{}{}
    \[
    \lim_{x\to +\infty} \sqrt{x^2+x+1} - x
    \]
\end{esercizio}

\begin{proof}
    Proviamo a calcolare il limite ma notiamo subito che viene fuori una forma indeterminata della forma $\bigl[+\infty - \infty\bigr]$ e quindi dobbiamo fare dei rimaneggiamenti. Ricordandoci la formula della somma per differenza ($(A+B)(A-B)=A^2-B^2$) possiamo moltiplicare e dividere per il binomio coniugato
    \begin{align*}
        \lim_{x\to +\infty} \sqrt{x^2+x+1} - x &=     \lim_{x\to +\infty} (\sqrt{x^2+x+1} - x) \cdot \frac{\sqrt{x^2+x+1} + x}{\sqrt{x^2+x+1} + x} \\ 
        &=  \lim_{x\to +\infty}   \frac{(\sqrt{x^2+x+1} - x)(\sqrt{x^2+x+1} + x)}{\sqrt{x^2+x+1} + x} \\ 
        &= \lim_{x\to +\infty} \frac{(\sqrt{x^2+x+1})^2 - x^2}{\sqrt{x^2+x+1} + x} \\
        &= \lim_{x\to +\infty} \frac{ x^2+x+1 - x^2}{\sqrt{x^2+x+1} + x} \\
        &= \lim_{x\to +\infty} \frac{ x+1 }{\sqrt{x^2+x+1} + x}
    \end{align*}
    
    Dopo tutti questi maneggiamenti sembra che abbiamo solo che complicato il limite, però li abbiamo fatto diventare in un limite nella forma $\bigl[\dfrac{\infty}{\infty}\bigr]$ che però abbiamo già visto come risolvere, infatti basta che raccogliamo il grado maggiore 

    \[
    \lim_{x\to +\infty} \frac{ x+1 }{\sqrt{x^2+x+1} + x} = \lim_{x\to +\infty} \frac{ x\left(1+\frac{1}{x}\right) }{\sqrt{x^2\left(1+\frac{1}{x}+\frac{1}{x^2}\right)} + x}
    \]

    Ora per "tirare fuori" $x^2$ dalla radice, dobbiamo ricordarci di mettere il modulo (perchè $\sqrt{x^2} = |x|$), però dato che noi stiamo analizzando per $x\to +\infty$ siamo sicuri che $x>0$ (per definizione di limite) e pertanto $|x| = x$

    \[
    \lim_{x\to +\infty} \frac{ x\left(1+\frac{1}{x}\right) }{x\sqrt{\left(1+\frac{1}{x}+\frac{1}{x^2}\right)} + x} = \lim_{x\to +\infty} \frac{ x\left(1+\frac{1}{x}\right) }{x\cdot\left(\sqrt{1+\frac{1}{x}+\frac{1}{x^2}} + 1\right)} = \lim_{x\to +\infty} \frac{ 1+\frac{1}{x}}{\sqrt{1+\frac{1}{x}+\frac{1}{x^2}} + 1}
    \]

    Possiamo calcolare il limite 

    \[
    \lim_{x\to +\infty} \frac{ 1+\circled[red!75!black]{\frac{1}{x}}}{\sqrt{1+\circled[red!75!black]{\frac{1}{x}}+\circled[red!75!black]{\frac{1}{x^2}}} + 1} = \frac{1 + 0}{\sqrt{1 + 0 +0 } +1} = \frac{1}{2}
    \]
\end{proof}

\begin{esercizio}{}{}
    Dato un generico polinomio $P(x) = \displaystyle\sum_{i=0}^{n} a_i x^i$ dove $a_i$ sono i coefficenti del polinomio e $n$ il grado del polinomio. Con $a_n > 0$
    \[
        \lim_{x\to +\infty} P(x)
    \]
\end{esercizio}


\begin{proof}
    Il limite è nella forma $[+\infty -\infty]$ e quindi procediamo raccogliendo il grado maggiore ($x^n$)

    \[
        \lim_{x\to +\infty} P(x) = \lim_{x\to +\infty} (a_0 + a_1x + ... + a_nx^n) = \lim_{x\to +\infty} x^n\left(\frac{a_0}{x^n} + \frac{a_1}{x^{n-1}} + ... + a_n\right) 
    \]

    Ora possiamo calcolare il limite infatti tutti i termini dentro le parentesi infatti tendonon tutti a 0. Quindi il termine dentro le parentesi tende a $a_0$ e che quindi moltiplicato per $x^n\to +\infty$  tente a $ +\infty$. Se invece $a_n<0$ il limite tendeva a $-\infty$.

    \[
        \lim_{x\to +\infty} x^n\left(\circled[red!75!black]{\frac{a_0}{x^n}} + \circled[red!75!black]{\frac{a_1}{x^{n-1}}} + \circled[red!75!black]{...} + a_n\right)  = +\infty
    \]

    Quindi con questo iniziamo a capire che nei polinomi quello che ci interessa quando $x\to \infty$ è il termine con il grado più alto ($x^n$), intatti per risolvere questo esercizio i termini più piccoli di $x^n$ è come se li avessimo trascurati. Infatti è vera la seguente equazione $\displaystyle\lim_{x\to \infty} P(x) =\lim_{x\to \infty} a_nx^n$ per qualsiasi polinomio $P(x)$ e $\forall a_n \in \mathbb{R}\setminus \{0\}$. 
\end{proof}


\addcontentsline{toc}{subsection}{Teorema del Cambio di Variabile}
\begin{teorema}{Cambio di Variabile}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $\varnothing \ne B \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $g:B \to \mathbb{R}$ e $x_0 \in \mathbb{R} \cup \{\pm \infty\}$ un punto di accumulazione in $f(A)\cap B$ allora se $\exists \displaystyle\lim_{x\to x_0} f(x) = y_0$, con $y_0$ punto di accumulazione in $B$   e se è vera almeno una delle due proposizioni 
    \begin{itemize}
        \item $f(x) \ne y_0$ definitivamente per $x\to x_0$
        \item $\displaystyle \lim_{y\to y_0} g(y) = g(y_0)$ \;\;\;\;\; (continuità di $g(x)$)
    \end{itemize}

    Allora 
    \[
        \lim_{x\to x_0} g(f(x)) = \lim_{y\to y_0} g(y)
    \]
\end{teorema}


\begin{esercizio}{}{}
    Ora vediamo perchè è fondamentale che almeno una dei due requisiti sia vero, proviamo con un controesempio. Infatti sia $f(x)=5$ e $g(x)=\begin{cases}
        2 & \text{se } x\ne 5 \\
        1  & \text{se } x= 5 \\
    \end{cases}$ e vediamo subito che nessuna delle due proposizioni è vera. 
\end{esercizio}

\begin{proof}
    Infatti il limite effettivo, senza usare il teorema del cambio di variabile è
    \[
    \lim_{x\to x_0} g(f(x)) = \lim_{x\to x_0} g(5) = \lim_{x\to x_0} 1 = 1 
    \]

    Invece se proviamo a usare il cambio di variabile, dobbiamo prima calcolare $y_0$
    \[
    \lim_{x\to x_0} f(x) = 5\;\;\; [=y_0]
    \]

    Ora il limite diventa
    \[
    \lim_{x\to x_0} g(f(x)) = \lim_{y\to 5} g(y) = 2
    \]

    Quindi usando solo le funzioni composte il limite è uscito 1, mentre con il teorema del cambio di variabile è venuto fuori 2, cosa impossibile per il teorema di unicità del limite e pertanto il teorema del cambio di variabile non si può applicare in questo esercizio, proprio perchè mancavano i criteri richiesti dal teorema stesso. 
    
\end{proof}


\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} \frac{\sin(x^2)}{x^2}
    \]
\end{esercizio}


\begin{proof}
    Vediamo che assomiglia molto al limite $\displaystyle\lim_{x\to 0} \frac{\sin(x)}{x} = 1$ l'unica cosa che cambia è che abbiamo $x^2$ anzichè $x$, quindi proviamo a cambiare la variabile $x^2$ con $y$, quindi dobbiamo calcolare 
    \[
        \lim_{x\to 0} x^2 = 0\;\;\; [=y_0]
    \]
    Visto che sono valide tutte le condizioni del teorema del cambio di variabile, infatti $x^2 \ne 0$ in un intorno di $0$. Mentre l'altra condizione non è valida infatti non si può calcolare in $0$ la funzione $g(y)=\frac{\sin(y)}{y}$, però non ci interessa perchè il teorema richiede almeno una delle due proposizioni. 

    Quindi il limite diventa
    \[
        \lim_{x\to 0} \frac{\sin(x^2)}{x^2} = \lim_{y\to 0} \frac{sin(y)}{y} = 1
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to +\infty} e^{\frac{1}{x^2-x}}
    \]
\end{esercizio}

\begin{proof}
    Al denominatore abbiamo una forma del tipo $\bigl[+\infty - \infty \bigr]$, quindi proviamo a vedere come si comporta quel denominatore per $x\to+\infty$. Per calcolarlo possiamo usare la proprietà dei polinomi che abbiamo visto nell'esercizio dei polinomi, infatti basta tenere il grado maggiore ($x^2$)
    \[
        \lim_{x\to +\infty} x^2 - x = \lim_{x\to +\infty} x^2  = +\infty \;\;\; [=y_0]
    \]
    Vediamo che il denominatore ha limite e quindi possiamo fare il cambio di variabile e possiamo applicarlo perchè è valida la prima condizione, infatti $x^2-x \ne +\infty$ sempre, mentre la seconda non può mai essere vera perchè non possiamo calcolare $g(+\infty)$, perchè ricordiamo che $\pm \infty$ non sono punti di nessun dominio
    \[
    \lim_{x\to +\infty} e^{\frac{1}{x^2-x}} = \lim_{y\to +\infty} e^{\frac{1}{y}}
    \]
    Ora possiamo riutilizzare il teorema del cambio di variabile, visto che non siamo ancora in un limite noto, e quindi vediamo come si comporta la frazione all'esponente
    \[
    \lim_{y\to+\infty} \frac{1}{y} = 0 \;\;\; [=z_0]
    \]
    Visto che ha limite e rispetta sempre il primo criterio e anche il secondo del teorema del cambio di variabile, allora possiamo riapplicare il teorema e finalmente calcolare il limite.
    \[
    \lim_{y\to +\infty} e^{\frac{1}{y}} = \lim_{z\to 0} e^z = 1
    \]
    Quindi 
    \[
    \lim_{x\to +\infty} e^{\frac{1}{x^2-x}} = 1
    \]
\end{proof}

\newpage

\addcontentsline{toc}{subsection}{Limite di funzioni Monotone caso Finito}
\begin{teorema}{Limite di funzioni Monotone}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $I\subseteq \mathbb{R}$ intorno di $x_0 \in \mathbb{R}$ tale che $f(x)$ è monotona in $I$

    \begin{itemize}
        \item Se $f(x)$ è monotona crescente allora 
        \[
        \lim_{x\to x_0^-} f(x) = \sup\{f(A \cap I \cap (-\infty, x_0))\}
        \]
        \[
        \lim_{x\to x_0^+} f(x) = \inf\{f(A \cap I \cap (x_0, +\infty))\}
        \]
        \item Se $f(x)$ è monotona decrescente allora 
        \[
        \lim_{x\to x_0^-} f(x) = \inf\{f(A \cap I \cap (-\infty, x_0))\}
        \]
        \[
        \lim_{x\to x_0^+} f(x) = \sup\{f(A \cap I \cap (x_0, +\infty))\}
        \]
    \end{itemize}
\end{teorema}

\begin{proof}
    Faremo la dimostrazione del caso $f(x)$ è crescente e per il limite sinistro, gli altri casi sono analoghi.

    Per ipotesi chiaramente supponiamo che esista $S=\sup\{f(A \cap I \cap (-\infty, x_0))\}$, quindi per definizione di superiore, sappiamo che il superiore ($S$) è più grande di qualsiasi elemento nell'insieme (cioè $f(x)$), quindi
    \begin{equation}\label{eq:extr1}
        f(x) \leq S \;\;\;\;\; \forall x  \in A \cap I \cap (-\infty, x_0)
    \end{equation}

    Ora usando la caratterizzazione degli estremi e sappiamo che 
    \[
    f(\hat{x}) > S - \varepsilon \;\;\; \forall \varepsilon > 0 \;\; \exists \hat{x} \in A \cap I \cap (-\infty, x_0)
    \]
    Poi, per monotonia della funzione sappiamo che se $\hat{x} < x$ allora 
    \begin{equation}\label{eq:extr2}
        f(\hat{x}) < f(x) \implies S - \varepsilon < f(\hat{x}) < f(x) \;\;\; \forall \varepsilon > 0
    \end{equation}
    Ora combinando le informazioni (\ref{eq:extr1}) e (\ref{eq:extr2}) sappiamo che
    \[
    S-\varepsilon < f(x) < S \;\;\; \forall \varepsilon > 0 \;\; \forall x \in A \cap I \cap (-\infty, x_0)
    \]
    Visto che $\varepsilon >0$ sappiamo che $S < S+ \varepsilon$ e quindi 
    \[
    S-\varepsilon < f(x) < S < S+ \varepsilon
    \]
    \[
    S-\varepsilon < f(x) < S + \varepsilon \;\;\; \forall \varepsilon > 0 \;\; \forall x \in A \cap I \cap (-\infty, x_0)
    \]
    E questa non è altro che la definizione di limite
    \[
        \lim_{x\to x_0^-} f(x) = \sup\{f(A \cap I \cap (-\infty, x_0))\}
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to x_0} a^x = a^{x_0}
    \]
\end{esercizio}

\begin{proof}
    Questo è vero proprio perchè se $a>1$ la funzione $a^x$ è monotona crescente, se $a=1$ è costante e invece se $a<1$ la funzione è monotona decrescente, quindi si può sempre applicare il teorema.
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to x_0} \log_ax = \log_ax_0 \;\; \;\; \; \forall x_0 > 0
    \]
\end{esercizio}

\begin{proof}
    Possiamo fare lo stesso ragionamento del per il logaritmo, infatti se $a>1$ la funzione $\log_ax$ è monotona crescente, se $a=1$ è costante e invece se $a<1$ la funzione è monotona decrescente, quindi si può sempre applicare il teorema. L'unica cosa che cambia dall'esercizio precedente è che $x_0$ deve essere positivo, perche il dominio di $\log_ax$ è $\forall x > 0$, e di conseguenza qualsiasi punto $x_0 < 0$ non è punto di accumulazione e pertanto non può essere calcolato il limite in quel punto.
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to x_0} x^\alpha = x_0^\alpha \;\; \;\; \; \forall x_0 \ne 0 \;\; \forall \alpha \in \mathbb{R}
    \]
\end{esercizio}

\begin{proof}
    Se $\alpha > 0$ avremo una potenza che è sempre monotona crescente per $x_0 > 0$, mentre se $\alpha$ è pari allora la funzione sarà decrescente per $x_0 < 0$ mentre se $\alpha$ è dispari la funzione è crescente anche per $x < 0 $ . Se $\alpha = 0$ allora avremo una funzione costante e se $\alpha<0$ la funzione sarà del tipo $\frac{1}{x^\alpha}$ che sarà monotona decrescente.     
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to 0^+} x^\alpha = \begin{cases}
            0 & \text{se } \alpha > 0 \\
            +\infty & \text{se } \alpha < 0 \\
        \end{cases}
    \]
\end{esercizio}

\begin{proof}
    Questo è un caso particolare dell'esercizio precedente, infatti se $x\to 0^+$ allora con $\alpha>0$ avremo una forma del tipo $0^\alpha$ che chiaramente tende a 0, mentre se $\alpha < 0$ la funzione diventa $\frac{1}{x^{|\alpha|}}$ che fa tendere il denominatore a $0^+$ e che quindi fa tendere la funzione a $+\infty$.    
\end{proof}


\addcontentsline{toc}{subsection}{Limite di funzioni Monotone caso Infinito}
\begin{teorema}{Limite di funzioni Monotone caso Infinito}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $I\subseteq \mathbb{R}$ intorno di $\pm \infty $ tale che $f(x)$ è monotona in $I$

    \begin{itemize}
        \item Se $f(x)$ è monotona crescente allora 
        \[
        \lim_{x\to +\infty} f(x) = \sup\{f(A \cap I)\}
        \]
        \[
        \lim_{x\to -\infty} f(x) = \inf\{f(A \cap I )\}
        \]
        \item Se $f(x)$ è monotona decrescente allora 
        \[
        \lim_{x\to +\infty} f(x) = \inf\{f(A \cap I )\}
        \]
        \[
        \lim_{x\to -\infty} f(x) = \sup\{f(A \cap I)\}
        \]
    \end{itemize}
\end{teorema}

\begin{esercizio}{}{}
    \[
        \lim_{x\to +\infty} a^x = \begin{cases}
            +\infty & \text{se } a > 1 \\
            1 & \text{se } a = 1 \\
            0 & \text{se } 0< a < 1 \\
        \end{cases}
    \]
\end{esercizio}

\begin{esercizio}{}{}
    \[
        \lim_{x\to -\infty} a^x = \begin{cases}
            0 & \text{se } a > 1 \\
            1 & \text{se } a = 1 \\
            +\infty & \text{se } 0< a < 1 \\
        \end{cases}
    \]
\end{esercizio}

\begin{esercizio}{}{}
    \[
        \lim_{x\to +\infty} log_a(x) = \begin{cases}
            +\infty & \text{se } a > 1 \\
            -\infty & \text{se } 0< a < 1 \\
        \end{cases}
    \]
\end{esercizio}

\begin{esercizio}{}{}
    \[
        \lim_{x\to \pm\infty} |x|^\alpha = \begin{cases}
            +\infty & \text{se } \alpha > 0 \\
            1 & \text{se } \alpha = 0 \\
            0 & \text{se } \alpha < 0 \\
        \end{cases}
    \]
\end{esercizio}



\newpage
\begin{teorema}{Potenza di Funzioni}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$ e $f,g:A\to \mathbb{R}$, $x_0$ punto di accumulazione in $A$

    \[
        \lim_{x\to x_0} f(x) ^ {g(x)} 
    \]
\end{teorema}

\begin{proof}
    Per calcolare questo limite possiamo usare la continuità dell'esponenziale. Perchè il limite lo possiamo calcolare come
    \[
        f(x) ^ {g(x) }  = e ^ {\log\bigl({f(x) ^ {g(x)}}\bigr)} = e ^ {g(x) \cdot \log({f(x)}) }
    \]

    ora con il cambio di variabile possiamo fare 
    \[
    \lim_{x\to x_0} g(x) \cdot \log({f(x)}) = y_0
    \]
    \[
        \lim_{x\to x_0} f(x) ^ {g(x)} = \lim_{x\to x_0} e ^ {\log\bigl({f(x) ^ {g(x)}}\bigr)} = \lim_{y\to y_0} e^y = e^{y_0}
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to +\infty} x ^ {\frac{1}{\log(x+1)}} 
    \]
\end{esercizio}

\begin{proof}
    Usiamo il ragionamento dell'esercizio precedente, con il caso $f(x) = x$, $g(x) = \frac{1}{log(x+1)}$
    \[
        x ^ {\frac{1}{\log(x)}} = e ^ {\log\bigl(x ^ {\frac{1}{\log(x+1)}}\bigr)}= e ^ {\frac{1}{\log(x+1)} \cdot \log(x)} =e ^ {\frac{\log(x)}{\log(x+1)} } 
    \]
    ora calcoliamo il limite dell'esponente
    \[
        \lim_{x\to +\infty} = \frac{log(x)}{log(x+1)}
    \]
    Questo limite è della forma $\bigl[\frac{\infty}{\infty}\bigr]$ e pertanto proviamo a raggruppare come nei polinomio
    \[
        \lim_{x\to +\infty} = \frac{\log(x)}{\log(x+1)}  
        = \lim_{x\to +\infty}  \frac{\log(x)}{\log(x\bigl(1+\frac{1}{x}\bigr))} 
    \]
    \[
         = \lim_{x\to +\infty}\frac{\log(x)}{\log (x) + \log\bigl(1+\frac{1}{x}\bigr)} 
         = \lim_{x\to +\infty}\frac{1}{1 + \frac{\log\bigl(1+\frac{1}{x}\bigr)}{\log(x)}}
    \]
    Ora il termine $\log\bigl(1+\frac{1}{x}\bigr)$ tende a 0, invece $\log(x)$ tende a $+\infty$, quindi complessivamente la frazione tende a 0 e quindi possiamo calcolare il limite e sostituirlo  
    \[
    \lim_{x\to +\infty}\frac{1}{1 + \circled[red]{\frac{\log\bigl(1+\frac{1}{x}\bigr)}{\log(x)}}} = \frac{1}{1+0} = 1 \implies \lim_{y\to 1} e^y = e^1 = e
    \]
\end{proof}


\begin{definizione}{Numero di Nepero ($e$)}{}
    \[
        e := \lim_{x\to +\infty} \left(1 + \frac{1}{x}\right)^{x}
    \]
\end{definizione}
\begin{esercizio}{}{}
    \[
    \lim_{x\to -\infty} \left(1 + \frac{1}{x}\right)^{x}
    \]
\end{esercizio}
\begin{proof}
    Per vedere come tende la funzione a $-\infty$ possiamo provare usando il cambio di variabile con $y=-x$ per provare a ricondurci alla definizione del numero di Nepero
    \[
        \lim_{x\to -\infty} \left(1 + \frac{1}{x}\right)^{x} = \lim_{y\to +\infty} \left(1 + \frac{1}{-y}\right)^{-y}  
    \]
    Ora racciamo qualche riarrangiamento 
\[
        \lim_{y\to +\infty} \left(1 + \frac{1}{-y}\right)^{-y}  =  \lim_{y\to +\infty} \left(\frac{-y + 1}{-y}\right)^{-y} =   \lim_{y\to +\infty} \left(\frac{y - 1}{y}\right)^{-y} = \lim_{y\to +\infty} \left(\frac{y}{y-1}\right)^{y}
    \]
    Il denominatore $y-1$ è molto scomodo, quindi proviamo a sostituirlo con $z=y-1$
    \[
\lim_{y\to +\infty} \left(\frac{y}{y-1}\right)^{y} = \lim_{z\to +\infty} \left(\frac{z+1}{z}\right)^{z+1} = \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z+1}
    \]
    Per sistemare l'esponente basta usare la proprietà degli esponenti e l'algebra dei limiti per il prodotto
    \[
        \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z+1} = \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z} \cdot \left(1 + \frac{1}{z}\right) = \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z} \cdot \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)
    \]
    Il primo limite tende a $e$ per la definizione di numero di Nepero, mentre nel secondo limite il termine ($\frac{1}{z}$) tende a 0 e quindi complessivamente il limite tende a 1
    \[
    \lim_{z\to +\infty} \left(1 + \frac{1}{z}\right)^{z} \cdot \lim_{z\to +\infty} \left(1 + \circled[red]{\frac{1}{z}}\right) = e \cdot 1 = e
    \]
    Quindi notiamo che il limite tende ad $e$ anche per $x\to-\infty$, pertanto possiamo modificare la definizione con
    \[
    e = \lim_{x\to \pm\infty} \left(1 + \frac{1}{x}\right)^{x}
    \]
\end{proof}
\newpage

\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} = (1+x)^{\frac{1}{x}}
    \]
\end{esercizio}

\begin{proof}
    Per risolvere questo limite dobbiamo usare un cambio di variabile con $y = \frac{1}{x}$ però dobbiamo stare attenti infatti per valori di $x\to 0^+ \implies y\to +\infty$ mentre $x\to 0^- \implies y\to -\infty$ quindi dobbiamo studiare in due casi separati. Indichiamo con $(i)$ per il caso $y\to+\infty$  e $(ii)$ per il caso $y\to-\infty$ 
    \[
        (i) \;\;\; \lim_{x\to 0^+} = (1+x)^{\frac{1}{x}} = \lim_{y\to +\infty} = \left(1+\frac{1}{y}\right)^{y} = e
    \] 
     \[
        (ii) \;\;\; \lim_{x\to 0^-} = (1+x)^{\frac{1}{x}} = \lim_{y\to -\infty} = \left(1+\frac{1}{y}\right)^{y} = e
    \]
    
    Vediamo che nonostante abbiamo dovuto dividere in due casistiche separate il limite tende allo stesso valore, e che quindi per il teorema della relazione tra limite e limite destro/sinistro sappiamo che
    \[
    \lim_{x\to 0} = (1+x)^{\frac{1}{x}} = e
    \]
\end{proof}

\addcontentsline{toc}{subsection}{Limiti Notevoli}
\begin{definizione}{Limiti Notevoli}{}
    Si definiscono Limiti Notevoli tutti i limiti della seguente forma. (Le dimostrazione le vediamo subito dopo)
    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $\displaystyle\lim_{x\to 0} \frac{\sin(x)}{x} = 1$ (Già dimostrato)
        \item $\displaystyle\lim_{x\to 0} \frac{1-\cos(x)}{x^2} = \frac{1}{2}$
        \item $\displaystyle\lim_{x\to 0} \frac{\tan(x)}{x} = 1$
        \item $\displaystyle\lim_{x\to +\infty} \left(1 + \frac{\alpha}{x}\right)^{x} = e^\alpha$  $\forall \alpha \in \mathbb{R}$
        \item $\displaystyle\lim_{x\to 0} \frac{\log(1+x)}{x} = 1$
        \item $\displaystyle\lim_{x\to 0} \frac{e^x-1}{x} = 1$ 
    \end{enumerate}
\end{definizione}
\newpage
\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{1-\cos(x)}{x^2} = \frac{1}{2}
    \]
\end{esercizio}
\begin{proof}
    Si vede subito che è una forma $\bigl[\frac{0}{0}\bigr]$ e però non sembra riconducibile a nessun limite tra quelli che abbiamo visto, però proviamo a a "trasformare" il coseno in seno , visto che del seno sappiamo un limite notevole $(i)$. Per farlo dobbiamo ricordarci la formula fondamentale della trigonometria: $\cos^2(x) + \sin^2(x) = 1$
    \begin{align*}
    \lim_{x\to 0} \frac{1-\cos(x)}{x^2} &= \lim_{x\to 0} \frac{1-\cos(x)}{x^2} \cdot \frac{1+\cos(x)}{1+\cos(x)} = \lim_{x\to 0} \frac{1-\cos^2(x)}{x^2} \cdot \frac{1}{1+\cos(x)} \\
    &= \lim_{x\to 0} \frac{\sin^2(x)}{x^2} \cdot \frac{1}{1+\cos(x)} = \lim_{x\to 0} \left(\frac{\sin(x)}{x}\right)^2 \cdot \frac{1}{1+\cos(x)}
    \end{align*}
    Ora il primo termine, visto che è il limite notevole $(i)$, tende a 1, mentre il secondo tende a $\frac{1}{2}$
    \[
    \lim_{x\to 0} \left(\frac{\sin(x)}{x}\right)^2 \cdot \frac{1}{1+\cos(x)} = (1)^2 \cdot \frac{1}{1+1} = \frac{1}{2}
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
    \frac{\tan(x)}{x} = 1
    \]
\end{esercizio}
\begin{proof}
    Questo è molto semplice infatti basta usare la definizione di tangente ($\tan(x) = \frac{\sin(x)}{\cos(x)}$)
    \[
    \lim_{x\to 0}\frac{\tan(x)}{x} = \lim_{x\to 0}\frac{\frac{\sin(x)}{\cos(x)}}{x} = \lim_{x\to 0}\frac{\sin(x)}{x} \cdot \frac{1}{\cos(x)} = 1\cdot \frac{1}{1} = 1
    \]
\end{proof}

\begin{esercizio}{}{}
 \[
 \lim_{x\to +\infty} \left(1 + \frac{\alpha}{x}\right)^{x}
 \]   
\end{esercizio}

\begin{proof}
    Questo chiaramente assomiglia molto alla definizione di $e$, soltanto che c'è un $\alpha$ di troppo. Possiamo provare a sostituire ma notiamo una cosa, infatti se vogliamo sostituire $\alpha y = x$ dobbiamo distinguere i casi $\alpha > 0$, $\alpha = 0$, $\alpha < 0$.  Studiamo i singoli casi e numeriamoli rispettivamente $(i)$,$(ii)$,$(iii)$
    \[
    (i) \;\;\; \lim_{x\to +\infty} \left(1 + \frac{\alpha}{x}\right)^{x} = \lim_{y\to +\infty} \left(1 + \frac{1}{y}\right)^{\alpha y} = \lim_{y\to +\infty} \left(\left(1 + \frac{1}{y}\right)^{y}\right)^\alpha = (e)^\alpha = e^\alpha
    \]

    \[
    (ii) \;\;\; \lim_{x\to +\infty} \left(1 + \frac{0}{x}\right)^{x} =  \lim_{x\to +\infty} 1 ^{x} = 1 \;\;\; [=e^0]
    \]

    \[
    (iii) \;\;\; \lim_{x\to +\infty} \left(1 + \frac{\alpha}{x}\right)^{x} = \lim_{y\to -\infty} \left(1 + \frac{1}{y}\right)^{\alpha y} = \lim_{y\to -\infty} \left(\left(1 + \frac{1}{y}\right)^{y}\right)^\alpha = (e)^\alpha = e^\alpha
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{\log(1+x)}{x} = 1
    \]
\end{esercizio}

\begin{proof}
    Per risolvere questo basta usare le proprietà dei logaritmi
    \[
\lim_{x\to 0} \frac{\log(1+x)}{x} = \lim_{x\to 0} \frac{1}{x} \cdot \log(1+x) = \lim_{x\to 0} \log\left((1+x)^{\frac{1}{x}}\right) = \log(e) = 1
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} \frac{e^x-1}{x} = 1
    \]
\end{esercizio}

\begin{proof}
    Questo invece è un pò più complicato, infatti non abbiamo visto limiti di questo. Però possiamo provare con una sostituzione $y = \log(x)$ e vediamo che succede, ricordandoci che se $x\to0$ allora $y\to -\infty$
    \[
        \lim_{x\to 0} \frac{e^x-1}{x} = \lim_{y\to -\infty} \frac{e^{\log(y)}-1}{\log(y)} = \lim_{y\to -\infty} \frac{y-1}{\log(y)}
    \]
    Ora assomiglia al limite dell'esercizio precedente, soltanto che all'interno dell'logaritmo abbiamo $y$ e non $y+1$, quindi per farlo "sbucare" fuori possiamo fare un'altra sostituzione $z=y+1$
    \[
    \lim_{y\to -\infty} \frac{y-1}{\log(y)} = \lim_{z\to -\infty} \frac{z}{\log(z+1)}
    \] 
    Adesso il limite è riconducibile a limite notevole $(v)$
    \[
    \lim_{z\to -\infty} \frac{z}{\log(z+1)} = \lim_{z\to -\infty} \frac{1}{\frac{\log(z+1)}{z}} = \frac{1}{1} = 1
    \]
\end{proof}

\newpage

\addcontentsline{toc}{subsection}{Definizione di Funzioni Asintotiche}
\begin{definizione}{Funzioni Asintotiche}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di accumulazione in $A$ e se

    \begin{itemize}
        \centering
        \item $f(x)\ne 0$, $g(x) \ne 0$ definitivamente per $x\to x_0$
        \item $\displaystyle \exists \lim_{x\to x_0} \frac{f(x)}{g(x)} = 1$
    \end{itemize}

    Allora diciamo che "$f(x)$ è asintotica per $x\to x_0$ a $g(x)$" e lo indichiamo con il simbolo
    \[
        f(x) \sim g(x) \;\;\;\;\; x\to x_0
    \]
\end{definizione}

\begin{esempio}{}{}
    \[
        \sin(x) \sim x \;\;\;\;\; x\to 0
    \]
\end{esempio}

\begin{proof}
    Dai limiti notevoli sappiamo che $\displaystyle\lim_{x\to 0} \frac{\sin(x)}{x}$, sappiamo inoltre che $\sin(x) \ne 0$ definitivamente per $x\to 0$ e lo stesso vale per $x \ne 0$. Pertanto possiamo dire che 
    \[
        \sin(x) \sim x \;\;\;\;\; x\to 0
    \] 

    \textbf{N.B.} questo ragionamento lo possiamo fare per tutti i limiti notevoli, quindi sarà vero anche $\log(1+x) \sim x$ per $x\to 0$, $\tan(x) \sim x$ per $x\to 0$, $e^x -1 \sim x$ per $x\to 0$ (che lo possiamo scrivere anche come $e^x \sim 1+ x$).  
\end{proof}

\begin{esempio}{}{}
    \[
        \cos(x) \sim 1-\frac{x^2}{2} \;\;\;\;\; x\to 0
    \]
\end{esempio}

\begin{proof}
    Per il limite notevole del coseno dobbiamo fare qualche ragionameto in più, infatti il limite fa come risultato $\frac{1}{2}$ e non 1, quindi non possiamo dire nulla sull'asintoticità, ma possiamo fare qualche mageggio, infatti
    \[
        \lim_{x\to 0} \frac{1-\cos(x)}{x^2} = \frac{1}{2} \implies \lim_{x\to 0} \frac{1-\cos(x)}{\frac{x^2}{2}} = 1
    \]
    Dopo questo riarrangiamento possiamo dire che 
    \[
    1 - \cos(x) \sim \frac{x^2}{2} \;\;\;\;\; x \to 0
    \]
    Che possiamo riscrivere come
    \[
    \cos(x) \sim 1-\frac{x^2}{2} \;\;\;\;\; x\to 0
    \]
\end{proof}
\newpage

\addcontentsline{toc}{subsection}{Teorema delle Proprità delle Funzioni Asintotiche}
\begin{teorema}{Proprietà delle Funzioni Asintotiche}{}
     Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g, h, \hat{f}, \hat{g}:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ 


     $(i)$ se $f \sim g$ allora è vera una delle due proposizioni
    \begin{itemize}
        \item $f$ e $g$ hanno entrambe limite in $x_0$
        \item $f$ e $g$ entrambe non hanno limite in $x_0$
    \end{itemize}
    $(ii)$ se $f \sim g$ e $h \sim g$ per $x\to x_0$ allora è vero che $f \sim h$ per $x\to x_0$
\vspace{0.2cm}

    $(iii)$ se $f \sim \hat{f}$ per $x\to x_0$, $g \sim \hat{g}$ per $x\to x_0$ allora sono vere entrambe le equivalenze:
\[
\begin{array}{c  @{\qquad\qquad}  c}
f\cdot g \sim \hat{f}\cdot \hat{g} 
&
\dfrac{f}{g} \sim \dfrac{\hat{f}}{\hat{g}}
\end{array}
\]
\end{teorema}

\begin{proof}
    Segue la dimostrazione del punto $(ii)$ e $(iii)$.

    $(ii)$ Noi sappiamo che $f \sim g$ e che $g \sim h$ ma vogliamo vedere se è vero  che $f \sim h$, e se fosse vera quest'ultima equivalenza allora dovrebbe essere vero che 
    \[
        f \sim h \iff \lim_{x\to x_0} \frac{f(x)}{h(x)} = 1
    \]
    Quindi proviamo a vedere se il limite fa 1, e per farlo dividiamo e moltiplichiamo per $g(x)$
    \[
    \lim_{x\to x_0} \frac{f(x)}{h(x)} = \lim_{x\to x_0} \frac{f(x)}{h(x)} \cdot \frac{g(x)}{g(x)}  =  \lim_{x\to x_0} \frac{f(x)}{g(x)} \cdot \frac{g(x)}{h(x)}=  \lim_{x\to x_0}  \frac{f(x)}{g(x)} \cdot \frac{1}{\frac{h(x)}{g(x)}} 
    \]
    Ma visto che per ipotesi $f \sim g$ e $g \sim h$, allora i rapporti varranno 1, e pertanto il teorema è verificato
    \[
        \lim_{x\to x_0}  \frac{f(x)}{g(x)} \cdot \frac{1}{\frac{h(x)}{g(x)}}  = 1 \cdot \frac{1}{1} = 1
    \]

    $(iii)$ Riscriviamo $f\cdot g \sim \hat{f}\cdot \hat{g}$ usando la definizione di funzione asintotica
    \[
        f(x)\cdot g(x) \sim \hat{f}(x)\cdot \hat{g}(x) \iff \lim_{x\to x_0} \frac{f(x)\cdot g(x)}{\hat{f}(x)\cdot \hat{g}(x)} = 1
    \]
    Quindi per dimostrare il teorema basta controllare che il limite faccia 1, ma è molto semplice infatti se spezziamo la frazione e grazie alle ipotesi ($f \sim \hat{f}$ e $g \sim \hat{g}$) allora 
    \[
    \lim_{x\to x_0} \frac{f(x)\cdot g(x)}{\hat{f}(x)\cdot \hat{g}(x)} = \lim_{x\to x_0} \frac{f(x)}{\hat{f}(x)} \cdot \frac{g(x)}{\hat{g}(x)}  = \lim_{x\to x_0} \frac{f(x)}{\hat{f}(x)} \cdot  \lim_{x\to x_0}  \frac{g(x)}{\hat{g}(x)}  = 1 \cdot 1 = 1
    \]
    Il ragionamento è analogo per $\dfrac{f}{g} \sim \dfrac{\hat{f}}{\hat{g}}$.
\end{proof}


\addcontentsline{toc}{subsection}{Gerarchia degli Infiniti}
\begin{teorema}{Gerarchia degli Infiniti}{}
    Siano $a > 1$, $\alpha > 0$, $\beta >0$ allora sono vere
    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $\displaystyle\lim_{x\to +\infty} \frac{x^\alpha}{a^x} = 0 $
        \item $\displaystyle\lim_{x\to +\infty} \frac{(\log_{a}x)^\beta}{x^\alpha} = 0 $
    \end{enumerate}

\end{teorema}

\begin{esempio}{}{}
    \[
        \lim_{x\to 0^+} x \log(x)
    \]
\end{esempio}

\begin{proof}
    Si può notare come sia un limite nella forma $\bigl[0\cdot\infty\bigr]$ e quindi dobbiamo fare degli riarrangiamenti. Partimo portando il termine $x$ a denominatore.
    \[
     \lim_{x\to 0^+} x \log(x) =  \lim_{x\to 0^+} \frac{\log(x)}{\frac{1}{x}}
    \]
    Ora possiamo fare una sostituzione con $y = \frac{1}{x}$, e quindi se $x\to 0^+$ allora $y\to +\infty$
    \[
\lim_{x\to 0^+} \frac{\log(x)}{\frac{1}{x}} = \lim_{y\to +\infty} \frac{\log(\frac{1}{y})}{y}
    \]
    Usando le proprietà dei logaritmi abbiamo 
    \[
    \lim_{y\to +\infty} \frac{\log(\frac{1}{y})}{y} = \lim_{y\to +\infty} \frac{\log(y^{-1})}{y} = \lim_{y\to +\infty} -\frac{\log(y)}{y}
    \]
    E che quindi con la gerarchia degli Infiniti
    \[
    \lim_{y\to +\infty} -\frac{\log(y)}{y} = 0
    \]
\end{proof}

\begin{esempio}{}{}
    \[
        \lim_{x\to 0^+} x^x
    \]
\end{esempio}

\begin{proof}
    Per svolgere questo limite possiamo usare la regola delle potenze di funzioni per riarrangiarla e possiamo usare il limite dell'esempio precedente per calcolarlo
    \[
        \lim_{x\to 0^+}x^x =\lim_{x\to 0^+} e^{\log(x^x)} = \lim_{x\to 0^+}e^{x\log(x)} = e^0 = 1
    \]
\end{proof}

\section{Simboli di Landau}

\addcontentsline{toc}{subsection}{Definizione di o-piccolo}
\begin{definizione}{o-piccolo}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$  e se 
    \[
        \lim_{x\to x_0} \frac{f(x)}{g(x)} = 0
    \]

    Allora diciamo che "$f(x)$ è un o-piccolo di $g(x)$ per $x\to x_0$" e lo indichiamo con il simbolo
    \[
        f(x) = o(g(x))\;\;\;\;\; x\to x_0
    \]
\end{definizione}
Ora seguiranno alcuni esempi per familiarizzare con il simbolo 

\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{x^3}{x} = 0 \iff x^3 = o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}
\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{\sin^2(x)}{x} = 0 \iff \sin^2(x) = o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}

\textbf{N.B.} anche se $\sin^2(x) = o(x)$ e $x^3 = o(x)$ per $x\to 0$ \textbf{NON} possiamo dire che $sin^2(x) = x^2$. Anche perche $sin^2(x)$ e $x^2$ sono due cose separate. Infatti con la nozione di o-piccolo sappiamo che una funzione è più grande di un'altra, ma se due funzioni sono entrambe o-piccolo di una funzione, non possiamo dire nulla delle due funzioni.
\begin{esempio}{}{}
    \[
        \lim_{x\to +\infty} \frac{x}{x^2} = 0 \iff x = o(x^2) \;\;\;\;\; x\to+\infty
    \]
\end{esempio}
\begin{esempio}{}{}
    \[
        \lim_{x\to 0} x = 0 \iff x = o(1) \;\;\;\;\; x\to 0
    \]
\end{esempio}

Questo perchè possiamo riscrivere $x$ come $\frac{x}{1}$ e per questo possiamo scrivere $x = o(1)$. Difatto in generale se $\displaystyle\lim_{x\to x_0}f(x) = 0$ allora possiamo dire che $f(x) = o(1)$ per $x\to x_0$.
\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{x^3}{x^2} = 0 \iff x^3 = o(x^2) \;\;\;\;\; x\to 0
    \]
\end{esempio}

\newpage

\addcontentsline{toc}{subsection}{Proprietà degli o-piccoli \textit{I}}
\begin{teorema}{Proprietà o-piccoli}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$  allora 

    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $\displaystyle\lim_{x\to x_0} \frac{o(f(x))}{f(x)} = 0$
        \item $o(f(x)) \pm o(f(x)) = o(f(x)) \;\;\;\;\; x\to x_0$
        \item $o(f(x)) \cdot o(g(x)) = o(f(x)\cdot g(x)) \;\;\;\;\; x\to x_0$
        \item $|o(f(x))|^\alpha = o(|f(x)|^\alpha) \;\;\;\;\; x\to x_0 \;\;\; \forall \alpha > 0$
    \end{enumerate}
    Se $g(x)\ne 0$  definitivamente per $x\to x_0$, allora 

    

    \begin{enumerate}[label=(\roman*)]
        \setcounter{enumi}{4}
        \centering
        \item $o(f(x)) \cdot g(x) = o(f(x) \cdot g(x)) \;\;\;\;\; x\to x_0$
    \end{enumerate}
    Se, oltre a $g(x)\ne 0$, è vero che $|g(x)|$ è limitata definitivamente allora vale

    

    \begin{enumerate}[label=(\roman*)]
        \setcounter{enumi}{5}
        \centering
        \item $o(f(x)) \cdot g(x) = o(f(x)) \;\;\;\;\; x\to x_0$
    \end{enumerate}

    
\end{teorema}

\begin{proof}
    ($i$) sia una qualsiasi funzione allora $g(x) = o(f(x))$, allora per definizione di o-piccolo sappiamo che 
    \[
        g(x) = o(f(x)) \iff \lim_{x\to x_0}    \frac{g(x)}{f(x)} = 0
    \]  
    Ora però possiamo sostituire $g(x)$ con $o(f(x))$ visto che è vera per ipotesi
    \[
    \lim_{x\to x_0}    \frac{g(x)}{f(x)} = 0 \implies \lim_{x\to x_0}    \frac{o(f(x))}{f(x)} = 0
    \]
    E con questo abbiamo verificato il primo teorema.

    $(ii)$ Sia $g_1(x) = o(f(x))$ e $g_2(x) = o(f(x))$ allora proviamo a vedere se è vero il teorema, quindi usiamo la definizione di o-piccolo
    \[
    g_1(x) \pm g_2(x) = o(f(x)) \iff \lim_{x\to x_0} \frac{g_1(x) \pm g_2(x)}{f(x)} = \lim_{x\to x_0} \frac{g_1(x)}{f(x)}\pm \frac{g_2(x)}{f(x)} 
    \]
    Ora visto che $g_1(x) = o(f(x))$ e $g_2(x) = o(f(x))$ allora se vengono divise per $f(x)$ tenderanno a 0 e quindi
    \[
    \lim_{x\to x_0} \circled[red]{\frac{g_1(x)}{f(x)}}\pm \circled[red]{\frac{g_2(x)}{f(x)}} = 0 \pm 0 = 0
    \]
    E quindi il teorema è verificato visto che è venuto proprio 0. 
    
    \textbf{N.B.} anche $o(f(x)) - o(f(x)) = o(f(x))$  e NON si eliminano gli o-piccoli.
\newpage

    ($iii$) Come nel punto $(ii)$, basta che controlliamo se è vero usando la definizione di o-piccolo
    \[
        o(f(x)) \cdot o(g(x)) = o(f(x)\cdot g(x)) \iff \lim_{x\to x_0} \frac{o(f(x)) \cdot o(g(x)) }{f(x)\cdot g(x)}
    \]
    Ora possiamo fare degli riarrangiamenti 
    \[
    \lim_{x\to x_0} \frac{o(f(x)) \cdot o(g(x)) }{f(x)\cdot g(x)} = \lim_{x\to x_0} \frac{o(f(x))}{f(x)} \cdot \frac{o(g(x)) }{g(x)}
    \]
    Ora usiamo la proprietà $(i)$ 
    \[
    \lim_{x\to x_0} \frac{o(f(x))}{f(x)} \cdot \frac{o(g(x)) }{g(x)} = 0\cdot 0 = 0
    \]
    E quindi il teorema è verificato.

    $(iv)$  Usiamo la definizione di o-piccolo
    \[
    |o(f(x))|^\alpha = o(|f(x)|^\alpha) \iff \lim_{x\to x_0} \frac{|o(f(x))|^\alpha}{|f(x)|^\alpha}
    \]
    Usiamo le proprietà dei moduli e delle potenze e la proprietà $(i)$
    \[
    \lim_{x\to x_0} \frac{|o(f(x))|^\alpha}{|f(x)|^\alpha} = \lim_{x\to x_0} \left|\frac{o(f(x))}{f(x)}\right|^\alpha = 0 ^ \alpha = 0
    \]
    L'ultimo passaggio è valido perchè abbiamo definito $\alpha > 0$ e quindi non reca alcun problema l'esponente. Il modulo serve solo per poter farlo anche di radici, così è valido anche per radici negative.


    $(v)$ Usiamo la definizione di o-piccolo
    \[
     o(f(x)) \cdot g(x) = o(f(x)\cdot g(x)) \iff \lim_{x\to x_0} \frac{o(f(x)) \cdot g(x) }{f(x)\cdot g(x)}
    \]
    Notiamo che il termine $g(x)$ si può semplificare e poi possiamo usare la proprietà $(i)$
    \[
    \lim_{x\to x_0} \frac{o(f(x)) \cdot g(x) }{f(x)\cdot g(x)} = \lim_{x\to x_0} \frac{o(f(x))}{f(x)} = 0
    \]

    $(vi)$ Usiamo la definizione di o-piccolo
    \[
     o(f(x)) \cdot g(x) = o(f(x)) \iff \lim_{x\to x_0} \frac{o(f(x)) \cdot g(x) }{f(x)} = \lim_{x\to x_0} \frac{o(f(x)) }{f(x)} \cdot g(x) 
    \]
    Ora notiamo che il termine $\frac{o(f(x)) }{f(x)}$ tende a 0 per proprietà $(i)$, quindi l'unico caso a cui dobbiamo stare attenti è quando $h(x)$ tende a $\infty$, perchè qualora fosse avremmo una forma indeterminata $\big[0\cdot \infty\big]$, però per ipotesi noi sappiamo che $|g(x)|$ è limitata, allora possiamo usare la proprietà $(ii)$ dell'algebra dei limiti finiti per scoprire che tende a 0, grazie al fatto che $|g(x)|$ è limitata
    \[
    \lim_{x\to x_0} \frac{o(f(x)) }{f(x)} \cdot g(x)  = 0
    \]
    Pertanto il teorema è verificato.
\end{proof}

\addcontentsline{toc}{subsection}{Relazione tra o-piccolo e Asintoticità}
\begin{teorema}{Relazione tra o-piccolo e Asintoticità}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ e $g(x)\ne 0$ definitivamente per $x\to x_0$

    \[
        f(x) \sim g(x) \;\;\; x\to x_0 \implies f(x) = g(x) + o(g(x))  \;\;\; x\to x_0
    \]
\end{teorema}
\begin{proof}
    Usando la definizione di funzione asintotica sappiamo che 
    \[
    f(x) \sim g(x) \iff  \lim_{x\to x_0} \frac{f(x)}{g(x)} = 1
    \]
    ora possiamo portare 1 dall'altra parte e facciamo denominatore comune 
    \[
    \lim_{x\to x_0} \frac{f(x)}{g(x)} = 1 \implies \lim_{x\to x_0} \big[\frac{f(x)}{g(x)} - 1\big]= 0 \implies \lim_{x\to x_0} \frac{f(x)-g(x)}{g(x)} = 0 
    \]
    A questo punto possiamo usare la definizione di o-piccolo, visto che abbiamo un limite che tende a 0
    \[
        \lim_{x\to x_0} \frac{f(x)-g(x)}{g( x)}= 0 \iff f(x)-g(x) = o(g(x))
    \]
    Facendo qualche riarrangiamento abbiamo 
    \[
    f(x)-g(x) = o(g(x)) \implies f(x) = g(x) + o(g(x)) \;\;\; x\to x_0
    \]
\end{proof}

Ora vediamo l'applicazione di questo teorema sui limiti notevoli
\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{\sin(x)}{x} = 1 \implies \sin(x) = x + o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}

Per il limite del coseno usiamo lo stesso trick che abbiamo usato l'ulima volta, ovvero di dividere per $\frac{1}{2}$ in modo che ora il limite tenda a 1

\begin{esempio}{}{}
    \[
        \lim_{x\to x_0} \frac{1-\cos(x)}{\frac{1}{2}x^2} = 1 \implies 1-\cos(x) = \frac{1}{2}x^2 + o(x^2) \implies \cos(x) = 1 -\frac{1}{2}x^2 + o(x^2) 
    \]
\end{esempio}

\text{N.B.} non ho scritto $o(\frac{1}{2}x^2)$ perchè ho usato la proprietà $(vi)$ delle proprietà degli o-piccolo, e lo abbiamo potuto applicare perchè la funzione $g(x)= \frac{1}{2}$ è sempre limitata.

\begin{esempio}{}{}
    \[
        \lim_{x\to 0} \frac{\tan(x)}{x} = 1 \implies \tan(x) = x + o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}


\begin{esempio}{}{}
    \[
        \lim_{x\to x_0} \frac{\log(1+x)}{x} = 1 \implies \log(1+x) = x + o(x) \;\;\;\;\; x\to 0
    \]
\end{esempio}

\begin{esempio}{}{}
    \[
        \lim_{x\to x_0} \frac{e^x-1}{x} = 1 \implies e^x -1  = x + o(x) \implies e^x  = 1 + x + o(x)  \;\;\;\;\; x\to 0
    \]
\end{esempio}


\addcontentsline{toc}{subsection}{Teorema del Cambio di variabile con o-piccolo}
\begin{teorema}{Cambio di variabile con o-piccolo}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g_1, g_2$ a valori reali tali che $g_1 \circ f,g_2 \circ f :A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ e supponiamo che
    \begin{itemize}
        \item $\displaystyle\lim_{x\to x_0} f(x) = y_0$
        \item $g_1(y) = g_2(y) + o(g_2(y))\;\;\;\;\; y\to y_0$
        \item $f(x) \ne y_0$ definitivamente per $x\to x_0$
    \end{itemize}

    Allora 
    \[
        g_1(f(x)) = g_2(f(x)) + o(g_2(f(x)))\;\;\;\;\; y\to y_0
    \]
\end{teorema}


\begin{esempio}{}{}
    Proviamo a fare qualche applicazione pratica di questo teorema, per esempio proviamo con $f(x) = x^2$, $g_1(y) = \sin(y)$ e $x_0=0$
\end{esempio}

In primis dobbiamo controllare che esista il limite di $f(x)$
\[
\lim_{x\to 0} x^2 = 0 \;\;\; [= y_0]
\]
Ora proviamo a sviluppare $g_1(y)$, e possiamo usare proprio lo sviluppo che abbiamo scoperto prima ($\sin(y) = y + o(y)$) per $y\to 0$, seguendo la notazione del teorema, $g_2(y) = y$. Ora basta controllare se $x^2 \ne 0$ definitivamente ed effettivamente lo è visto che $x^2>0$ $\forall x \ne 0$ e quindi possiamo applicare il teorema e scopriamo che 
\[
\sin(x^2) = x^2 + o(x^2) \;\;\;\;\; x\to 0
\]

\begin{esempio}{}{}
    Ora proviamo a fare un altro esempio con $f(x) = \sin(x)$, $g_1(y) = e^y$ e $x_0=0$, vediamo subito che 
    \[
    \lim_{x\to 0} \sin(x) = 0 \;\;\; [= y_0]
    \]
    Sappiamo in oltre lo svilutto di $e^y = 1+ y +o(y)$ per $y\to 0$, in oltre sappiamo che $\sin(x) \ne 0$ in un intorno di 0 e di conseguenza possiamo applicare il teorema e scopriamo che 
    \[
        e^{\sin(x)} = 1 + \sin(x) + o(\sin(x)) \;\;\;\;\; x\to 0
    \]
\end{esempio}


\addcontentsline{toc}{subsection}{Principio di Sostituzione}
\begin{teorema}{Principio di Sostituzione}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g, \hat{f}, \hat{g}:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ e $g(x)\ne 0$,$\hat{f}(x)\ne 0$,$\hat{g}(x)\ne 0$  definitivamente per $x\to x_0$ e se 
    \[
        f(x) = \hat{f}(x) + o(\hat{f}(x)) \;\;\;\;\; x\to x_0
    \]
     \[
        g(x) = \hat{g}(x) + o(\hat{g}(x)) \;\;\;\;\; x\to x_0
    \]
    Allora
    \[
        \lim_{x\to x_0} \frac{f(x)}{g(x)} = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)}  
    \]
\end{teorema}

\begin{proof}
    Per capire quanto vale $\displaystyle\lim_{x\to x_0} \frac{f(x)}{g(x)} $ basta che facciamo una sostituzione con le ipotesi
    \[
    \lim_{x\to x_0} \frac{f(x)}{g(x)}  = \lim_{x\to x_0} \frac{\hat{f}(x) + o(\hat{f}(x))}{\hat{g}(x) + o(\hat{g}(x)) } 
    \]
    Ora possiamo raccogliere a numeratore $\hat{f}(x)$ e a denominatore $\hat{g}(x)$
    \[
    \lim_{x\to x_0} \frac{\hat{f}(x) + o(\hat{f}(x))}{\hat{g}(x) + o(\hat{g}(x)) }  = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)} \cdot \frac{1 + \frac{o(\hat{f}(x))}{\hat{f}(x)}}{ 1+ \frac{o(\hat{g}(x))}{\hat{g}(x)} } 
    \]
    Ora possiamo dividere per l'algebra dei limiti e possiamo usare la proprietà $(i)$ degli o-piccoli
    \[
    \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)} \cdot \frac{1 + \frac{o(\hat{f}(x))}{\hat{f}(x)}}{ 1+ \frac{o(\hat{g}(x))}{\hat{g}(x)} }  = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)} \cdot \lim_{x\to x_0}\frac{1 + \circled[red]{\frac{o(\hat{f}(x))}{\hat{f}(x)}}}{ 1+ \circled[red]{\frac{o(\hat{g}(x))}{\hat{g}(x)}} } = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)} \cdot \frac{1 + 0}{ 1+0 } = \lim_{x\to x_0} \frac{\hat{f}(x)}{\hat{g}(x)}   
    \]
\end{proof}

Questo teorema è molto forte infatti vediamo un esercizio dove lo applichiamo

\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{1-\cos(x)}{e^{x^2}-1}
    \]
\end{esercizio}

Per applicare il teorema dobbiamo trovare quelle equivalenze con gli o-piccoli, e infatti le conosciamo sia per $1-\cos(x) = \frac{1}{2}x^2 + o(x^2)$ che per $e^{x^2}-1 = x^2 +o(x^2)$, e di conseguenza possiamo calcolare il limite come
\[
\lim_{x\to 0} \frac{1-\cos(x)}{e^{x^2}-1} = \lim_{x\to 0} \frac{\frac{1}{2}x^2}{x^2} = \frac{1}{2}
\]
Questo esercizio si sarebbe potuto svolgere anche con i limiti notevoli ma avrebbe richiesto molti calcoli in più.


\addcontentsline{toc}{subsection}{Proprietà degli o-piccoli \textit{II}}
\begin{teorema}{Ulteriori proprietà degli o-piccoli}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$  allora 

    \begin{enumerate}[label=(\roman*)]
        \setcounter{enumi}{6}
        \centering
        \item $o(o(f(x))) = o(f(x))$ per $x\to x_0$
        \item $o(f(x) + o(f(x))) = o(f(x))$ per $x\to x_0$
    \end{enumerate}
\end{teorema}


\begin{proof}
    $(vii)$ Usiamo la definizione di o-piccolo 
    \[
        o(o(f(x))) = o(f(x)) \iff \lim_{x\to x_0} \frac{o(o(f(x)))}{f(x)}
    \]
    Per calcolare e controllare che il limite faccia 0, dobbiamo moltiplicare e dividere per $o(f(x))$
    \[
    \lim_{x\to x_0} \frac{o(o(f(x)))}{f(x)} = \lim_{x\to x_0} \frac{o(o(f(x)))}{f(x)} \cdot \frac{o(f(x))}{o(f(x))} = \lim_{x\to x_0} \frac{o(o(f(x)))}{o(f(x))} \cdot \frac{o(f(x))}{f(x)}
    \]
    Il secondo termine ($\frac{o(f(x))}{f(x)}$) per la proprietà $(i)$ degli o-piccoli sappiamo che tende a 0, e lo stesso vale per il primo termine, infatti la regola generale per questi casi è $\frac{o(f(x))}{f(x)} \to 0$ però se scelgo $f(x)=o(g(x))$, allora il limite diventa $\frac{o(o(g(x)))}{o(g(x))} \to 0$, pertanto
    \[
        \lim_{x\to x_0} \frac{o(o(f(x)))}{o(f(x))} \cdot \frac{o(f(x))}{f(x)} = 0\cdot 0 = 0
    \]
    E visto che il limite viene 0, la proprietà è verificata.

    $(viii)$ Usiamo la definizione di o-piccolo 
    \[
    o(f(x) + o(f(x))) = o(f(x)) \iff \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x)}
    \]
    Possiamo moltiplicare e dividere per $f(x) + o(f(x))$
    \begin{align*}
        \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x)} &= \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x)} \cdot \frac{f(x) + o(f(x))}{f(x) + o(f(x))} \\
        &=  \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x) + o(f(x))} \cdot \frac{f(x) + o(f(x))}{f(x)} \\
        &= \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x) + o(f(x))} \cdot \left(1 +\frac{o(f(x))}{f(x)}     \right)
    \end{align*}
    Possiamo usare lo stesso ragionamento usato per il punto $(vii)$ dicendo che $\frac{o(f(x) + o(f(x)))}{f(x) + o(f(x))}\to 0$ 
    \[
    \lim_{x\to x_0} \frac{o(f(x) + o(f(x)))}{f(x) + o(f(x))} \cdot \left(1 +\frac{o(f(x))}{f(x)}     \right) = 0 \cdot ( 1+ 0) = 0
    \]
    Di conseguenza il teorema è verificato.
\end{proof}


\addcontentsline{toc}{subsection}{Esercizi con o-piccolo}
\begin{esercizio}{}{}
    \[
    \lim_{x\to 0^+} \frac{e^{\sin(x)} - e^{-x}}{1-\cos(\sqrt{x})}
    \]
\end{esercizio}

\begin{proof}
    Per risolvere questo esercizio dobbiamo usare gli sviluppi degli o-piccoli, in questo esercizio ci basta quello di $e^x$ e di $1-\cos(x)$ e quindi sappiamo che 
    \[
        e^{-x} = 1 + -x + o(x)
    \]
    \[
        1-\cos(\sqrt{x}) = \frac{1}{2} (\sqrt{x})^2 + o((\sqrt{x})^2 )
    \]
    Semplificando lo sviluppo del coseno abbiamo che $1-\cos(\sqrt{x}) = \frac{1}{2}x + o(x )$, non serve mettere il modulo perchè $x\to 0^+$ e quindi $x$ è positivo. Ora però dobbiamo capire quanto vale $e^{\sin(x)}$ e intanto usiamo lo sviluppo di $e^x$
    \[
        e^{\sin(x)} = 1 + \mathunderline{red}{\sin(x)} + o(\mathunderline{blue}{\sin(x)})
    \]
    Però il seno lo possiamo sviluppare a sua volta come $\sin(x) = x+ o(x)$ e di conseguenza
    \[
    e^{\sin(x)} = 1 +  \mathunderline{red}{x + o(x)} + o(\mathunderline{blue}{x+ o(x)})
    \]
    E quindi grazie alla proprietà $(viii)$ sappiamo che possiamo riscrivere $(o(x+ o(x)))$ come $o(x)$
    \[
    e^{\sin(x)} = 1 +  x + o(x) + o(x) 
    \]
    Poi per la proprietà $(ii)$ sappiamo che 
    \[
    e^{\sin(x)} = 1 +  x + o(x)
    \]
    Ora possiamo sostituire gli sviluppi nell'esercizio 
    \[
    \lim_{x\to 0^+} \frac{\mathunderline{red}{e^{\sin(x)}}- \mathunderline{blue}{e^{-x}}}{\mathunderline{green}{1-\cos(\sqrt{x})}} = \frac{\mathunderline{red}{1 +  x + o(x)} - \mathunderline{blue}{(1 -  x + o(x))}}{\mathunderline{green}{\frac{1}{2}x + o(x )}} = \lim_{x\to 0^+} \frac{2x + o(x)}{\frac{1}{2}x + o(x)}
    \]
    Per il prinicipio di sostituzione sappiamo che 
    \[
    \lim_{x\to 0^+} \frac{2x + o(x)}{\frac{1}{2}x + o(x)} = \lim_{x\to 0^+} \frac{2x }{\frac{1}{2}x} = \lim_{x\to 0^+} \frac{2}{\frac{1}{2}} = 4
    \]
    Se il numeratore fosse stato $e^{\sin(x)} - e^{x}$, allora dopo lo svilutto il numeratore sarebbe diventato $o(x)$ e quindi la frazione sarebbe stata $\frac{o(x)}{\frac{1}{2}x + o(x)}$ e quindi dividento tutto per $x$ avremmo avuto 
    \[
     \lim_{x\to 0^+}\frac{\frac{o(x)}{x}}{\frac{1}{2} + \frac{o(x)}{x}}
    \]
    E che quindi per la proprietà $(i)$ avremmo avuto 
    \[
     \lim_{x\to 0^+}\frac{\circled[red]{\frac{o(x)}{x}}}{\frac{1}{2} + \circled[red]{\frac{o(x)}{x}}} = \frac{0}{\frac{1}{2} + 0} = 0
    \]
\end{proof}

\begin{esercizio}{}{}
    \[
        \lim_{x\to 0} \frac{e^{\tan(\sin(x))} - 1}{e^{\sin(\tan(x))} - 1}
    \]
\end{esercizio}

\begin{proof}
    Iniziamo analizzando il numeratore, vediamo che possiamo fare lo sviluppo di $e^x-1$
    \[
    e^{\tan(\sin(x))} -1  = \mathunderline{red}{\tan(\sin(x))} + o(\mathunderline{blue}{\tan(\sin(x))})
    \]
    Adesso facciamo lo sviluppo di $\tan(\sin(x)) = \sin(x) + o(\sin(x))$
    \[
    e^{\tan(\sin(x))} -1  = \mathunderline{red}{\sin(x)+ o(\sin(x))} + o(\mathunderline{blue}{\sin(x)+ o(\sin(x))})
    \]
    Possiamo usare la proprietà $(viii)$ e $(ii)$
    \[
    e^{\tan(\sin(x))} -1  = \sin(x)+ o(\sin(x)) +  o(\sin(x)) = \mathunderline{green}{\sin(x)}+ o(\mathunderline{purple}{\sin(x)})
    \] 
    Sviluppiamo anche il seno
    \[
    e^{\tan(\sin(x))} -1  = \mathunderline{green}{x + o(x)} + o(\mathunderline{purple}{x+o(x)})
    \]
    Ripetendo le proprietà $(viii)$ e $(ii)$
    \[
    e^{\tan(\sin(x))} -1  = x + o(x) + o(x) = x + o(x)
    \]
    Sistemato il numeratore, dobbiamo fare gli stessi passaggi al denominatore, ma invertendo il passaggio dello sviluppo del seno con quello della tangente
    \begin{align*}
        e^{\sin(\tan(x))} - 1 &= \mathunderline{red}{\sin(\tan(x))} + o(\mathunderline{blue}{\sin(\tan(x))}) \\
        &= \mathunderline{red}{\tan(x)+ o(\tan(x))} + o(\mathunderline{blue}{\tan(x)+ o(\tan(x))}) \\
        &= \tan(x)+ o(\tan(x)) +  o(\tan(x))  \\
        &= \mathunderline{green}{\tan(x)}+ o(\mathunderline{purple}{\tan(x)}) \\
        &= \mathunderline{green}{x + o(x)} + o(\mathunderline{purple}{x+o(x)})\\ 
        &= x + o(x) + o(x) \\
        &= x + o(x)
    \end{align*}

    E quindi sostituendo questi sviluppi nel limite abbiamo che 
    \[
    \lim_{x\to 0} \frac{e^{\tan(\sin(x))} - 1}{e^{\sin(\tan(x))} - 1} = \lim_{x\to 0} \frac{x + o(x)}{x + o(x)} = \lim_{x\to 0} \frac{x }{x } = 1
    \]
\end{proof}

\newpage
\begin{esercizio}{}{}
    \[
    \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)\cos(3x^2)}
    \]
\end{esercizio}

\begin{proof}
    In primis notiamo che il termine $\cos(3x^2) \to 1$ per $x\to 0$, pertanto possiamo staccarlo dal limite e calcolarlo a parte 
    \[
    \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)\cos(3x^2)} = \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)} \cdot \frac{1}{\cos(3x^2)} = \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)} \cdot 1
    \] 
    Dopo questo, vediamo che abbiamo un $\log(1+x)$ e $\tan(x)$ che possiamo sviluppare
    \[
    \log(1 + x\sin^2(2x)) = x\sin^2(2x) + o(x\sin^2(2x)) 
    \]
    Facciamo lo stesso per $\sin(2x) = 2x+ o(x)$
    \[
    \log(1 + x\sin^2(2x)) = x \cdot (2x+ o(x))^2 + o(x\cdot (2x+ o(x))^2) 
    \] 
    ora capiamo il come calcolare il termine $(2x+ o(x))^2$ infatti proviamo a sviluppare il quadrato di binomio e troviamo che $4x^2 + 4x \cdot o(x) + (o(x))^2$, per la proprietà $(iv)$ possiamo riscrivere $(o(x))^2$ come $(o^2)$, mentre il termine $4x\cdot o(x)$ possiamo usare la proprietà $(iii)$ per portare $4x$ dentro l'o-piccolo. Quindi il termine diventa $4x^2 + o(4x^2)$. Di conseguenza il logaritmo diventa, usando le proprietà $(iii)$ e $(viii)$
    \begin{align*}
        \log(1 + x\sin^2(2x)) &= x \cdot (4x^2+ o(x^2)) + o(x\cdot (4x^2+ o(x^2))) \\
        &= 4x^3+ o(4x^3) + o( 4x^3+ o(4x^3)) \\ 
        &= 4x^3+ o(4x^3) + o(4x^3) \\
        &= 4x^3+ o(4x^3) 
    \end{align*}
    
    Per la tangente facciamo gli stessi procedimenti
    \begin{align*}
        \tan^3(x) &= (x+o(x))^3 \\
        &= x^3 + 3x^2 \cdot o(x) + 3x \cdot (o(x))^2 + (o(x))^3 \\
        &= x^3 +  o(3x^3) + 3x \cdot o(x^2) + o(x^3) \\
        &= x^3 +  o(x^3) +  o(3x^3) + o(x^3) \\
        &= x^3 +  o(x^3) + o(x^3) + o(x^3) \\
        &= x^3 +  o(x^3) \\
    \end{align*}

    E quindi sostituiendo gli sviluppi abbiamo che 
    \[
    \lim_{x\to 0} \frac{\log(1 + x\sin^2(2x))}{4\tan^3(x)} = \lim_{x\to 0} \frac{4x^3+ o(4x^3) }{4(x^3 +  o(x^3))} = \lim_{x\to 0} \frac{4x^3+ o(4x^3) }{4x^3 +  o(4x^3)} = \lim_{x\to 0} \frac{4x^3}{4x^3 }  = 1
    \]
\end{proof}


Con l'esercizio precedente abbiamo scoperto che 

\addcontentsline{toc}{subsection}{Binomio con o-piccolo}
\begin{teorema}{Binomio con o-piccolo}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$  allora
    \[
        (f(x) + o(f(x)))^n = f^n(x) + o(f^n(x)) \;\;\;\;\; \forall n \in \mathbb{N}
    \] 
\end{teorema}
\begin{proof}
    Per dimostrare questo teorema dobbiamo usare il binomiale di Newton e quindi 
    \[
    (f(x) + o(f(x)))^n = \sum_{k=0}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot [o(f(x))]^{k} 
    \]
    Estraiamo il primo termine con $k=0$ visto che in quel caso $o(f(x))$ avrebbe esponente 0 e quindi non ci sarebbe
    \[
    \sum_{k=0}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot [o(f(x))]^{n} = f^{n}(x) +  \sum_{k=1}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot [o(f(x))]^{k}
    \]
    Ora possiamo usare le proprietà $(iv)$ per poter portare dentro l'esponente nel o-piccolo, poi la proprietà ($iii$) per portare il termine $[f(x)]^{n-k}$ dentro al o-piccolo
    \begin{align*}
        \sum_{k=1}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot [o(f(x))]^{k} &= \sum_{k=1}^{n} \binom{n}{k} \cdot [f(x)]^{n-k} \cdot o(f^{n}(k)) \\
        &= \sum_{k=1}^{n} \binom{n}{k}  \cdot o([f(x)]^{n-k} \cdot f^{k}(x)) \\
        &= \sum_{k=1}^{n} \binom{n}{k}  \cdot o([f(x)]^{n-k+ k}) \\
        &= \sum_{k=1}^{n} \binom{n}{k}  \cdot o(f^n(x)) 
    \end{align*}
    Poi visto che $\binom{n}{k}$ è una costante la possiamo togliere per la $(vi)$, e poi dato che dentro la sommatoria non ci sono più termini rispetto a $k$ possiamo calcolarla ($\sum_{k=1}^{n} c = n\cdot c$)
    \[
        \sum_{k=1}^{n} \binom{n}{k}  \cdot o(f^n(x))  = \sum_{k=1}^{n} o(f^n(x)) = n\cdot o(f^n(x))
   \]
   Ora possiamo usare di nuovo la proprietà $(vi)$ per togliere $n$ e così il teorema è verificato
   \[
   n\cdot o(f^n(x)) = o(f^n(x))
   \]
   \[
   (f(x) + o(f(x)))^n = f^n(x) + o(f^n(x))
   \]

\end{proof}

\begin{esercizio}{}{}
    Sia $f:\mathbb{R} \to\mathbb{R} $  tale che $f(x) = o(x^2)$ per $x\to +\infty$, discutere il limite 
    \[
        \lim_{x\to +\infty} \frac{f(x)}{x^\alpha + x + 1}
    \]
    Al variare di $\alpha \in \mathbb{R}$
\end{esercizio}
\begin{proof}
    Visto che sappiamo che $f(x) = o(x^2)$, l'unica unformazione che sappiamo è che 
    \[
        \lim_{x\to +\infty} \frac{f(x)}{x^2} = 0
    \]
    Pertanto divisiamo numeratore e denominatore per $x^2$
    \[
    \lim_{x\to +\infty} \frac{f(x)}{x^\alpha + x + 1} = \lim_{x\to +\infty} \frac{\frac{f(x)}{x^2}}{x^{\alpha-2} +\frac{1}{x} + \frac{1}{x^2}}
    \]
    Ora il numeratore tende a 0, i termini $\frac{1}{x}$ e $\frac{1}{x^2}$ tendono a 0 per $x\to +\infty$, quindi ci manca da capire il termine $x^ {\alpha-2}$. Per questo dobbiamo dividere in 3 casi. 

    Se $\alpha > 2$ allora $\alpha -2 > 0$ e quindi $x^{\alpha -2} \to +\infty$ e quindi il denominatore complessivamente tende a $+\infty$ e pertanto di conseguenza il limite tende a 0
    \[
    \lim_{x\to +\infty} \frac{\frac{f(x)}{x^2}}{x^{\alpha-2} +\frac{1}{x} + \frac{1}{x^2}} = \frac{0}{+\infty +0 + 0} = \bigl[\frac{0}{+\infty}\bigr] = 0
    \]

    \textbf{N.B.} scrivere $\bigl[\frac{0}{+\infty}\bigr]$ è tecnicamente sbagliato perchè $\infty$ non è un numero ma un simbolo,  e pertanto non si possono fare le operazioni con quel numero. L'ho scritto soltato per enfatizzare il concetto di limite ma non è tecnicamente corretto scriverlo. Volendo essere più precisi, avremmo dovuto usare l'algebra dei limiti infiniti.

Se $\alpha = 2$ allora il termine $x^{\alpha -2} = x^{2 -2} = 1$ e quindi il limite diventare
    \[
    \lim_{x\to +\infty} \frac{\frac{f(x)}{x^2}}{x^{\alpha-2} +\frac{1}{x} + \frac{1}{x^2}}  = \frac{0}{1+0+0} = 0
    \]

    Se $\alpha < 2$ allora possiamo il termine $x^{\alpha -2} \to 0$ dato che l'esponente sarebbe negativo e quindi andrebbe a denominatore. E quindi il limite diventerebbe
    \[
    \lim_{x\to +\infty} \frac{\frac{f(x)}{x^2}}{x^{\alpha-2} +\frac{1}{x} + \frac{1}{x^2}}  = \frac{0}{0+0+0} = \bigl[\frac{0}{0}\bigr]
    \]
    In questo caso siamo incappati in una forma indeterminata, e che quindi con i dati che abbiamo a nostra disposizione non possiamo dire nulla in merito al limite.
    
\begin{center}
    $\displaystyle\lim_{x\to +\infty} \frac{f(x)}{x^\alpha + x + 1}=\begin{cases}
        0 & \text{se } \alpha \ge 2 \\
        \text{Non Definibile} & \text{se } \alpha < 2
    \end{cases}$
\end{center}
\end{proof}


\addcontentsline{toc}{subsection}{Definizione di O-grande}
\begin{definizione}{O-grande}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ . Se esiste $I \subseteq \mathbb{R}$ intorno di $x_0$  $\exists M >0$ tale che 
    \[
        |f(x)| \leq M|g(x)| \;\;\;\;\; \forall x \in A \cap I \setminus \{x_0\}
    \]
    Allora diciamo che "$f(x)$ è un O-grande di $g(x)$ per $x\to x_0$" e lo indichiamo con il simbolo
    \[
        f(x) = O(g(x))\;\;\;\;\; x\to x_0
    \]
\end{definizione}

\begin{esempio}{}{}
    Sia $f(x) = x\sin(\frac{1}{x})$ e possiamo vedere che 
    \[
    \left|x\sin\left(\frac{1}{x}\right)\right| \leq |x| \;\;\;\;\; \forall x \in \mathbb{R}
    \]
    Visto che $|\sin(x)| \leq 1$. Di conseguenza possiamo prendere un qualsiasi punto $x_0 \in \mathbb{R}$ e sapremo che 
    \[
    x\sin\left(\frac{1}{x}\right) = O(x) \;\;\;\;\; x\to x_0
    \]
\end{esempio}

\begin{esercizio}{}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$. Allora 
    \[
        f(x) = o(g(x)) \;\;\; x\to x_0 \implies f(x) = O(g(x)) \;\;\; x\to x_0
    \]
\end{esercizio}

\begin{proof}
    Per ipotesi sappiamo che $f(x) = o(g(x))$, che usando la definizione di o-piccolo sappiamo che 
    \[
    f(x) = o(g(x)) \iff \lim_{x\to x_0} \frac{f(x)}{g(x)} = 0
    \]
    Pertanto usando la definizione di limite sappiamo che 
    \[
        \left|\frac{f(x)}{g(x)}\right| < \varepsilon \;\;\; \forall x \in I
    \]
    Dove $I\subseteq \mathbb{R}$ è un intorno di $x_0$. Ora usiamo le proprietà dei modili
    \[
    \left|\frac{f(x)}{g(x)}\right| < \varepsilon  \implies \frac{|f(x)|}{|g(x)|} < \varepsilon \implies |f(x)| < \varepsilon |g(x)|
    \]
    Però la definizione O-grande richiedeva che ci fosse almeno un $M>0$ tale che $ |f(x)| < M|g(x)|$, però con la definizione di limite abbiamo trovato che vale $\forall \varepsilon >0$ e di conseguenza basta scegliere $M = \varepsilon$ e implicazione è verificata.

   
\end{proof}

 \newpage
    \textbf{ATTENZIONE} Non è vero il contrario, infatti lo vediamo con un esempio, infatti se  prendiamo $f(x)=x+1$ e $g(x)=x+2$ è facile vedere che 
    \[
    |x+1| \leq |x+2| \;\;\;\;\; \forall x \geq -1
    \]
    Pertanto possiamo scegliere un qualsiasi intorno di 0 della forma $I = (-\delta, +\delta)$ con $\delta\leq 1$, e di conseguenza è verificata la definizione di O-grande e quindi
    \[
        x+1 = O(x+2)\;\;\;\;\; \forall x \in I
    \]
    Però se proviamo a vedere se $f(x)=o(g(x))$ vediamo subito che non lo è dato che
    \[
        \lim_{x\to 0}\frac{f(x)}{g(x)} = \lim_{x\to 0}\frac{x+1}{x+2} = \frac{1}{2} \ne 0 
    \]
    Questo succede perchè O-grande ci dice che una funzione è più piccola di un'altra, mentre o-piccolo ci dice che una funzione è tanto più piccola di un'altra, a tal punto da rendere il limite uguale a 0.


\addcontentsline{toc}{subsection}{Confronti di Infiniti}
\begin{definizione}{Confronti di Infiniti}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f, g:A \to \mathbb{R}$, $x_0 \in \mathbb{R} \cup \{\pm\infty\}$ punto di acc. in $A$ e se 
    \[
    \begin{array}{c @{\hspace{2cm}} c}
    \displaystyle\exists \lim_{x\to x_0}f(x) \in \{\pm \infty\} &  \displaystyle\exists \lim_{x\to x_0}g(x) \in \{\pm \infty\} 
    \end{array}
    \]
    Allora dichiamo che 
    \begin{itemize}
        \item "$f(x)$ è infinito dello stesso ordine di $g(x)$" se
        \[
        \exists \lim_{x\to x_0}\frac{f(x)}{g(x)} \in \mathbb{R} \setminus \{0\}
        \] 
        \item "$f(x)$ è infinito ordine di ordine inferiore $g(x)$"  se
        \[
        \exists \lim_{x\to x_0}\frac{f(x)}{g(x)} = 0
        \] 
         \item "$f(x)$ è infinito ordine di ordine superiore $g(x)$" se
        \[
        \exists \lim_{x\to x_0}\frac{g(x)}{f(x)} = 0
        \] 
        \item "$f(x)$ è infinito di ordine non confrontabile a $g(x)$" se
        \[
        \nexists \lim_{x\to x_0}\frac{f(x)}{g(x)} 
        \] 
    \end{itemize}
\end{definizione}

\addcontentsline{toc}{subsection}{Confronti di Infinitesimi}
\begin{definizione}{Confronti di Infinitesimi}{}
    Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0 $ punto di acc. in $A$ e se

    \begin{itemize}
        \item $x_0 \in \mathbb{R}$ allora diciamo che 
        
        \begin{itemize}
            \item "$f(x)$ è infinitesima di ordine $\alpha \in \mathbb{R}$ per $x\to x_0$" se
            \[
                \exists \lim_{x\to x_0}\frac{f(x)}{|x-x_0|^\alpha} \in \mathbb{R} \setminus \{0\}
            \]
            \item "$f(x)$ è infinita di ordine $\alpha \in \mathbb{R}$ per $x\to x_0$" se
            \[
                \exists \lim_{x\to x_0}f(x)|x-x_0|^\alpha \in \mathbb{R} \setminus \{0\}
            \]
        \end{itemize}

        \item $x_0 \in \{\pm \infty\}$ allora diciamo che 
        
        \begin{itemize}
            \item "$f(x)$ è infinitesima di ordine $\alpha \in \mathbb{R}$ per $x\to x_0$" se
            \[
                \exists \lim_{x\to x_0}f(x)|x|^\alpha \in \mathbb{R} \setminus \{0\}
            \]
            \item "$f(x)$ è infinita di ordine $\alpha \in \mathbb{R}$ per $x\to x_0$" se
            \[
                \exists \lim_{x\to x_0}\frac{f(x)}{|x|^\alpha} \in \mathbb{R} \setminus \{0\}
            \]
        \end{itemize}
         
    \end{itemize}

\end{definizione}

\begin{esercizio}{}{}
    Calcolare l'ordine di infinitesimo per $x\to 0^+$ di $f(x)=\sqrt{1+x^2} - \cos(x)$,
    \end{esercizio}
    \begin{proof}
     per farlo dobbiamo usare la definzione di funzione infinitesima
    \[
        \lim_{x\to 0^+} \frac{\sqrt{1+x^2} - \cos(x)}{x^\alpha}
    \]
    Per capire quanto vale dobbiamo fare una razionalizzazione
   \begin{align*}
    \lim_{x\to 0^+} \frac{\sqrt{1+x^2} - \cos(x)}{x^\alpha} &= \lim_{x\to 0^+} \frac{\sqrt{1+x^2} - \cos(x)}{x^\alpha} \cdot \frac{\sqrt{1+x^2} + \cos(x)}{\sqrt{1+x^2} + \cos(x)} \\ 
    &= \lim_{x\to 0^+} \frac{1+x^2 - \cos^2(x)}{x^\alpha (\sqrt{1+x^2} + \cos(x))}
    \end{align*}

    Il termine $\sqrt{1+x^2} + \cos(x)\to 2$ quindi possiamo levarlo visto che non influisce sul grado della funzione. Notiamo in oltre che possiamo spaccare la funzione nel seguente modo
    \[
    \lim_{x\to 0^+} \frac{1+x^2 - \cos^2(x)}{x^\alpha (\sqrt{1+x^2} + \cos(x))} = \lim_{x\to 0^+} \frac{x^2 }{x^\alpha} + \frac{1 - \cos^2(x)}{x^\alpha}
    \]
    Ed entrambi convergono solo se $\alpha = 2$, e quindi il gradi infinitesimale di $f(x)$ è 2.
    Se fosse stato $f(x)=\sqrt{1-x^2} - \cos(x)$ era necessario usare gli sviluppi dell'o-piccolo.
\end{proof}

\section{Successioni e Serie}
\addcontentsline{toc}{subsection}{Definizione di Successione}
\begin{definizione}{Successioni}{}
    Si definisce successione una funzione a variabile naturale a valori reali, $a: \mathbb{N} \to \mathbb{R}$ e si indica con $(a_n)_{n\in \mathbb{N}}$ oppure $\{a_n\}_{n\in \mathbb{N}}$, mentre il ternime generale si indica con $a(n)$ oppure $a_n$. Il dominio può essere $\mathbb{N}$, $\mathbb{N}_0$ oppure $A = \{n \in \mathbb{N}: n \geq n_0\}$, dove $n_0$ è un qualsiasi numero naturare dal quale inizia la successione. 
\end{definizione}

vediamo qualche esempio di successione 
\begin{esempio}{}{}
    \begin{itemize}
        \item $a_{n} = n! \;\;\; \forall n \in \mathbb{N}_0$
        \item $a_n = (n-5)! \;\;\;\forall n \geq 5$
    
        Qui chiaramente il fattoriale è definitio per numeri non negativi, quindi la successione deve partire da 5, perchè prima non è definita.
        \item $a_n = \frac{1}{n} \;\;\;\forall n \in \mathbb{N}$
        
        In questo caso basta togliere soltanto il caso $n=0$
        \item $a_n = \frac{1}{n-7} \;\;\;\forall n \geq 8$
        
        Teoricamente in questo caso basterebbe levare il caso $n=7$, ma per evitare di avere un "buco" nella sequenza, la facciamo partire da $n=8$.
    \end{itemize}
\end{esempio}

\addcontentsline{toc}{subsection}{Punti di Accumulazione per le Successioni }
\begin{teorema}{Punti di Accumulazione per le Successioni}{}
    Dato che le successioni sono delle funzioni, allora saranno validi tutti i teoremi visti in precedenza, però abbiamo qualche particolarità, in fatti nelle successioni esiste un unico punto di accumulazione: $+\infty$, dato che se prendiamo un qualsiasi altro punto vedremo che è isolato. Infatti per ogni punto del dominio posso sempre trovare un intervallo vuoto, basta scegliere un raggio $< 1$. Quindi nelle successioni possiamo fare solo ed esclusivamente il seguente limite 
    \[
    \lim_{n\to +\infty} a_n =l
    \]
    Di conseguenza possiamo scrivere anche soltanto $\lim a_n = l$ oppure ancora più semplicemente $a_n \to l$.
\end{teorema}

\addcontentsline{toc}{subsection}{Convergenza, Divergenza e Irregolarità delle successioni }
\begin{definizione}{Convergenza, Divergenza e Irregolarità delle successioni}{}
    Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ allora
    \begin{itemize}
        \item Se $\lim  a_n = l\in \mathbb{R}$ allora diciamo che $(a_n)_{n\in A}$ è convergente a $l$
        \item Se $\lim  a_n = l\in \{\pm \infty\}$ allora diciamo che $(a_n)_{n\in A}$ è divergente a $\{\pm \infty\}$
        \item Se $\nexists\lim  a_n$  allora diciamo che $(a_n)_{n\in A}$ è irregolare
 
    \end{itemize}
     
\end{definizione}


\addcontentsline{toc}{subsection}{Monotonia e Limitatezza delle Successioni}
\begin{definizione}{Monotonia e Limitatezza delle Successioni}{}
    Dato che le successioni sono delle funzioni, riprendiamo le prinicipali definizioni delle funzioni a variabili reali e le analiziamo per le successioni

    Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ allora diciamo che 
    \begin{itemize}
        \item $(a_n)_{n\in A}$ è monotona crescente se $a_n \leq a_{n+1} \;\;\; \forall n \in A$
        \item $(a_n)_{n\in A}$ è monotona decrescente se $a_n \geq a_{n+1} \;\;\; \forall n \in A$
    \end{itemize}
    E chiaramente ci sarà anche la monotonia crescente stretta con $a_n < a_{n+1}$ e uguale per la descescenza. In oltre possiamo dire che è definitivamente crescente se è crescente $\forall n \geq \bar{n}$, ragionamento analogo alla decrescenza.

    \begin{itemize}
        \item $(a_n)_{n\in A}$  è limitata se $\exists M \geq 0$ tale che $|a_n| \leq M \;\;\; \forall n \in A$
        \item $(a_n)_{n\in A}$  è limitata definitivamente se $\exists M \geq 0$ tale che $|a_n| \leq M \;\;\; \forall n \geq \bar{n}$
    \end{itemize}

    \textbf{N.B.} come dicevamo le successioni sono vere e proprie funzioni e pertanto saranno validi tutti i seguenti Teoremi: Teorema dell'unicità del limite, Relazione d'ordine \textit{I} e \textit{II}, Teorema dei due carabinieri, algebra dei limiti finiti e infiniti.
\end{definizione}


\addcontentsline{toc}{subsection}{Definizione di Successione Ricorsiva}
\begin{definizione}{Successioni Ricorsive}{}
     Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ allora diciamo che $a_n$ è ricorsiva se esiste una funzione tale che 
     \[
     a_n = f(a_{n-1})
     \]
     Un esempio classico è il fattoriale, infatti il fattoriale lo possiamo scrivere come
     \begin{align*}
        n! &= n\cdot (n-1)! \\
        a_n &= n\cdot a_{n-1}
     \end{align*}
\end{definizione}

\addcontentsline{toc}{subsection}{Limitatezza delle succesioni quando esiste Limite}
\begin{teorema}{Limitatezza delle succesioni quando esiste Limite}{}
    Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ e se $a_n \to l \in \mathbb{R}$ allora $(a_n)_{n\in A}$ è limitata globalmente.
     
\end{teorema}
\begin{proof}
    Nelle funzioni a variabili reali sapevamo che se esiste il limite allora la funzione era limitata definitivamente, proprio perche se $\displaystyle\exists\lim_{x\to x_0} f(x) = l$ sapevamo che 
    \[
        |f(x) - l| < \varepsilon \;\;\;\; \forall x \in I
    \] 
    E da questo potevamo dedurre che 
    \begin{align*}
        |f(x)| &= |f(x) - l + l| \\
        &\leq |f(x) - l| + |l|\\ 
        &< \varepsilon + |l|
    \end{align*}
    E di conseguenza noi sappiamo che $f(x)$ era limitata definitivamente, e quindi questo sarà vero anche per le successioni, ma con le successio possiamo dire qualcosa di più. Infatti se $a_n \to l$ per lo stesso ragionamento possiamo dire che 
    \[
        |a_n| < \varepsilon + |l|  \;\;\; \forall n \geq \bar{n}
    \]
    Ora possiamo prendere tutti i numeri prima di $\bar{n}$ e $\varepsilon + |l|$ e prendere il massimo tra questi valori
    \[
    M = \max\{|a_1|, |a_2|, |a_3|, ... ,|a_{\bar{n}}|,  \varepsilon + |l|\}
    \]
    Importante specificare che $M \not \in \{\pm \infty\}$, perche tutti i valori con $n\leq \bar{n}$ sono valori finiti, e lo stesso vale per $\varepsilon + |l|$. Ora notiamo che per i valori di $n\leq\bar{n}$ saranno sicuramente più piccoli del massimo tra loro. mentre per i valori con $n> \bar{n}$ abbiamo che saranno sempre minori di $\varepsilon + |l|$, di conseguenza $|a_n| < M$ per $\forall n \in A$ che è la definizione di limitata globarlmente, e non solamente definitivamente.
\end{proof}


\addcontentsline{toc}{subsection}{Definizione di Progressione Geormetrica}

\begin{definizione}{Progressione Geormetrica}{}
    Fissato $r\in \mathbb{R}$ detta ragione, allora si dice progressione geometrica la successione definita come
    \[
        a_n = r^n \;\;\;\;\; \forall n\in \mathbb{N}_0
    \]
    \begin{center} il cui limite è $\displaystyle\lim a_n=\begin{cases} +\infty & \text{se } r > 1 \\ 1 & \text{se } r = 1 \\0 & \text{se } -1<r < 1 \\ \text{Non Esiste} & \text{se } r \leq -1 \end{cases}$ \end{center}
    
\end{definizione}


\addcontentsline{toc}{subsection}{Definizione di Sottosuccessione}
\begin{definizione}{Sottosuccessione}{}
    Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$  e una funzione $\varphi: \mathbb{N} \to \mathbb{N}$ strettamente crescente, allora si dice Sottosuccessione, o estratta, di $(a_n)_{n \in A}$ la successione $(a_{\varphi(k)})_{k \in A}$
\end{definizione}

        Vediamo un esempio di sottosuccessione
    \begin{esempio}{}{}
        \[
    \begin{array}{c @{\hspace{2cm}} c}
    a_n = (-1)^n &  \varphi: n \to 2n
    \end{array}
    \]
    \end{esempio}
    \begin{proof}
        Notiamo subito che la funzione $\varphi(n)$ è strettamente crescente, quindi la possiamo usare. Proviamo quindi a trovare la sotto successione $(a_{\varphi(k)})_{k \in \mathbb{N}}$, per farlo basta sostituire la funzione nella $n$ nella funzione originale.
        \[
        a_{\varphi(n)} = (-1)^{2n} \implies a_{\varphi(n)} = ((-1)^{2})^{n} \implies a_{\varphi(n)} = 1^n \implies a_{\varphi(n)} = 1
        \]
        Quindi questa sottosuccessione è semplicemente una sequenza di 1. Da notare che le sottosuccessioni sono come le funzioni composte nelle funzioni a variabile reale. In oltre in questo esempio possiamo notare che se avessimo scelto $\varphi(n) = 2n+1$ la sottosuccessione risultante sarebbe stata  $a_{\varphi(n)} = -1$, e questo vedremo presto implica una cosa molto importante.
    \end{proof}


\addcontentsline{toc}{subsection}{Relazione tra Successione e le sue Sottosuccessioni}
    \begin{teorema}{Relazione tra Successione e le sue Sottosuccessioni}{}
        Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ allora $\exists a_n \to l \in \mathbb{R} \cap \{\pm \infty\}$  se e solo se tutte le sottosuccessioni hanno limite $l$
    \end{teorema}

    Il teorema per com'è scritto non è molto utile, infatti è molto difficile controllare se tutte le possibili sottosuccessioni facciano lo stesso limite, ma è molto più utile la negazione di questo teorema, infatti se è vero che tutte le sottosuccessioni devono avere limite $l$, è anche vero che ne basta trovare almeno una che hanno limite diverso per dire che la successione originale non ha limite.

    \begin{esempio}{}{}
        \[
            a_n = (-1)^n
        \]
    \end{esempio}
    \begin{proof}
        Infatti come avevamo visto se usiamo la sottosuccessione con $\varphi(n) = 2n$, veniva fuori $a_{\varphi(n)} = 1$, mentre se prendevo la sottosuccessione $\varphi(n) = 2n +1$ diventerebbe $a_{\varphi(n)} = -1$, di conseguenza due sottosuccessioni hanno limite diverso allora la successione $a_n = (-1)^n$ non ha limite.
    \end{proof}


\addcontentsline{toc}{subsection}{Teorema di Bolzano-Weierstrass}
    \begin{teorema}{Teorema di Bolzano-Weierstrass}{}
        Se $(a_n)_{n\in A}$ è limitata, allora esiste almeno una sottosuccessione $(a_{\varphi(n)})_{n\in A}$ che converge
    \end{teorema}

    Un esempio lo abbiamo visto appena adesso con la successione $ a_n = (-1)^n$, visto che è una funzione limitata allora almeno una, noi ne abbiamo trovate 2, sottosuccessione converge.

\addcontentsline{toc}{subsection}{Caratterizzazione sequenziale del Limite}
    \begin{teorema}{Caratterizzazione sequenziale del Limite}{}
        Sia $\varnothing \ne A \subseteq \mathbb{R}$, $f:A \to \mathbb{R}$, $x_0$ punto di acc. in $A$ allora 
        \[
            \exists \lim_{x\to x_0} f(x) = l \in \mathbb{R} \cap \{\pm\infty\} \iff  \lim_{n\to +\infty} f(a_n) = l
        \]
        Per ogni successione $(a_n)_{n\in \mathbb{N}}$ tale che
        \begin{itemize}
            \centering
            \item $a_n \ne 0$ definitivamente per $n \to +\infty$
            \item $\lim a_n \to x_0$
        \end{itemize}
    \end{teorema}

    Anche qua come nella relazione tra Successione e Sottosuccessioni, è impossibile trovare qualsiasi successione e controllare che tutte facciano $l$. Questo teorema è molto ultile usare la sua negazione, infatti basta trovare due successioni che rendono il limite diverso per capire che il limite della funzione originale non esiste. Infatti proviamo a calcolare un limite che è difficile da dimostrare, ma che con questo teorema diventa molto semplice.
    
\addcontentsline{toc}{subsection}{Esempi di applicazione della Caratterizzazione sequenziale del Limite}
    \newpage
    \begin{esercizio}{}{}
        \[
            \lim_{x\to +\infty} \sin(x)
        \]
    \end{esercizio}
    \begin{proof}
        Possiamo scegliere $a_n = \frac{\pi}{2} + 2\pi n$ e possiamo vedere che possiamo applicare il teorema perchè $a_n \ne 0$ definitivamente, e che $a_n \to +\infty$, quindi vediamo come diventa il limite
        \[
            \lim_{n\to +\infty} \sin(a_n) = \lim_{n\to +\infty} \sin\left(\frac{\pi}{2} + 2\pi n\right)
        \]
        Usando le proprietà del seno sappiamo che $\sin(\frac{\pi}{2} + 2\pi n) = 1 \;\;\; \forall n \in \mathbb{Z}$ e quindi 
        \[
        \lim_{n\to +\infty} \sin\left(\frac{\pi}{2} + 2\pi n\right) = \lim_{n\to +\infty } 1 = 1
        \]
        Ora ripetiamo lo stesso procedimento per $b_n = \frac{3\pi}{2} + 2\pi n$, ricordanto che per le proprietà del seno $\sin(\frac{3\pi}{2} + 2\pi n) = -1 \;\;\; \forall n \in \mathbb{Z}$
        \[
                   \lim_{n\to +\infty} \sin(b_n) = \lim_{n\to +\infty} \sin\left(\frac{3\pi}{2} + 2\pi n\right) = \lim_{n\to +\infty } -1 = -1
        \]
        Abbiamo trovato quindi due successione che fanno tendere la funzione a due limiti diversi e di conseguenza $\displaystyle\nexists            \lim_{x\to +\infty} \sin(x)$
    \end{proof}

    \begin{esercizio}{}{}
        \[
            \lim_{x\to 0} \frac{1}{x}
        \]
    \end{esercizio}

    \begin{proof}
        Anche se lo abbiamo già dimostrato che questo limite non esiste, proviamo a dimostrarlo grazie a questo teorema, infatti se scelgo $a_n = \frac{1}{n}$, è valida perchè $a_n \to 0$ e di condeguenza il limite diventa
        \[
        \lim_{x\to 0} \frac{1}{x} = \lim_{n\to +\infty} \frac{1}{a_n} = \lim_{n\to +\infty} \frac{1}{\frac{1}{n}} = \lim_{n\to +\infty} n = +\infty
        \] 

        Invece se provia con la successione $b_n = \frac{-1}{n}$ succede che
        \[
        \lim_{x\to 0} \frac{1}{x} = \lim_{n\to +\infty} \frac{1}{b_n} = \lim_{n\to +\infty} \frac{1}{\frac{-1}{n}} = \lim_{n\to +\infty} -n = -\infty
        \] 
        Anche qua vediamo che con la sucessione $a_n$ il nostro limite tende a $+\infty$, mentre con $b_n$ tende a $-\infty$ e pertanto $\displaystyle\nexists \lim_{x\to 0} \frac{1}{x}$, come avevamo già dimostrato.
    \end{proof}
    \newpage
    \begin{esercizio}{}{}
        Sia $f:\mathbb{R} \to \mathbb{R}$ una funzione periodica non costante, Allora
        \[
            \nexists\lim_{x\to +\infty} f(x)
        \]
    \end{esercizio}
    \begin{proof}
        In primis capiamo che ipotesi abbiamo infatti una funzione è periodica se $\exists T > 0$ tale che $f(x+nT) = f(x)\;\;\; \forall n \in \mathbb{Z}, \forall x\in \mathbb{R}$. In più sappiamo che non è costante, che vuol dire  $\exists x_1 \ne x_2$ tali che $f(x_1) \ne f(x_2)$. 

        Quindi se scelgo $a_n = x_1 + nT$, e posso farlo perchè $a_n \to +\infty$ visto che $T >0$ per ipotesi, allora posso sostituirlo
        \[
        \lim_{x\to +\infty} f(x) = \lim_{n\to +\infty} f(a_n) = \lim_{n\to +\infty} f(x_1 + nT)
        \]
        Per definizione di funzione periodica sappiamo che $f(x_1 + nT) = f(x_1)$
        \[
        \lim_{n\to +\infty} f(x_1 + nT) = \lim_{n\to +\infty} f(x_1) = f(x_1)
        \]
        Ora posso scegliere $b_n = x_2 + nT$, e per la stessa logica di $a_n$
        \[
        \lim_{x\to +\infty} f(x) = \lim_{n\to +\infty} f(b_n) = \lim_{n\to +\infty} f(x_2 + nT) = \lim_{n\to +\infty} f(x_2) = f(x_2)
        \]
        Notiamo che in un caso il limite tende a $f(x_1)$ e in un altro tende a $f(x_2)$, e visto che per ipotesi $f(x_1) \ne f(x_2)$ allora $\displaystyle\nexists\lim_{x\to +\infty} f(x)$ 
    \end{proof}

\addcontentsline{toc}{subsection}{Gerarchia degli Infinito per le Successioni}
    \begin{teorema}{Gerarchia degli Infinito per le Successioni}{}
        Visto che le successioni sono funzioni ci portiamo dietro i seguenti teoremi
        
        Sia $\alpha>0, \beta>0, a>1$ 
        \begin{enumerate}[label=(\roman*)]
            \centering
            \item $\displaystyle\lim_{n\to+\infty} \frac{(\log(n))^\beta}{n^\alpha}= 0$
            \item $\displaystyle\lim_{n\to+\infty} \frac{n^\alpha}{a^n}= 0$ 
        \end{enumerate}

        In più, visto che le successioni sono definite per i numeri interi, allora possiamo usare anche il fattoriale, e quindi abbiamo due limiti nuovi
        \begin{enumerate}[label=(\roman*)]
            \setcounter{enumi}{2}
            \centering
            \item $\displaystyle\lim_{n\to+\infty} \frac{a^n}{n!}= 0$
            \item $\displaystyle\lim_{n\to+\infty} \frac{n!}{n^n}= 0$ 
        \end{enumerate}

    \end{teorema}
    \begin{proof}
        I primi due sono verificati perchè sono vere per le funzioni a variabili reali, e di conseguenza valgono anche per le successioni. Iniziamo dimostrando la $(iv)$.
\newpage
        $(iv)$ Notiamo subito che sia $n!\ge 0$ che $n^n \ge 0$ e di conseguenza $\frac{n!}{n^n} \geq 0$. Ora riscriviamo la frazione usando la definizione di fattoriale
        \[
            \frac{n!}{n^n} = \frac{n \cdot (n-1)\cdot (n-2)\cdot ... \cdot 2\cdot 1}{n \cdot n \cdot n \cdot ...\cdot n  \cdot n } = \frac{n}{n} \cdot \frac{n-1}{n} \cdot \frac{n-2}{n} \cdot ...\cdot \frac{2}{n} \cdot \frac{1}{n}     
        \]
        Notiamo subito che il primo termine $\frac{n}{n} = 1$ quindi possiamo toglierlo, in più tutti i termini sono $\leq 1$, quindi 
        \begin{align*}
            \frac{n-1}{n} \cdot \frac{n-2}{n} \cdot ...\cdot \frac{2}{n} \cdot \frac{1}{n} &\leq 1 \cdot \frac{n-2}{n} \cdot ...\cdot \frac{2}{n}  \cdot \frac{1}{n} \\ 
            &\leq 1 \cdot 1 \cdot ... \cdot \frac{2}{n} \cdot \frac{1}{n} \\
            &\leq ... \\
            &\leq 1 \cdot 1 \cdot 1 \cdot 1 \cdot \frac{1}{n} \\
            &\leq \frac{1}{n}
        \end{align*}
        Quindi abbiamo scoperto che
        \[
            0 \leq \frac{n!}{n^n} \leq \frac{1}{n}
        \]
        Visto che $\displaystyle\lim_{n\to+\infty}\frac{1}{n} = 0$, per il teorema dei carabinieri
        \[
        \lim_{n\to+\infty} \frac{n!}{n^n}= 0
        \]

        $(iii)$ Come prima, sappiamo che $a^n \geq 0$ e che $n! \geq 0$ e quindi $\frac{a^n}{n!} \ge 0$. Visto che $a>1$ allora $\exists N \in \mathbb{N}$ tale che $N>a$. Quindi potremo riscrivere la sequenza con $n\geq N$ come        
        \[
        \frac{a^n}{n!} =  \mathrect{red}{\frac{a}{1} \cdot \frac{a}{2} \cdot \frac{a}{3} \cdot ... \frac{a}{N-1} \cdot \frac{a}{N}} \cdot \mathrect{blue}{\frac{a}{N+1} \cdot .. \frac{a}{n-1}} \cdot \frac{a}{n} 
        \]
        Notiamo che tutti i termini nel riquadro rosso sono tutti $\leq a$, dato che $N>1$, mentre tutti i termini nel riquadro blu sono $\leq 1$ perchè $N\ge a \implies 1 \ge \frac{a}{N}$, quindi
        \begin{align*}
        \frac{a^n}{n!}  &\leq \mathrect{red}{a \cdot a \cdot a \cdot ... \cdot a \cdot a } \cdot \mathrect{blue}{1 \cdot ... \cdot 1} \cdot \frac{a}{n} \\    
            &\leq a^N \cdot 1 \cdot \frac{a}{n} \\
            &\leq \frac{a^{N+1}}{n}
    \end{align*}
        Quindi abbiamo scoperto che
        \[
            0 \leq \frac{a^n}{n!} \leq \frac{a^{N+1}}{n}
        \]
        Visto che $a^{N+1}$ è un numero ben definito e non è dipendente da $n$ allora $\displaystyle \lim_{n\to+\infty} \frac{a^{N+1}}{n} = 0$, e quindi per il teorema dei carabinieri 
        \[
        \lim_{n\to+\infty} \frac{n!}{n^n}= 0
        \]
        
    \end{proof}

\addcontentsline{toc}{subsection}{Formula di Stirling}
    \begin{teorema}{Formula di Stirling}{}
        \[
         n! \sim \left(\frac{n}{e}\right)^n \sqrt{2\pi n} \;\;\;\;\; \text{ per } n\to+\infty
        \]
    \end{teorema}
 Vediamo qualche esercizio per allenarci con questa formula
    \begin{esercizio}{}{}
        \[
        \lim_{n\to +\infty} \sqrt[n]{n!} 
        \]
    \end{esercizio}
    \begin{proof}
        Visto che è una equivalenza asintotica possiamo sistutire $n!$
        \[
        \lim_{n\to +\infty} \sqrt[n]{n!} = \lim_{n\to +\infty} \sqrt[n]{ \left(\frac{n}{e}\right)^n \sqrt{2\pi n}} 
        \]
        Facciamo qualche riarrangiamento
        \begin{align*}
             \lim_{n\to +\infty} \sqrt[n]{ \left(\frac{n}{e}\right)^n \sqrt{2\pi n}}  &= \lim_{n\to +\infty} \sqrt[n]{ \left(\frac{n}{e}\right)^n} \cdot \sqrt[n]{ \sqrt{2\pi n}} \\ 
          &= \lim_{n\to +\infty} \left(\frac{n}{e}\right) \cdot \sqrt[2n]{2\pi n} \\
          &= e^{-1} \lim_{n\to +\infty} n\sqrt[2n]{2\pi n}
        \end{align*}
        
        Ora possiamo usare la tecnica delle potenze di funzioni e  applichiamo le regole dei logaritmi
        \begin{align*}
            \lim_{n\to +\infty} n\sqrt[2n]{2\pi n} &= \lim_{n\to +\infty} e^{\log(n\sqrt[2n]{2\pi n})}\\
                &= \lim_{n\to +\infty} e^{\log(n)+ \log((2\pi n)^\frac{1}{2n})} \\ 
                 &= \lim_{n\to +\infty} e^{\log(n)+ \frac{1}{2n}\log(2\pi n)} \\ 
                 &= \lim_{n\to +\infty} e^{\log(n)+ \frac{1}{2}\left(\frac{\log(2)}{n} +\frac{\log(\pi)}{n} +\frac{\log(n)}{n} \right)} 
        \end{align*}
         Ora i termini $\frac{\log(2)}{n}$ e $\frac{\log(\pi)}{n}$ è facile vedere che tendono a 0, poi per la gerarchia degli infiniti anche $\frac{\log(n)}{n}\to 0$, mentre il termine $\log(n) \to +\infty$, quindi l'esponente complessivamente tende a $+\infty$, quindi l'esponenziale tende anche lui a $+\infty$, e di conseguenza 
         \[
         \lim_{n\to +\infty} \sqrt[n]{n!}  = +\infty
         \]
    \end{proof}
\newpage

\addcontentsline{toc}{subsection}{Criterio di convergenza per le Successione}
    \begin{teorema}{Criterio di convergenza per le Successione}{}
        Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ e $(a_n)_{n\in A}$ definitivamente positiva allora se 
        \[
            \exists \lim \frac{a_{n+1}}{a_n} = l \in [0, +\infty) \cup \{+\infty\}
        \]
        allora 
        \begin{enumerate}[label=(\roman*)]
            \centering
            \item $l \in [0, 1) \implies \lim a_n = 0 $
            \item $l \in (1, +\infty) \cup \{+\infty\} \implies \lim a_n = +\infty$
            \item $l = 1 \implies$ nulla si può dire
        \end{enumerate}
    \end{teorema}

    \begin{proof}
        Per dimostrare dobbiamo riprendere la definizione di limite, infatti se esiste il limite allora sappiamo che
        \[
            \lim \frac{a_{n+1}}{a_n} = l \iff \frac{a_{n+1}}{a_n} \in  (l-\varepsilon, l+\varepsilon) \;\;\;\;\; \forall x \in I
        \]
        Riscriviamo il termine
        \[
        \frac{a_{n+1}}{a_n} \in  (l-\varepsilon, l+\varepsilon) \iff l-\varepsilon < \frac{a_{n+1}}{a_n} < l+\varepsilon \implies (l-\varepsilon)a_n < a_{n+1}< (l+\varepsilon)a_n 
        \]
        $(i)$ Per dimostrare il punto $(i)$ ci basta prendere un qualsiasi $N \in \mathbb{N}$ e la disequazione  $\mathunderline{red}{a_{N+1}\leq (l+\varepsilon)a_N }$, infatti dato che è vera possiamo dire anche che $\mathunderline{blue}{a_{N+2}\leq (l+\varepsilon)a_{N+1} }$ e anche $a_{N+3}\leq (l+\varepsilon)a_{N+2} $ e così via, e possiamo dire che  %e seguendo questo ragionamento possiamo dire che  che $a_{n+k+1}< (l+\varepsilon)a_{n+k} $
        \begin{align*}
            a_{N+3} \leq(l+\varepsilon)\mathunderline{blue}{a_{N+2} \leq } & (l+\varepsilon)\mathunderline{blue}{(l+\varepsilon)a_{N+1}} \\
             \leq &(l+\varepsilon)^2a_{N+1}   \\
            a_{N+3} \leq (l+\varepsilon)^2\mathunderline{red}{a_{N+1} \leq} & (l+\varepsilon)^2\mathunderline{red}{(l+\varepsilon)a_{N}} \\
             \leq &(l+\varepsilon)^3a_{N}   \\
            a_{N+3}  \leq &(l+\varepsilon)^3a_{N}
        \end{align*}
        Seguendo questo ragionamento possiamo dire che 
        \[
            a_{N+k} \leq  (l+\varepsilon)^k a_N \;\;\;\;\; \forall k \in \mathbb{N}
        \]
        Ora noi per ipotesi sappiamo che $l \in [0, 1)$, quindi se scegliamo $\varepsilon \in [0, 1-l)$ in modo tale che il termine $(l+\varepsilon)< 1$. Ricordiamo anche che la funzione, per ipotesi, è definitivamente positiva, quindi sarà vero che $a_{N+k}\geq 0$. Ora se mandiamo al limite la seguente disequazione 
        \[
             0 \leq \lim_{k \to +\infty} a_{N+k} \leq \lim_{k \to +\infty} (l+\varepsilon)^k a_N
        \]
        Scopriamo che il termine di destra tende a 0 per le regole degli esponenziali, e quindi complessivamente il termine di destra $0\cdot a_N \to 0$, visto che $a_N$ è un numero finito. In più nel termine centrale possiamo togliere $N$ visto che $k \sim k + N$ per $k\to+\infty$, pertanto per il teorema dei due carabinieri abbiamo che 
        \[
        \lim_{k \to +\infty} a_{k} = 0
        \]
        
        $(ii)$ Per dimostrare il secondo punto ci serve riprendere la disequazione impostata all'inizio della dimostrazione 
        \[
        (l-\varepsilon)a_n < a_{n+1}
        \]
        Ora per lo stesso ragionamento del punto scorso possiamo scegliere un $N \in \mathbb{N}$ e potremo dire che
        \[
         (l-\varepsilon)^k a_N \leq  a_{N+k} \;\;\;\;\; \forall k \in \mathbb{N}
        \] 
        Se scegliamo un $\varepsilon \in [0, l-1)$ avremo che $l-\varepsilon > 1$ e quindi se lo portiamo al limite avremo che $(l-\varepsilon)^k \to +\infty$, e di conseguenza per il corollario del teorema dei due carabinieri abbiamo che 
        \[
        \lim_{k \to +\infty} a_{k} = +\infty
        \] 
        \text{N.B.} è impossibile che  $a_N=0$, infatti $a_N$ è il termine generale di una serie, quindi ammeno che non sia la serie $a_N = 0$ che in quel caso converge, non ci sono problemi che venisse fuori una forma indeterminata $\bigl[\infty \cdot 0\bigr]$.
    \end{proof}


\addcontentsline{toc}{subsection}{Teorema dell'Esistenza del limite di funzioni Monotone}
    \begin{teorema}{Esistenza del limite di funzioni Monotone}{}
        Sia $A = \{n \in \mathbb{N} : n \geq n_0\}$, $a:A \to \mathbb{R}$ e $(a_n)_{n\in A}$ definitivamente monotona allora $\displaystyle\lim a_n \in \mathbb{R} \cup \{\pm \infty\}$ e possiamo dire anche che 
        \begin{itemize}
            \item Se  $(a_n)_{n\in A}$ è monotona crescente allora
            \[
                \lim a_n = \sup\{a_n : n \in A\}
            \]
            \item Se  $(a_n)_{n\in A}$ è monotona decrescente allora
            \[
                \lim a_n = \inf\{a_n : n \in A\}
            \]
        \end{itemize}
        
    \end{teorema}
        Questo teorema è la versione del Teorema del limite di funzioni monotone applicato alle successioni, quindi non serve la dimostrazione perchè è la stessa delle funzioni.
    

\addcontentsline{toc}{subsection}{Definizione di Serie Numerica}
    \begin{definizione}{Serie Numeriche}{}
        Sia $(a_n)_{n\in\mathbb{N}}$ un successione a valori reali, definiamo la successione delle \textbf{somme parziali} $(S_k)_{k \in \mathbb{N}}$ definita come
        \[
            S_k = \sum_{n=1}^{k} a_n
        \]
        Diciamo "\textbf{serie numerica con termine generale $a_n$}" il limite della successione delle somme parziali, e la indichiamo come
        \[
            \sum_{n=1}^{+\infty} a_n = \lim_{k\to +\infty} S_k \in \mathbb{R} \cup \{\pm \infty\}
        \]
    \end{definizione}
    
    Visto che $S_k$ è una successione, allora anche nelle serie ereditiamo i termini già visti: \textbf{Convergenza},\textbf{Divergenza} e \textbf{Irregolare}
        
\addcontentsline{toc}{subsection}{Convergenza, Divergenza e Irregolarità delle Serie}
    \begin{definizione}{Convergenza, Divergenza e Irregolarità delle Serie}{}
        Sia $(a_n)_{n\in \mathbb{N}}$ una successione, allora diciamo che la serie numerica $\displaystyle\sum_{n=1}^{+\infty} a_n $ è
         \begin{itemize}
        \item Convergente se  $\lim  S_k = l\in \mathbb{R}$
        \item Divergente se $\lim  S_k = l\in \{\pm \infty\}$
        \item Irregolare se $\nexists\lim  S_k$  
    \end{itemize}
     
    \end{definizione}
    
\addcontentsline{toc}{subsection}{Serie Geometrica}
    \begin{esempio}{}{}
        Se prendiamo $a_n = r^n$, con $r \in \mathbb{R}$, per induzione si può dimostrare che la successione delle somme parziale è definita come 
        \[
        S_k = \sum_{n=1}^{k} r^n = \frac{1-r^{k+1}}{1-r} 
        \]
        Pertanto se portiamo tutto al limite abbiamo che 
        \begin{center}
            $\displaystyle\sum_{n=1}^{k} r^n = \begin{cases}
                \frac{1}{1-r} &\text{se } r \in (-1, 1)\\
                +\infty &\text{se } r \geq 1 \\ 
                \nexists &\text{se } r \leq 1 \\ 
            \end{cases}$
        \end{center}
        
        E questa è detta \textbf{Serie Geometrica}.
    \end{esempio}

\addcontentsline{toc}{subsection}{Serie Armonica Generalizzata}
    \begin{teorema}{Serie Armonica Generalizzata }{}
        Dato $\alpha \in [0, +\infty)$ allora con \textbf{Serie Armonica Generalizzata} intendiamo la serie
        \begin{center}
            $\displaystyle\sum_{n=1}^{+\infty} \frac{1}{n^\alpha} = \begin{cases}
                \text{Divergente a } +\infty & \text{se } \alpha \in [0,1] \\
                \text{Convergente}  & \text{se } \alpha > 1 
            \end{cases}$
        \end{center}
    \end{teorema}
    
    \begin{proof}
        (caso $\alpha \in [0,1)$) Scriviamo le somme parziali per capire meglio
        \[
        S_k = \sum_{n=1}^{k} \frac{1}{n^\alpha}= 1 + \frac{1}{2^\alpha} + \frac{1}{3^\alpha} + ... + \frac{1}{k^\alpha}
        \]
        Notiamo che $\forall n\leq k$ vale $\frac{1}{n^\alpha} \geq \frac{1}{k^\alpha}$, quindi possiamo dire che 
         \begin{align*}
            1 + \frac{1}{2^\alpha} + \frac{1}{3^\alpha} + ... + \frac{1}{k^\alpha} &\geq \frac{1}{k^\alpha}+ \frac{1}{k^\alpha} + \frac{1}{k^\alpha} + ... + \frac{1}{k^\alpha} \\
            &= \frac{k}{k^\alpha} = k^{1-\alpha}
         \end{align*}
         Dato che $\alpha \in [0,1)$, appiamo che $1-\alpha > 0$, e quindi se portiamo la sommatoria al limite abbiamo che $k^{1-\alpha} \to +\infty$, e visto che la serie armonica è maggiore di una somma divergente, anche lei diventa divergente.

        (caso $\alpha = 1$)  Notiamo che  la sommatoria $S_k$ è monotona crescente, infatti
        \begin{align*}
           S_{k+1} = \sum_{n=1}^{k+1} \frac{1}{n} &=  \left(\sum_{n=1}^{k} \frac{1}{n}\right) + \frac{1}{k+1} \\ 
         S_{k+1} &= S_k + \frac{1}{k+1}
        \end{align*}
        E notiamo che $\frac{1}{k+1} > 0$, $\forall k > 0$, quindi
       \begin{align*}
         S_{k+1} &= S_k + \frac{1}{k+1} > S_k  + 0\\ 
         S_{k+1}&> S_k
       \end{align*}
       E pertanto la serie è monotona crescente, e visto che la somma delle serie parziali è una successione, per il teorema dell'esistenza di funzioni monotone sappiamo che il limite esiste e deve valere $S \in [1, +\infty] \cup \{+\infty\}$. Osserviamo che
\begin{align*}
    S_{2k} - S_k &= \left(\sum_{n=1}^{2k} \frac{1}{n}\right) - \left(\sum_{n=1}^{k} \frac{1}{n}\right) \\
    &=  \left(\sum_{n=1}^{k} \frac{1}{n}\right) + \left(\sum_{n=k+1}^{2k} \frac{1}{n}\right) - \left(\sum_{n=1}^{k} \frac{1}{n}\right) \\
    &=    \sum_{n=k+1}^{2k} \frac{1}{n} =    \frac{1}{k+1} + \frac{1}{k+2} + \frac{1}{k+3} + ... + \frac{1}{2k} 
\end{align*}
    Come per il caso $\alpha \in [0,1)$, vediamo che $\frac{1}{k+n} \geq \frac{1}{2k}$, $\forall n\leq k$ e quindi
    \begin{align*}
        \frac{1}{k+1} + \frac{1}{k+2} + \frac{1}{k+3} + ... + \frac{1}{2k}  &\geq \frac{1}{2k} + \frac{1}{2k} + \frac{1}{2k} + ... + \frac{1}{2k} \\
        &=  k \cdot \frac{1}{2k} = \frac{1}{2}
    \end{align*}
    Quindi abbiamo scoperto che $S_{2k} - S_k \geq \frac{1}{2}$, quindi sappiamo anche che $S_{2k} \geq S_k + \frac{1}{2}$, ma se supponiamo che  $S\ne +\infty$ vediamo che per $S_{2k} \to S$ e anche $S_k \to S$ e quindi 
    \begin{align*}
        S_{2k}  &\geq S_k +\frac{1}{2}\\
        S &\geq S + \frac{1}{2} \\
        0 &\geq \frac{1}{2}
    \end{align*} 
    Chiaramente è impossibile che $0 \geq \frac{1}{2}$, e quindi vuole dire che l'ipotesi che $S \ne +\infty$ è sbagliata e di conseguenza abbiamo scoperto che $S = +\infty$ e che quindi la serie $\displaystyle\sum_{n=1}^{+\infty} \frac{1}{n}$ è divergente. 
    
\newpage
    (caso $\alpha > 1$) Come per il caso $\alpha = 1$ possiamo dire che la successione delle serie parziali è monotona crescente
    \[
     S_{k+1} = S_k + \frac{1}{(k+1)^\alpha} \geq S_k\;\;\;\;\; \forall k \in \mathbb{N}
    \]
    E di conseguenza il limite esiste e varrà $S \in [1, +\infty) \cup \{+\infty\}$, notiamo che
    \begin{align*}
        S_{2k+1} &= 1 + \frac{1}{2^\alpha} + \frac{1}{3^\alpha} + \frac{1}{4^\alpha} + \frac{1}{5^\alpha} + ... + \frac{1}{(2k)^\alpha} + \frac{1}{(2k+1)^\alpha} \\
        &= 1 + \left(\frac{1}{2^\alpha} + \frac{1}{3^\alpha}\right) + \left(\frac{1}{4^\alpha} + \frac{1}{5^\alpha}\right) + ... + \left(\frac{1}{(2k)^\alpha} + \frac{1}{(2k+1)^\alpha}\right)  \\
        &= 1+ \sum_{n=1}^{k} \left(\frac{1}{(2n)^\alpha} +\frac{1}{(2n+1)^\alpha} \right)
    \end{align*}
    
    Sappiamo che $\frac{1}{(2n+1)^\alpha} \le \frac{1}{(2n)^\alpha}$, $\forall n \in \mathbb{N}$ e quindi 
    \begin{align*}
        \sum_{n=1}^{k} \left(\frac{1}{(2n)^\alpha} +\frac{1}{(2n+1)^\alpha} \right) &\leq \sum_{n=1}^{k} \left(\frac{1}{(2n)^\alpha} +\frac{1}{(2n)^\alpha} \right) \\
        &= \sum_{n=1}^{k} \frac{2}{(2n)^\alpha} \\
        &= \sum_{n=1}^{k} \frac{2^{1-\alpha}}{n^\alpha} \\
        &= 2^{1-\alpha}\sum_{n=1}^{k}\frac{1}{n^\alpha} 
    \end{align*}
    Quindi abbiamo scoperto che 
    \[
        S_{2k+1} \leq 1 + 2^{1-\alpha}\sum_{n=1}^{k}\frac{1}{n^\alpha} 
    \]  
    Però ricordiamo che $\displaystyle\sum_{n=1}^{k}\frac{1}{n^\alpha} = S_k$ e quindi 
    \[
     S_{2k+1} \leq 1 + 2^{1-\alpha} S_k
    \]
    Prima abbiamo detto che la sommatoria ha limite visto che è monotona crescente, quindi supponiamo che $S \in [1, +\infty)$, pertanto se portiamo tutto al limite abbiamo che $S_{2k+1} \to S$ e $S_k \to S$.
    \[
        S \leq 1+2^{1-\alpha} S
    \]
    \[
        S \leq \frac{1}{1-2^{1-\alpha}}
    \]
    Pertanto abbiamo che il limite delle somme parziali è minore di un certo valore finito, quindi la serie converge.
   \end{proof}

   
\addcontentsline{toc}{subsection}{Linearità delle Serie}
    \begin{teorema}{Linearità delle Serie}{}
        Se $\displaystyle \sum_{n=1}^{+\infty} a_n$ e $\displaystyle \sum_{n=1}^{+\infty} b_n$ convergono allora anche $\displaystyle \sum_{n=1}^{+\infty}(c\cdot a_n +  d\cdot b_n)$ converge $(\forall c, d \in \mathbb{R})$. 
    \end{teorema}

    \begin{proof}
        Per dimostrarlo è necessario usare l'algebra dei limiti finiti, infatti se, per ipotesi, $\displaystyle \sum_{n=1}^{+\infty} a_n = A$ e $\displaystyle \sum_{n=1}^{+\infty} b_n = B$, allora sappiamo che 
        \begin{align*}
            \sum_{n=1}^{+\infty}(c\cdot a_n +  d\cdot b_n) &= \lim_{k\to +\infty} \sum_{n=1}^{k}(c\cdot a_n +  d\cdot b_n) \\
            &= \lim_{k\to +\infty} \sum_{n=1}^{k}(c\cdot a_n) +  \lim_{k\to +\infty} \sum_{n=1}^{k}(d\cdot b_n) \\
            &=\lim_{k\to +\infty} c\sum_{n=1}^{k}a_n +  \lim_{k\to +\infty} d\sum_{n=1}^{k}b_n \\
            &= c\cdot A + d\cdot B
        \end{align*}
        E chiaramente, visto che $A$ e $B$ sono dei numeri finiti, allora anche $c\cdot A + d\cdot B$ converge, e pertanto la sommatoria della combinazione lineare è convergente.
    \end{proof}
   
    
   
   \begin{teorema}{Termine Generale in funzione dalla  relativa Serie Numerica}{}
    Dato $(S_k)_{k \in \mathbb{N}}$ una successione delle somme parziali di una successione $(a_n)_{n \in \mathbb{N}}$, allora possiamo trovare l'espressione analitica del termine generale di $(a_n)_{n \in \mathbb{N}}$ come
    \[
        a_n = S_k - S_{k-1}
    \]
    \end{teorema}


\addcontentsline{toc}{subsection}{Condizione Necessaria per la Convergenza di una Serie}
\begin{teorema}{Condizione Necessaria per la Convergenza}{}
        Sia $(a_n)_{ n\in \mathbb{N}}$ una successione, e $(S_k)_{k \in \mathbb{N}}$ la successione delle somme parziali, allora se $(S_k)_{k \in \mathbb{N}}$ è convergente allora 
        \[
            \lim_{n\to +\infty} a_n = 0
        \]
    \end{teorema}

\begin{proof}
    Usando il  termine Generale in funzione dalla  relativa Serie Numerica sappiamo che $a_n = S_k - S_{k-1}$, ma visto che per ipotesi $S_k$ converge a un numero ($S$), allora $S_k \to S$ e anche $S_{k-1} \to S$ pertanto se portiamo quell'informazione al limite abbiamo che
    \begin{align*}
         \lim_{n\to +\infty} a_n &=\lim_{n\to +\infty}( S_k - S_{k-1}) \\
         \lim_{n\to +\infty} a_n &=S - S \\
          \lim_{n\to +\infty} a_n &= 0
    \end{align*}  
\end{proof}

Negli esercizi dovremmo trovare il carattere di una serie, quindi per come è posto questo teorema, non ci è molto utile. Però per le regole della implicazione logica, sappiamo anche che
\begin{center}
    $\displaystyle \lim_{n\to +\infty} a_n\ne 0 \implies \sum_{n=1}^{+\infty} a_n$ non converge.
\end{center} 

\textbf{N.B.} dall'implicazione sappiamo che la serie non converge, cioò vuol dire che o diverge o è irregolare, quindi attensione a non dire che diverge, perchè potrebbe essere irregolare. Per controllare se è divergente o convergente basta controllare se la serie è monotona, e in quel caso allora la serie è divergente.
    
Però se $\displaystyle \lim_{n\to +\infty} a_n = 0$, NON possiamo dire nulla, quindi per questi casi è necessario usare altri criteri. 
Faccendo un esempio:  sia $a_n = \frac{1}{n}$ che $b_n = \frac{1}{n^2}$ abbiamo che
\[
\begin{array}{c @{\qquad} c}
    \displaystyle\lim_{n\to +\infty} a_n =  \lim_{n\to +\infty} \frac{1}{n} = 0 & \displaystyle\lim_{n\to +\infty} b_n =  \lim_{n\to +\infty} \frac{1}{n^2} = 0
\end{array}
\]

Però abbiamo visto con la serie armonica generalizzata abbiamo che $\displaystyle \sum_{n=1}^{+\infty} \frac{1}{n}$ diverge, mentre la serie $\displaystyle \sum_{n=1}^{+\infty} \frac{1}{n^2}$ converge. 

\addcontentsline{toc}{subsection}{Definizione di Serie a Termini Positivi}
\begin{definizione}{Serie a Termini Positivi}{}
    Sia $(a_n)_{ n\in \mathbb{N}}$, se $a_n \geq 0$ $\forall n \in \mathbb{N}$ oppure $\forall n\geq \bar{n}$, allora diciamo che la serie $\displaystyle \sum_{n=1}^{+\infty} a_n$ è a \textbf{termini positivi} oppure a \textbf{termini definitivamente positivi}. 
\end{definizione}
\begin{teorema}{Proprietà delle Serie a Termini Positivi}{}
    Se $\displaystyle \sum_{n=1}^{+\infty} a_n$ è a termini positivi, allora ha limite $S \in [0, +\infty) \cup \{+\infty\}$.
\end{teorema}

\begin{proof}
    Visto che la serie è a termini definitivamente positivi, allora sappiamo che $a_n \geq 0$, $\forall n \geq \bar{n}$ allora sappiamo che
    \[
        S_{k} = S_{k-1} + a_{k} \geq S_{k-1} \;\;\;\;\; \forall n \geq \bar{n}
    \]
    \[
    S_{k} \geq S_{k-1}
    \]
    Abbiamo scoperto che se la serie è a termini positivi allora è anche anche monotona crescente, e dato che sappiamo che se una serie è monotona crescente allora ha limite, di conseguenza anche se una serie è a termini positivi avrà limite convergente o divergente.
\end{proof}

\addcontentsline{toc}{subsection}{Criterio del Confronto delle Serie}
\begin{teorema}{Criterio del Confronto delle Serie}{}
    Sia $(a_n)_{ n\in \mathbb{N}}$ e $(b_n)_{ n\in \mathbb{N}}$ due successioni tali che $0 \leq a_n \leq b_n$ abbiamo che 
    \begin{enumerate}[label=(\roman*)]
        \item se $\displaystyle  \sum_{n=1}^{+\infty} b_n$ converge a $B\in \mathbb{R}$ allora anche $\displaystyle  \sum_{n=1}^{+\infty} a_n$ converge a $A\in \mathbb{R}$ con $A \leq B$.
        \item se $\displaystyle  \sum_{n=1}^{+\infty} a_n$ diverge a $+\infty$ allora anche $\displaystyle  \sum_{n=1}^{+\infty} b_n$ diverge a $+\infty$
    \end{enumerate}
\end{teorema}

\begin{proof}
    Per dimostrarlo è necessario usare le somme parziali, infatti siano
    \[
    \begin{array}{c @{\qquad} c}
        \displaystyle A_k =  \sum_{n=1}^{k} a_n & \displaystyle B_k =  \sum_{n=1}^{k} b_n 
    \end{array}
    \]

    $(i)$  Poi dato che $a_n \leq b_n$ allora è vero anche che $A_k \leq B_n$, pertanto per il teorema di relazione d'ordine sappiamo che se $\displaystyle \sum_{n=1}^{+\infty} b_n= B$ allora   $\displaystyle  \sum_{n=1}^{+\infty} a_n = A$, in più, sempre per il teorema delle relazioni d'ordine sappiamo che $A \leq B$

     $(ii)$ Per dimostrare questo punto invece è necessario usare il corollario del teorema dei carabinieri, infatti se $A_k$ diverge allora anche $B_k$ diverge.
\end{proof}

\begin{esempio}{}{}
    Determinare il carattere della seguente serie
    \[
     \sum_{n=1}^{+\infty} \frac{1}{n(n+1)}
    \]
\end{esempio}

Intanto notiamo che assomiglia molto alla serie $\sum \frac{1}{n^2}$, quindi proviamo a vedere se una è maggiore dell'altra
\[
    \frac{1}{n(n+1)} \leq \frac{1}{n^2}
\]
\[
    n^2 \leq n^2+n
\]
\[
    0 \leq n
\]
Quindi sappiamo che $\frac{1}{n(n+1)} \leq \frac{1}{n^2}$ e quindi per il criterio del confronto sappiamo che se $\sum \frac{1}{n^2}$ convergesse allora anche $\sum \frac{1}{n(n+1)}$ converge. Però abbiamo visto con le serie armoniche che $\sum \frac{1}{n^2}$ converge, visto che l'esponente $2>1$ e di conseguenza anche $\displaystyle\sum_{n=1}^{+\infty} \frac{1}{n(n+1)}$ converge. Per questa serie lo potevamo scoprire anche usando le serie telescopische, infatti $\frac{1}{n(n+1)} = \frac{1}{n} - \frac{1}{n+1}$ e quindi $\displaystyle\lim\sum_{n=1}^{k} \frac{1}{n(n+1)} = \lim_{k\to+\infty} \left(1 - \frac{1}{k+1}\right) = 1$, e infatti la serie converge a 1.

\addcontentsline{toc}{subsection}{Definizione Serie Assolutamente Convergente}
\begin{definizione}{Serie Assolutamente Convergente}{}
    Sia $(a_n)_{ n\in \mathbb{N}}$ una successione, la serie $ \displaystyle \sum_{n=1}^{+\infty} a_n$ si dice \textbf{Assolutamente Convergente} se la serie $\displaystyle \sum_{n=1}^{+\infty} |a_n|$ converge. 
\end{definizione}

\begin{teorema}{Relazione Convergenza Assoluta e Semplice}{}
    Sia $(a_n)_{ n\in \mathbb{N}}$ una successione, la serie $ \displaystyle \sum_{n=1}^{+\infty} a_n$  è assolutamente convergente, allora è anche converge anche semplicemente.
\end{teorema}
\begin{proof}
    Proviamo a sviluppare la seguente serie, e applichiamo k-volte la diseguaglianza triangolare
    \begin{align*}
        \left|\sum_{n=1}^{k}a_n\right| &= |a_1 + a_2 + a_3 + ... + a_k| \\
        &\leq |a_1 |+ |a_2 + a_3 + ... + a_k|  \\
        &\leq |a_1 |+ |a_2| + |a_3 + ... + a_k|  \\
        &\leq ...  \\
        &\leq |a_1 |+ |a_2| + |a_3| + ... + |a_k|  \\
        \left|\sum_{n=1}^{k}a_n\right| &\leq \sum_{n=1}^{k} |a_n| 
    \end{align*}
    Pertanto se, per ipotesi, $\sum |a_n|$ converge a $S\in \mathbb{R}^+$, allora se portiamo al limite la disequazione abbiamo che  
    \[
    \left|\sum_{n=1}^{+\infty}a_n\right| \leq S \iff  \sum_{n=1}^{+\infty}a_n \in [-S, S]
    \]
    Però visto che $S$ è un numero finito, e sappiamo che la serie è compresa tra due valori finiti, allora di conseguenza anche la serie $\displaystyle\sum_{n=1}^{+\infty} a_n$ converge semplicemente.
\end{proof}

\textbf{N.B.} la covergenza assoluta implica la convergenza semplice, ma NON è vero il viceverca, vediamolo con un esempio.

\begin{esempio}{}{}
    Determinare il carattere della serie
    \[
        \sum_{n=1}^{+\infty} \frac{(-1)^n}{n}
    \]
\end{esempio}

Notiamo subito che la serie non converge assolutamente infatti 
\[
\left|\frac{(-1)^n}{n}\right| = \frac{1}{n}
\]
Che per la serie armonica sappiamo che diverge, quindi sappiamo che la serie $\sum \frac{(-1)^n}{n}$ diverge assolutamente. Però ora proviamo a vedere se converge o diverge semplicemente, per farlo proviamo analizando la serie delle somme parziali
\begin{align*}
    S_{2k} &= \frac{(-1)^1}{1} + \frac{(-1)^2}{2} + \frac{(-1)^3}{3} + \frac{(-1)^4}{4} + ... + \frac{(-1)^{2k-1}}{2k-1} +\frac{(-1)^{2k}}{2k}\\
    &= \left(1+ \frac{1}{2}\right) + \left(\frac{-1}{3}+ \frac{1}{4}\right)  + ... + \left(\frac{-1}{2k-1} +\frac{1}{2k}\right) \\
    &= \sum_{n=1}^{k} \left(\frac{-1}{2n-1} + \frac{1}{2n}\right)\\
    &= \sum_{n=1}^{k} \frac{-2n + 2n -1}{(2n-1)2n} =  -\sum_{n=1}^{k} \frac{1}{(2n-1)2n}
\end{align*}

Ora questa serie assomiglia molto alla serie armonica con $\alpha = 2$ che converge, quindi proviamo a vedere se può funzionare il criterio del confronto
\[
     \frac{1}{(2n-1)2n} \leq \frac{1}{n^2} \iff n^2 \leq 4n^2 - 2n
\]
\[
    \iff 2n \leq 3n^2 \iff 2 \leq 3n \iff n \geq \frac{2}{3}
\]
Quindi sappiamo che la nostra serie è minore della serie armonica, pertanto la serie $\sum \frac{(-1)^n}{n}$ converge semplicemente, ma diverge assolutamente.

\addcontentsline{toc}{subsection}{Criterio del Confronto Asintotico}
\begin{teorema}{Criterio del Confronto Asintotico}{}
    Siano $(a_n)_{ n\in \mathbb{N}}$ e $(b_n)_{ n\in \mathbb{N}}$ due successioni definitivamente positive tali che
    \[
        \exists \lim_{n\to +\infty} \frac{a_n}{b_n} = l \in [0, +\infty) \cup \{+\infty\}
    \]
    Allora 
    \begin{enumerate}[label=(\roman*)]
        \item se $l \in [0, +\infty)$ allora $\sum a_n$ e $\sum b_n$ hanno lo stesso carattere.
        \item se $l=0$ allora valgono:
        \begin{itemize}
            \item se $\sum b_n$ converge allora $\sum a_n$ converge
            \item  se $\sum a_n$ diverge allora $\sum b_n$ diverge
        \end{itemize}
        \item se $l=+\infty$ allora valgono:
        \begin{itemize}
            \item se $\sum a_n$ converge allora $\sum b_n$ converge
            \item  se $\sum b_n$ diverge allora $\sum a_n$ diverge
        \end{itemize}
    \end{enumerate}
\end{teorema}

\begin{proof}
    $(i)$ Usiamo la definizione di limite
    \[
        l-\varepsilon < \frac{a_n}{b_n} < l+\varepsilon \;\;\;\;\; \forall n \geq N
    \]
    Ora per comodità possiamo scegliere $\varepsilon = \frac{l}{2}$
    \[
        l-\frac{l}{2} < \frac{a_n}{b_n} < l+\frac{l}{2}
    \]
    \[
    \frac{l}{2} < \frac{a_n}{b_n} < \frac{3l}{2}
    \]
    \[
    \frac{l}{2}b_n< a_n < \frac{3l}{2}b_n
    \]
    E quindi 
    \[
        \mathunderline{red}{\frac{l}{2}\sum_{n=N}^{k}b_n < \sum_{n=N}^{k}} \mathunderline{blue}{a_n < \frac{3l}{2}\sum_{n=N}^{k}b_n}
    \]
    E quindi dalla disequazione sottolineata di rosso sappiamo, grazie al criterio del confronto, se $\sum b_n$ diverge allora $\sum a_n$ diverge, mentre con la disequazione in blu abbiamo che se $\sum b_n $ converge allora anche $\sum a_n$ converge. Pertanto notiamo che le due serie hanno sempre lo stesso carattere. 

    \textbf{N.B.} non possono essere irregolari visto che per ipotesi devono essere a termini positivi, che ricordiamo che per le proprietà delle serie a termini positivi, non possono essere irregolari. 

    $(ii)$ Usiamo la definizio di limite
    \[
         -\varepsilon < \frac{a_n}{b_n} < +\varepsilon \;\;\;\;\; \forall n \geq N
    \]
    Poi dato che $a_n$ e $b_n$ sono definitivamente positivi abbiamo che $\frac{a_n}{b_n} > 0$, in più possiamo scegliere $\varepsilon = 1$ e quindi abbiamo
    \[
    0 < \frac{a_n}{b_n} < 1 \implies a_n < b_n
    \]
    Pertanto $\sum a_n < \sum b_n$ e quindi per il teorema del confronto verifichiamo il teorema.

     $(iii)$ Usiamo la definizione di limite
     \[
     \frac{a_n}{b_n} > M \;\;\;\;\; \forall n \geq N
     \]
     Pertanto possiamo dedurre che
     \[ 
     a_n > M\cdot b_n \implies \sum a_n > M \sum b_n
     \]
     Ed ora possiamo il teorema del confronto per dimostrare questo punto.
\end{proof}
\newpage
\begin{esercizio}{}{}
    Determina il carattere della seguente serie
    \[
    \sum_{n=1}^{+\infty} \frac{\sqrt{n^2+1}-n}{n}
    \]
\end{esercizio}

\begin{proof}
    In primis proviamo a controllare la condizione necessaria per la convergenza
    \[
        \lim_{n\to+\infty} \frac{\sqrt{n^2+1}-n}{n} = \lim_{n\to+\infty} \frac{\sqrt{n^2+1}}{n} + \frac{n}{n} = \lim_{n\to+\infty} \sqrt{1+\frac{1}{n^2}} - 1 = 1-1 = 0
    \]
    E quindi non possiamo dire nulla con la condizione necessaria di convergenza, quindi proviamo razionalizzando
    \begin{align*}
        \frac{\sqrt{n^2+1}-n}{n} &= \frac{\sqrt{n^2+1}-n}{n} \cdot \frac{ \sqrt{n^2+1}+n}{\sqrt{n^2+1}+n} \\
        &= \frac{(n^2+1)-n^2}{n\bigl(\sqrt{n^2+1}+n\bigr)} = \frac{1}{n\bigl(\sqrt{n^2+1}+n\bigr)}
     \end{align*}
     Possiamo notare che la nuova frazione è $\sim \frac{1}{2n^2}$ infatti
     \[
        \lim_{n\to +\infty} \frac{\frac{1}{n\bigl(\sqrt{n^2+1}+n\bigr)}}{\frac{1}{2n^2}} = \lim_{n\to +\infty} \frac{\frac{1}{n^2\bigl(\sqrt{1+\frac{1}{n^2}}+1\bigr)}}{\frac{1}{2n^2}} = \lim_{n\to +\infty} \frac{\frac{1}{\sqrt{1+\frac{1}{n^2}}+1}}{\frac{1}{2}} = \frac{\frac{1}{1+1}}{\frac{1}{2}} = 1 
     \]
     Pertanto le serie avranno lo stesso carattere, ma $\frac{1}{2n^2}$ sappiamo che converge visto che è la serie armonica generalizzata con $\alpha=2$, e di conseguenza anche la serie $\sum \frac{\sqrt{n^2+1}-n}{n}$ convergerà per il criterio del confronto asintotico.
    \end{proof}

    \begin{esercizio}{}{}
    Determina il carattere della seguente serie
    \[
    \sum_{n=1}^{+\infty} \sin\left(\frac{1}{3^n}\right)\arcsin\left(\frac{1}{2^n}\right)
    \]
\end{esercizio}
\begin{proof}
    Per i confronti asintotici sappiamo che per $n\to +\infty$
    \[
    \begin{array}{c @{\qquad}@{\qquad} c}
       \displaystyle \sin\left(\frac{1}{3^n}\right) \sim \frac{1}{3^n} & \displaystyle\arcsin\left(\frac{1}{2^n}\right) \sim \frac{1}{2^n}
    \end{array}
    \]
    Pertanto $ \displaystyle\sin\left(\frac{1}{3^n}\right)\arcsin\left(\frac{1}{2^n}\right) \sim \frac{1}{3^n} \cdot  \frac{1}{2^n} = \frac{1}{6^n}$, ma noi sappiamo che $\displaystyle\sum \frac{1}{6^n}$ converge visto che è una serie geometrica di ragione $r = \frac{1}{6}$ e quindi converge, ma visto che la serie $\displaystyle\sum \sin\left(\frac{1}{3^n}\right)\arcsin\left(\frac{1}{2^n}\right)$ è asintotica a $\displaystyle\sum \frac{1}{6^n}$ allora anche l'altra serie è convergente.
\end{proof}

\begin{esercizio}{}{}
    Discutere il carattere della serie al variare di $\alpha \geq 0$
    \[
    \sum_{n=1}^{+\infty}\frac{1-\cos(1-\cos(\frac{1}{n}))}{e^{\tan(\frac{1}{n^\alpha})}-1}
    \]
\end{esercizio}

\begin{proof}
    Iniziamo a fare delle equivalenze asintotiche, infatti per $n\to+\infty$
    \[
        1-\cos\left(\frac{1}{n}\right) \sim \frac{1}{2n^2}
    \]
    E pertanto 
    \[
        1-\cos\left(1-\cos\left(\frac{1}{n}\right)\right) \sim 1-\cos\left(\frac{1}{2n^2}\right) \sim \frac{1}{8n^4}
    \]
    Mentre a denominatore iniziamo con 
    \[
        \tan\left(\frac{1}{n^\alpha}\right) \sim \frac{1}{n^\alpha}
    \]
    Di conseguenza
    \[
    e^{\tan(\frac{1}{n^\alpha})}-1 \sim e^{\frac{1}{n^\alpha}}-1 \sim \frac{1}{n^\alpha}
    \]
    Ricomponendo la frazione abbiamo che 
    \[
    \sum_{n=1}^{+\infty}\frac{1-\cos(1-\cos(\frac{1}{n}))}{e^{\tan(\frac{1}{n^\alpha})}-1} \sim \frac{\frac{1}{8n^4}}{\frac{1}{n^\alpha}} = \frac{1}{8n^{4-\alpha}}
    \]
    e per la serie armonica sappiamo che
    \[
    \sum \frac{1}{8n^{4-\alpha}} = \begin{cases}
        \text{Converge} & \text{se } 4-\alpha > 1 \\
        \text{Diverge} & \text{se } 4-\alpha \leq 1
    \end{cases} = \begin{cases}
        \text{Converge} & \text{se } \alpha < 3 \\
        \text{Diverge} & \text{se } \alpha \geq 3
    \end{cases}
    \]
    E chiaramente dato che le due serie sono asintotiche tra di loro, per forza devo avere lo stesso carattere, e quindi 
     \[
    \sum_{n=1}^{+\infty}\frac{1-\cos(1-\cos(\frac{1}{n}))}{e^{\tan(\frac{1}{n^\alpha})}-1} =\begin{cases}
        \text{Converge} & \text{se } \alpha < 3 \\
        \text{Diverge} & \text{se } \alpha \geq 3
    \end{cases}
    \]
\end{proof}
\newpage

\addcontentsline{toc}{subsection}{Criterio del Rapporto}
\begin{teorema}{Criterio del Rapporto}{}
    Sia $(a_n)_{n\in \mathbb{N}}$ tale che $a_n > 0$ definitivamente allora
    \begin{enumerate}[label=(\roman*)]
        \item Se $\exists r \in (0, 1)$ tale che $\displaystyle\frac{a_{n+1}}{a_n} \leq r$ definitivamente allora la serie $\sum a_n$ converge
         \item Se $\exists r\in [1, +\infty)$ tale che $\displaystyle\frac{a_{n+1}}{a_n} \geq r$ definitivamente allora la serie $\sum a_n$ diverge
    \end{enumerate}
\end{teorema}

\begin{proof}
    $(i)$ Per ipotesi sappiamo che è vero definitivamente che
    \[
        a_{N+1} \leq r\cdot a_N \;\;\;\;\; \text{con } N\in \mathbb{N}
    \]
    Per lo stesso ragionamento fatto per il criterio di convergenza delle Successioni possiamo dire che
    \[
    a_{N+k} \leq r^k\cdot a_N \;\;\;\;\; \forall k \geq 1
    \]
    Di conseguenza abbiamo che 
    \begin{align*}
        \sum a_{N+k} &\leq \sum r^k\cdot a_N \\
        \sum a_{N+k} &\leq a_N \sum r^k
    \end{align*}
   
    E chiaramente per $r \in (-1,1)$ abbiamo che la serie $ \sum r^k$ converge, e pertanto per il criterio del confronto abbiamo che anche la serie $\sum a_{N+k}$ converge. Dato che la successione è a termini positivi allora $\displaystyle\frac{a_{n+1}}{a_n} > 0$, pertanto è impossibile che $\displaystyle\frac{a_{n+1}}{a_n} \in (-1,0] $ e quindi possiamo trovare solamente valori di $r \in (0, 1)$.
    $(ii)$ è necessiario fare lo stesso ragionamento, chiaramente la disequazione sarà
    \[
     a_{N+k} \geq r^k\cdot a_N \;\;\;\;\; \forall k \geq 1
    \]
    E quindi la serie geometrica diverge per valori di $r\in [1,+\infty)$, e quindi la serie $\sum a_{N+k}$ diverge anch'essa.
\end{proof}

\addcontentsline{toc}{subsection}{Criterio del Rapporto Asintotico}
\begin{teorema}{Criterio del Rapporto Asintotico}{}
    Sia $(a_n)_{n\in \mathbb{N}}$ tale che $a_n > 0$ definitivamente allora
    \[
        \exists \lim_{n\to +\infty} \frac{a_{n+1}}{a_n} = l \in [0, +\infty) \cup \{ +\infty\}
    \]
    Allora 
    \begin{enumerate}[label=(\roman*)]
        \centering
        \item $\displaystyle l < 1 \implies \sum a_n$ converge 
        \item $\displaystyle l > 1 \implies \sum a_n$ diverge
        \item $\displaystyle l = 1 $ Non possiamo dire nulla sulla serie   
    \end{enumerate}
\end{teorema}

\begin{esercizio}{}{}
    Determina il carattere della serie 
    \[
    \sum_{n=1}^{+\infty} \frac{(3n)!}{n^{3n}}
    \]
\end{esercizio}
\begin{proof}
    Proviamo il criterio del rapporto asintotico 
    \begin{align*}
        \lim_{n\to +\infty} \frac{a_{n+1}}{a_n} &= \frac{(3(n+1))!}{(n+1)^{3(n+1)}} \cdot \frac{n^{3n}}{(3n)!} = \frac{(3n+3)!}{(n+1)^{3n+3}} \cdot \frac{n^{3n}}{(3n)!} = \frac{(3n+3)!}{(n+1)^{3n+3}} \cdot \frac{n^{3n}}{(3n)!} \\
        &= \frac{(3n+3)(3n+2)(3n+1)(3n)!}{(n+1)^{3n} (n+1)^3} \cdot \frac{n^{3n}}{(3n)!} \\
        &= \frac{(3n+3)(3n+2)(3n+1)}{ (n+1)^3} \cdot \frac{n^{3n}}{(n+1)^{3n}} \\
        &=\frac{(3n+3)(3n+2)(3n+1)}{ (n+1)^3} \cdot \left(\frac{n}{n+1}\right)^{3n} \sim   \frac{(3n)(3n)(3n)}{ (n)^3} \cdot \left(\frac{n+1}{n}\right)^{-3n} \\
        &=\frac{3^3n^3}{ n^3} \cdot \left(1+\frac{1}{n}\right)^{-3n} = 3^3\cdot e^{-3} = \left(\frac{3}{e}\right)^3 > 1
    \end{align*}
    E pertanto per il criterio del rapporto asintotico abbiamo che la serie diverge.
\end{proof}

Questa serie si poteva risolvere anche con il criterio necessario per la convergenza e tramite la formula di Stirling, infatti
\begin{align*}
    \lim_{n\to+\infty} \frac{(3n)!}{n^{3n}} &\sim \lim_{n\to+\infty} \frac{\left(\frac{3n}{e}\right)^{3n} \sqrt{2\pi (3n)}}{n^{3n}} = \lim_{n\to+\infty} \frac{\left(\frac{3}{e}\right)^{3n} n^{3n} \sqrt{6\pi n}}{n^{3n}}   \\
&= \lim_{n\to+\infty} \left(\frac{3}{e}\right)^{3n}  \sqrt{6\pi n} = +\infty
\end{align*}
    \begin{esercizio}{}{}
        Discutere il carattere della serie al variare di $x\in \mathbb{R}$
        \[
            \sum_{n=1}^{+\infty}\frac{x^n}{n^32^n}
        \]
    \end{esercizio}
    \begin{proof}
        Dato che $x\in \mathbb{R}$ per valori negativi avremo che la nostra serie non è a termini positivi, e quindi non potremmo applicare il teorema del rapporto asintotico, quindi proviamo a vedere se converge la serie $\sum \left|\frac{x^n}{n^32^n}\right| = \sum \frac{|x|^n}{n^32^n}$, in questa maniera la serie è a termini positivi e quindi possiamo applicare il teorema del criterio asintotico. 
        \[
            \lim_{n\to+\infty} \left|\frac{x^{n+1}}{(n+1)^32^{n+1}}\right| \cdot \left|\frac{n^32^n}{x^n}\right| = \lim_{n\to+\infty}\left|\frac{x}{2}\cdot \frac{n^3}{(n+1)^3}\right|  \sim \lim_{n\to+\infty}\left|\frac{x}{2}\right|\cdot \frac{n^3}{(n)^3}= \frac{|x|}{2}
        \]
        Per il criterio del rapporto asintotico sappiamo che se $\frac{|x|}{2}>1$, cioè $x < -2 \lor x >2$ allora la serie diverge assolutamente, mentre per $\frac{|x|}{2}<1$, cioò per $-2 <x < 2$ la serie converge assolutamente e quindi anche semplicemente. Però ora mancano i caso $x=\pm2$, visto che il criterio non ci permette di dedurre nulla, e quindi dobbiamo fare altro. Però notiamo subito che se $x=2$ allora
        \[
        \sum_{n=1}^{+\infty}\frac{x^n}{n^32^n} = \sum_{n=1}^{+\infty}\frac{2^n}{n^32^n} = \sum_{n=1}^{+\infty}\frac{1}{n^3}  
        \] 
        Notiamo che viene fuori la somma armonica generalizzata con $\alpha = 3$, che sappiamo che converge, mentre per il caso $x=-2$, possiamo riutilizzare il criterio della convergenza assoluta, infatti $\sum \left|\frac{(-2)^n}{n^32^n}\right| = \sum \frac{|-2|^n}{n^32^n} = \sum \frac{2^n}{n^32^n} = \sum \frac{1}{n^3}$ che abbiamo appena visto che converge. Pertanto per il caso $x=-2$ la serie converge assolutamente e quindi anche semplicemente. 
        \[
           \sum_{n=1}^{+\infty}\frac{x^n}{n^32^n} = \begin{cases}
            \text{Diverge} & x < -2 \lor x > 2 \\
            \text{Converge Semplicemente e Assolutamente} & -2 \leq x \leq 2 \\
           \end{cases} 
        \]
        \textbf{N.B.} n caso $x < -2 \lor x > 2 $ ho scritto che diverge, e questo si può verificare con la condizione necessaria alla convergenza.  
    \end{proof}

    \begin{esercizio}{}{}
        Discutere il carattere della serie al variare di $\alpha\geq 0$
        \[
            \sum_{n=1}^{+\infty}\frac{n!\cdot \alpha^n}{n^n}
        \]
    \end{esercizio}
\begin{proof}
        Proviamo usando il criterio del rapporto asintotico
\begin{align*}
       \lim_{n\to+\infty}\frac{a_{n+1}}{a_n} &= \lim_{n\to+\infty} \frac{(n+1)! \cdot \alpha^{n+1}}{(n+1)^{n+1}} \cdot \frac{n^n}{n! \cdot \alpha^n} = \lim_{n\to+\infty} \frac{(n+1)n! \cdot \alpha}{(n+1)(n+1)^n} \cdot \frac{n^n}{n!} \\
       &= \lim_{n\to+\infty}\alpha \cdot \left(\frac{n}{n+1}\right)^{n} = \lim_{n\to+\infty}\alpha \cdot \left(\frac{n+1}{n}\right)^{-n} \\
       &= \lim_{n\to+\infty}\alpha \cdot \left(1+\frac{1}{n}\right)^{-n}= \alpha \cdot e^{-1} =\frac{\alpha}{e}
    \end{align*}
    Quindi se $\frac{\alpha}{e} > 1 \implies \alpha > e$ allora la serie diverge, mentre se $\frac{\alpha}{e} < 1 \implies \alpha < e$ la serie converge. Proprio come l'esercizio precedente dobbiamo studiare a parte il caso $\alpha = e$, e notiamo che la serie diventa
    \[
    \sum_{n=1}^{+\infty}\frac{n!\cdot \alpha^n}{n^n} = \sum_{n=1}^{+\infty}\frac{n!\cdot e^n}{n^n}
    \] 
Proviamo a vedere con la condizione necessaria
\[
\lim_{n\to +\infty} \frac{n!\cdot e^n}{n^n} \sim \lim_{n\to +\infty} \frac{\left(\frac{n}{e}\right)^n\sqrt{2\pi n}\cdot e^n}{n^n} = \lim_{n\to +\infty} \frac{\frac{n^n}{e^n}\sqrt{2\pi n} \cdot e^n}{n^n} = \lim_{n\to+\infty}\sqrt{2\pi n} = +\infty
\]
E quindi per condizione necessaria sappiamo che con $\alpha = e$ la serie diverge. E quindi
\[
\sum_{n=1}^{+\infty}\frac{n!\cdot \alpha^n}{n^n} = \begin{cases}
            \text{Converge } & \alpha < e \\
            \text{Diverge} & \alpha \geq e \\
           \end{cases} 
\]
        \end{proof}

        \newpage

\addcontentsline{toc}{subsection}{Criterio della Radice}
        \begin{teorema}{Criterio della Radice}{}
             Sia $(a_n)_{n\in \mathbb{N}}$ tale che $a_n > 0$ definitivamente allora
        \begin{enumerate}[label=(\roman*)]
            \item Se $\exists r \in (0, 1)$ tale che $\displaystyle\sqrt[n]{a_n} \leq r$ definitivamente allora la serie $\sum a_n$ converge
            \item Se $\exists r\in [1, +\infty)$ tale che $\displaystyle\sqrt[n]{a_n} \geq r$ definitivamente allora la serie $\sum a_n$ diverge
        \end{enumerate}
        \end{teorema}

        \begin{proof}
            $(i)$ Dato che $a_n$ è definitivamente positiva, e visto che $n \in \mathbb{N}$ possiamo dire che
            \[
            \sqrt[n]{a_n} \leq r \implies a_n \leq r^n \implies \sum a_n \leq \sum r^n
            \]
            Se portiamo tutto al limite abbiamo che
            \[
               \lim_{k\to+\infty} \sum_{n=1}^{k} a_n \leq \lim_{k\to+\infty} \sum_{n=1}^{k} r^n
            \]
            Come abbiamo già visto, la serie geometrica converge solamente se $-1 < r < 1$, e quindi per il confronto abbiamo che anche la serie $ \sum a_n$ converge. Però dato che la serie è a termini positivi è impossibile che $r \in (-1,0]$ e quindi potremo avere valore di $r \in (0, 1)$.
            
            $(ii)$ ragionamento analogo al caso $(i)$, e quindi possiamo dire che 
            \[
            \sqrt[n]{a_n} \geq r \implies \sum a_n \geq \sum r^n
            \]
            Se portiamo tutto al limite abbiamo che
            \[
               \lim_{k\to+\infty} \sum_{n=1}^{k} a_n \geq \lim_{k\to+\infty} \sum_{n=1}^{k} r^n
            \]
            Ma la serie geometrica diverge per $r \leq -1 \lor r \geq 1$ e di conseguenza anche la serie $\sum a_n$ diverge. Però dato che la serie è a termini positivi è impossibile che $r \leq -1$ e quindi potremo avere valore di $r \geq 1$.
        \end{proof}

\addcontentsline{toc}{subsection}{Criterio della Radice Asintotico}
        \begin{teorema}{Criterio della Radice Asintotico}{}
        Sia $(a_n)_{n\in \mathbb{N}}$ tale che $a_n > 0$ definitivamente allora
            \[
                \exists \lim_{n\to +\infty} \sqrt[n]{a_n} = l \in [0, +\infty) \cup \{ +\infty\}
            \]
            Allora 
            \begin{enumerate}[label=(\roman*)]
                \centering
                \item $\displaystyle l < 1 \implies \sum a_n$ converge 
                \item $\displaystyle l > 1 \implies \sum a_n$ diverge
                \item $\displaystyle l = 1 $ Non possiamo dire nulla sulla serie   
            \end{enumerate}
        \end{teorema}

        \begin{esercizio}{}{}
            Determinare il carattere della serie 
            \[
                \sum_{n=1}^{+\infty} \left(\frac{1}{2} + \frac{1}{2n}\right)^n
            \]
        \end{esercizio}
        \begin{proof}
            Se proviamo ad usare la condizione necessaria abbiamo che
            \[
            \lim_{n\to+\infty} \left(\frac{1}{2} + \frac{1}{2n}\right)^n \sim \lim_{n\to+\infty} \left(\frac{1}{2}\right)^n = 0
            \]
            E quindi non possiamo dedurre nulla. Quindi proviamo ad usare il criterio della radice
            \[
            \lim_{n\to+\infty} \sqrt[n]{\left(\frac{1}{2} + \frac{1}{2n}\right)^n} =  \lim_{n\to+\infty} \left(\frac{1}{2} + \frac{1}{2n}\right) =\frac{1}{2} +0 = \frac{1}{2} < 1 
            \]
            Notiamo che il risultato è minore di 1, e quindi per il criterio della radice sappiamo che la seria $\displaystyle\sum \left(\frac{1}{2} + \frac{1}{2n}\right)^n$ è convergente.
        \end{proof}
        \begin{esercizio}{}{}
            Determinare il carattere della serie al variare $\alpha \geq 0$
            \[
                \sum_{n=1}^{+\infty} \alpha^n \left(1+ \frac{2}{n}\right)^n
            \]
        \end{esercizio}

        \begin{proof}
            Utiliziamo il criterio della radice
            \[
            \lim_{n\to+\infty} \sqrt[n]{\alpha^n \left(1+ \frac{2}{n}\right)^n} = \lim_{n\to+\infty}\alpha \left(1+ \frac{2}{n}\right) = \alpha
            \]
            Quindi visto che il risultato del limite è $\alpha$, abbiamo che per $\alpha>1$ la serie diverge, mentre per $\alpha <1$ la serie converge. Come al solito dobbiamo studiare a parte il caso $\alpha = 1$. Abbiamo che
            \[
            \sum_{n=1}^{+\infty} \alpha^n \left(1+ \frac{2}{n}\right)^n = \sum_{n=1}^{+\infty} 1^n \left(1+ \frac{2}{n}\right)^n = \sum_{n=1}^{+\infty} \left(1+ \frac{2}{n}\right)^n
            \]
            Per questa serie è necessario usare la condizione necessaria
            \[
            \lim_{n\to+\infty}  \left(1+ \frac{2}{n}\right)^n = e^2 \ne 0
            \]
            E quindi abbiamo che la serie con $\alpha = 1$ diverge. E quindi abbiamo che
            \[
                \sum_{n=1}^{+\infty} \alpha^n \left(1+ \frac{2}{n}\right)^n = \begin{cases}
                    \text{Convergente} & 0 \leq \alpha < 1 \\
                    \text{Divergente} & \alpha \geq 1 
                \end{cases}
            \]
        \end{proof}

        \begin{esercizio}{}{}
            Determinare il carattere della serie al variare $x \in \mathbb{R}$
            \[
            \sum_{n=1}^{+\infty} \frac{x^n}{3^n\sqrt{n+1}}
            \]
        \end{esercizio}

        \begin{proof}
            Visto che $x\in  \mathbb{R}$, abbiamo che per $x < 0$ la serie non è più definitivamente positiva, quindi per poter usare i vari criteri è necessario che studiamo la serie dei valori assoluti: $\displaystyle\sum_{n=1}^{+\infty} \left|\frac{x^n}{3^n\sqrt{n+1}}\right| = \sum_{n=1}^{+\infty} \frac{|x|^n}{3^n\sqrt{n+1}}$. Dato che ora la serie è definitivamente positivi possiamo applicare il criterio della radice
            \[
            \lim_{n\to +\infty} \sqrt[n]{ \frac{|x|^n}{3^n\sqrt{n+1}}} =           \lim_{n\to +\infty} \frac{|x|}{3\sqrt[2n]{n+1}} 
            \]
            Per calcolare il denominatore dobbiamo fare un paio di riarrangiamenti
            \[
            \lim_{n\to+\infty} \sqrt[2n]{n+1} = \lim_{n\to+\infty} e^{log(\sqrt[2n]{n+1})} = \lim_{n\to+\infty} e^{log\left((n+1)^\frac{1}{2n}\right)}  = \lim_{n\to+\infty} e^{\frac{log(n+1)}{2n}}   = e^0 = 1
            \]
            E quindi abbiamo che
            \[
            \lim_{n\to +\infty} \frac{|x|}{3\sqrt[2n]{n+1}}  = \frac{|x|}{3}
            \]
            Quindi per $\frac{|x|}{3} < 1$, e cioè $ -3 < x < 3$, la serie converge assolutamente e quindi anche semplicemente. Per vedere l'altra casistica dobbiamo usare la condizione necessaria
            \[
            \lim_{n\to+\infty} \frac{x^n}{3^n\sqrt{n+1}} = \lim_{n\to+\infty} \frac{\left(\frac{x}{3}\right)^n}{\sqrt{n+1}} \sim \lim_{n\to+\infty} \frac{\left(\frac{x}{3}\right)^n}{\sqrt{n}}
            \]
            Ora dato che $|x| > 3$ (che è la casistica che ci manca), abbiamo che $\frac{x}{3} > 1$ e quindi per gerarchia degli infiniti abbiamo che 
            \[
            \lim_{n\to+\infty} \frac{\left(\frac{x}{3}\right)^n}{\sqrt{n}} = \infty
            \] 
            E quindi abbiamo che la serie per $|x| > 3$ è divergente. Ci mancano i casi $x = \pm3$, iniziamo con $x=3$ e abbiamo Che
            \[
            \sum_{n=1}^{+\infty} \frac{x^n}{3^n\sqrt{n+1}} = \sum_{n=1}^{+\infty} \frac{3^n}{3^n\sqrt{n+1}} = \sum_{n=1}^{+\infty} \frac{1}{\sqrt{n+1}} \sim \sum_{n=1}^{+\infty} \frac{1}{\sqrt{n}} = \sum_{n=1}^{+\infty} \frac{1}{n^{\frac{1}{2}}}
            \]
            Per la serie armonica abbiamo che la serie diverge. Per il caso $x=-3$ è necessario usare un criterio che dobbiamo ancora vedere (quello di Leibniz), e che quindi per ora lo lasciamo stare. Quindi
            \[
            \sum_{n=1}^{+\infty} \frac{x^n}{3^n\sqrt{n+1}}\begin{cases}
                    \text{Convergente} & 0 -3 < x < 3 \\
                    \text{Divergente} & x < -3 \lor x \geq 3
                \end{cases}
            \]
        \end{proof}

\addcontentsline{toc}{subsection}{Relazione tra criterio del Rapporto e della Radice}
        \begin{teorema}{Relazione tra criterio del Rapporto e della Radice}{}
            Sia $(a_n)_{n\in \mathbb{N}}$ una successione positiva, allora se
            \[
            \exists \lim_{n\to+\infty} \frac{a_{n+1}}{a_n} = l\in [0, +\infty) \cup \{+\infty\} \implies \lim_{n\to+\infty} \sqrt[n]{a_n} = l
            \]
        \end{teorema}
        \begin{proof}
            Per risolverlo dobbiamo dividere in due casi: $l \in [0,+\infty)$ e $l = +\infty$. Partiamo con la prima casistica
            
            $(i)$ Per ipotesi sappiamo che esiste il limite del rapposto (e fa un numero finito), e quindi usando la definizione di limite abbiamo che
            \[
                l-\varepsilon < \frac{a_{n+1}}{a_n} < l + \varepsilon \;\;\;\;\; \forall n \geq \bar{n}
            \]
            Dato che questo vale $\forall n \geq \bar{n}$, allora sarà vero anche che $ l-\varepsilon < \frac{a_{\bar{n} +1}}{ a_{\bar{n}}} < l + \varepsilon$, ma anche $l-\varepsilon < \frac{a_{\bar{n}+2}}{a_{\bar{n}+1}} < l + \varepsilon$, sarà valido anche per $\frac{a_{\bar{n}+3}}{a_{\bar{n}+2}}$, $\frac{a_{\bar{n}+4}}{a_{\bar{n}+3}}$ e così via fino a$\frac{a_{n}}{a_{n-1}}$, $\frac{a_{n+1}}{a_n}$. E dato che tutti i termini sono positivi, possiamo dire 
            \[
                l-\varepsilon < \frac{a_{n+1}}{a_n} < l + \varepsilon
            \] 
            \[
                (l-\varepsilon)\frac{a_{n}}{a_{n-1}} < \frac{a_{n+1}}{a_n} \frac{a_{n}}{a_{n-1}} < (l + \varepsilon) \frac{a_{n}}{a_{n-1}}
            \] 
            \[
                (l-\varepsilon)(l-\varepsilon) < (l-\varepsilon)\frac{a_{n}}{a_{n-1}} < \frac{a_{n+1}}{a_{n-1}} < (l + \varepsilon) \frac{a_{n}}{a_{n-1}} < (l + \varepsilon)(l + \varepsilon)
            \] 
            \[
            (l-\varepsilon)^2 <  \frac{a_{n+1}}{a_{n-1}}  < (l + \varepsilon)^2
            \]
            E quindi possiamo continuare con questo ragionamento moltiplicando per tutte le frazioni e abbiamo
            \[
            (l-\varepsilon)^{n-\bar{n}} <  \frac{a_{n+1}}{a_{\bar{n}}}  < (l + \varepsilon)^{n-\bar{n}}
            \]
            \[
            (l-\varepsilon)^{n-\bar{n}} a_{\bar{n}}<  a_{n+1} < (l + \varepsilon)^{n-\bar{n}}a_{\bar{n}}
            \]
            Ora se applichiamo la radice n-esima abbiamo che
             \[
            \sqrt[n]{(l-\varepsilon)^{n-\bar{n}} a_{\bar{n}}}<  \sqrt[n]{a_{n+1}} < \sqrt[n]{(l + \varepsilon)^{n-\bar{n}}a_{\bar{n}}}
            \]
             \[
            (l-\varepsilon)^{1-\frac{\bar{n}}{n}}\sqrt[n]{ a_{\bar{n}}}<  \sqrt[n]{a_{n+1}} < (l + \varepsilon)^{1-\frac{\bar{n}}{n}}\sqrt[n]{ a_{\bar{n}}}
            \]
            Portando tutto al limite abbiamo abbiamo che $n+1 \to n$, $\frac{\bar{n}}{n} \to 0$ e $\sqrt[n]{a_{\bar{n}}}\to 1$ 
            \[
            (l-\varepsilon)<  \sqrt[n]{a_{n}} < (l + \varepsilon)
            \]
            Questo verifica il limite
            \[
                \lim_{n\to+\infty} \sqrt[n]{a_{n}} = l
            \]
            $(ii)$ basta fare lo stesso ragionamento ma il limite di partenza sara $\frac{a_{n+1}}{a_n} > M$, e poi il ragionamento è analogo. 
        \end{proof}

        \textbf{N.B.} quindi se durante un esercizio fai il criterio del rapporto è inutile che provi quello della radice perchè darà lo stesso risultato. Il criterio della radice va applicato dopo quello del rapporto solo se il limite del rapporto non esiste.
\newpage

        \begin{esercizio}{}{}
            Determinare il carattere della serie
            \[
                \sum_{n=1}^{+\infty} 2^{n+(-1)^n}
            \]
        \end{esercizio}


        
        \begin{proof}
            Se proviamo con il criterio del rapporto abbiamo che 
            \[
            \lim_{n\to +\infty}    \frac{2^{n+1+(-1)^{n+1}}}{2^{n+(-1)^n}} = \lim_{n\to +\infty}  2^{n+1+(-1)^{n+1} -n-(-1)^n-} = \lim_{n\to +\infty} 2^{1 +(-1)^{n+1}-(-1)^{n}}
            \]
            Però per il teorema della caratterizzazione sequenziale dei limiti e se scegliamo $a_n = 2n$ e $b_n = 2n+1$ abbiamo che il nostro limite tende a due valori diversi con $a_n$ il limite tende a $\frac{1}{2}$, mentre con $b_n$ tende a $8$, e quindi il limite non esiste.  In questo caso ha senso usare il criterio della radice, e quindi 
        \[
        \lim_{n\to +\infty}  \sqrt[n]{2^{n+(-1)^n}} = \lim_{n\to +\infty}  2^{\frac{n+(-1)^n}{n}} =  \lim_{n\to +\infty}  2^{ 1 + \frac{(-1)^n}{n}} = 2^{1+0} = 2 > 1
        \]
        Quindi la serie diverge.
        \end{proof}


\addcontentsline{toc}{subsection}{Criterio Condensazione di Cauchy}
        \begin{teorema}{Criterio Condensazione di Cauchy}{}
            Sia $(a_n)_{n\in \mathbb{N}}$ tale che
            \begin{itemize}
                \centering
                \item $a_n \geq 0$ $\forall n \in \mathbb{N}$ (non negativa)
                \item $a_{n+1} \leq a_n $ $\forall n \in \mathbb{N}$  ( decrescente)
            \end{itemize}
            Allora le due serie 
            \[
            \begin{array}{c @{\qquad}@{\qquad} c}
                \displaystyle\sum_{n=1}^{+\infty} a_n & \displaystyle\sum_{n=1}^{+\infty} 2^na_{2^n} 
            \end{array}
            \]
            Hanno lo stesso carattere e vale
            \[
            \frac{1}{2}\sum_{n=1}^{+\infty} 2^na_{2^n}  \leq\sum_{n=1}^{+\infty} a_n \leq \sum_{n=1}^{+\infty} 2^na_{2^n} 
            \]
        \end{teorema}
        Questo teorema può essere utilizzato per dimostrare la serie armonica generalizzata, infatti se $a_n = \frac{1}{n^\alpha}$, sappiamo che $a_n \geq 0$ e vale anche $\frac{1}{(n+1)^\alpha} \leq \frac{1}{n^\alpha}$, e quindi possiamo applicare il teorema e avremo
        \[
            \sum_{n=1}^{+\infty} \frac{1}{n^\alpha} \implies \sum_{n=1}^{+\infty} 2^n \cdot \frac{1}{(2^n)^\alpha} =\sum_{n=1}^{+\infty} \frac{2^n}{2^{\alpha n}} =  \sum_{n=1}^{+\infty} (2^{1-\alpha})^n 
        \]
        Che per la serie geometrica abbiamo che
        \[
\sum_{n=1}^{+\infty} (2^{1-\alpha})^n = \begin{cases}
    \text{Convergente } & 2^{1-\alpha} < 1 \\
    \text{Divergente } & 2^{1-\alpha} \geq 1
\end{cases}        = \begin{cases}
    \text{Convergente } & \alpha > 1 \\
    \text{Divergente } & \alpha \leq 1
\end{cases}   
        \]
        Difatti abbiamo ritrovato lo stesso risultato trovato precedentemente.
        \newpage

\addcontentsline{toc}{subsection}{Definizione Serie a Segno Alterno}
        \begin{definizione}{Serie a Segno Alterno}{}
            Una serie a \textbf{segno alterno} è detta una qualsiasi serie esprimibile come
            \[
                \sum_{n=1}^{+\infty} (-1)^n a_n
            \]
            Con  $(a_n)_{n\in \mathbb{N}}$ una qualsiasi successione definitivamente positiva
        \end{definizione}

        Con queste tipologie di serie non possiamo applicare nessuno dei teoremi visti prima, perchè la serie non è mai a definitivamente positiva (per via del $(-1)^n$). L'unico che potremmo applicare sarebbe la condizione necessaria o il criterio del valore assoluto. Quindi le serie a segno alterno esiste un solo criterio usabile: quello di Leiniz.

\addcontentsline{toc}{subsection}{Criterio di Leibniz}
        \begin{teorema}{Criterio di Leibniz}{}
            Sia $(a_n)_{n\in \mathbb{N}}$ una successione, se
            \begin{itemize}
                \centering
                \item $a_n \geq 0$ $\forall n \in \mathbb{N}$ (non negativa)
                \item $a_{n+1} \leq a_n $ $\forall n \in \mathbb{N}$  (decrescente)
                \item $\displaystyle\lim_{n\to+\infty} a_n = 0$ (infinitesima) 
            \end{itemize}
            Allora la serie $\displaystyle \sum_{n=1}^{+\infty} (-1)^n a_n $
            è convergente.
        \end{teorema}

        \begin{esempio}{}{}
            Studiamo il caso
            \[
            \sum_{n=1}^{+\infty} \frac{(-1)^n}{n^\alpha}
            \]
        \end{esempio}
        Ora per il criterio con il valore assoluto sappiamo Che
        \[
            \sum_{n=1}^{+\infty} \left|\frac{(-1)^n}{n^\alpha}\right| = \sum_{n=1}^{+\infty} \frac{1}{n^\alpha}
        \]
        Questa sappiamo che converge per $\alpha >1$, e quindi la nostra serie originale converge anche semplicemente, ma per $\alpha \in (0, 1]$, non sappiamo nulla, quindi usiamo il criterio di Leibniz. Notiamo che $a_n = \frac{1}{n^\alpha}$ e che 
        \[
        \begin{array}{c  @{\qquad}@{\qquad} c @{\qquad}@{\qquad} c}
            \displaystyle\frac{1}{n^\alpha} \geq 0\;\;\;\;\forall n \in \mathbb{N} & \displaystyle\frac{1}{(n+1)^\alpha} \leq \frac{1}{n^\alpha}\;\;\;\; \forall n \in \mathbb{N}  & \displaystyle\lim_{n\to+\infty}\frac{1}{n^\alpha}= 0
        \end{array}
    \]  
    Quindi la serie a segni alterni converge, e quindi per $\alpha \in (0, 1]$ abbiamo che la serie 
    \[
    \sum_{n=1}^{+\infty} \frac{(-1)^n}{n^\alpha}
    \]
    Converge semplicemente, ma diverge assolutamente. Mentre per $\alpha > 1$ converge sia semplicemente che assolutamente.
       
    
    
    
    
    
   \section{Continuità delle Funzioni} 

\addcontentsline{toc}{subsection}{Definizione di Funzione Continua}
    \begin{definizione}{Definizione di Funzione Continua}{}
        Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$ diciamo che \textbf{$f$ è continua in $x_0$} se vale una delle due proposizioni: 
        \begin{itemize}
            \item $x_0$ è un punto isolato in $A$
            \item $x_0$ è un punto di accumulazione in $A$ e vale
            \[
                \lim_{x\to x_0} f(x) = f(x_0)
            \] 
        \end{itemize}
        Se $f$ è continua $\forall x \in A$, diciamo che è \textbf{continua nel suo dominio} e lo indichiamo con il simbolo $f \in C^0(A)$, e lo definiamo come
        \[
            C^0(A) = \{f:A \to \mathbb{R} \text{ }| \text{ } f \text{ è continua in } A\}
        \] 
    \end{definizione}
    
    Usando questa definizione possiamo dimostrare che alcune funzioni sono continue, infatti
    \begin{itemize}
        \item $f(x) = c\in \mathbb{R}$, è continua in $\mathbb{R}$ infatti, come abbiamo già dimostrato 
        \[
            \lim_{x\to x_0} f(x) = \lim_{x\to x_0} c = c = f(x_0) \;\;\;\;\; \forall x_0 \in \mathbb{R}
        \] 
        \item $f(x) = \sum_{i} a_i x^i$, ogni polinomio è continuo, come abbiamo già dimostrato
        \item $\displaystyle f(x)=\frac{P(x)}{Q(x)}$, con $P, Q$ polinomi, abbiamo che è continua in $A = \{x : Q(x) \ne 0\}$
        \item $f(x)=a^x$ con $a > 0$
        \item $f(x) = \sin(x)$
        \item $f(x) = \cos(x)$
    \end{itemize}
    
    \begin{center}
    \begin{tikzpicture} 
         \begin{axis}[
        xmin=-2, xmax=3,
        ymin=-2, ymax=3,
        axis equal,
        axis lines=middle,
        enlargelimits=false,
        clip=false,
        axis line style={-stealth, thick},
        width=8cm
    ]
            \addplot[black, ultra thick, samples=50, domain=-2:2]({x}, {x}); 

            \addplot[
            only marks,
            mark=*,
            mark size=2pt,
            black
        ] coordinates {(3,1)};

        \node[black] at (axis cs:2,2.5) {$y=f(x)$};
        \end{axis} 
    \end{tikzpicture}
    \end{center}

    Notiamo che il dominio di questa funzione è $\mathbb{D}(f) = [-2, 2] \cup \{3\}$, e la funzione è continua nel suo dominio, questo perche $\forall x \in [-2,2]$ la vale la seconda proposizione di funzione continua, mentre per il punto $x=3$ dato che è un punto isolato è continuo per definizione.
    \newpage
    Proviamo a vedere se è continua la funzione parte intera: $f(x)= \lfloor x \rfloor$, il cui grafico è 

    
   

\begin{center}
\begin{tikzpicture} 
    \begin{axis}[
        xmin=-4, xmax=4,
        ymin=-4, ymax=4,
        axis equal,
        axis lines=middle,
        enlargelimits=false,
        clip=false,
        axis line style={-stealth, thick},
        width=10cm
    ]
        % Draw horizontal steps manually
        \foreach \x in {-4,-3,...,3} {
            % Horizontal segment
            \addplot[black, ultra thick] coordinates {(\x,\x) (\x+1,\x)};
            % Closed circle at start
            \addplot[only marks, mark=*, mark size=2pt] coordinates {(\x,\x)};
            % Open circle at end
            \addplot[only marks, mark=o, mark size=2pt] coordinates {(\x+1,\x)};
        }
        
        \node[black] at (axis cs:2,2.5) {$f(x)= \lfloor x \rfloor$};
    \end{axis} 
\end{tikzpicture}
\end{center}

Notiamo che per $x_0 \in \mathbb{Z}$ abbiamo che
\[
\lim_{x\to x_0^+} \lfloor x \rfloor = x_0 
\]
\[
\lim_{x\to x_0^-} \lfloor x \rfloor = x_0 -1
\]

Ma Dato che il limite destro e sinistro sono  diversi e quindi il limite in quel punto non esiste, e di conseguenza non è continua, visto che la continuità richiede che si possa calcolare il limite nel punto. 

Mentre per $x_0 \in\mathbb{R} \setminus \mathbb{Z}$, usando la definizione di limite e prendendo un valore di $\varepsilon < \min(x_0-\lfloor x_0\rfloor, \lceil x_0 \rceil - x_0 )$  la funzione $f(x) = \lfloor x \rfloor$ è costante $\forall x \in (x_0 - \varepsilon, x_0 +\varepsilon)$ è constate e, quindi esiste il limite in quel punto e fa proprio
\[
    \lim_{x\to x_0} \lfloor x \rfloor = \lfloor x_0 \rfloor
\]
E quindi per quei valori la funzione è continua. Di conseguenza possiamo scrivere che $\lfloor x \rfloor \in C^0(\mathbb{R} \setminus \mathbb{Z})$.
  
Un altro caso molto particolare è la funzione di Dirichlet, che è definita come
\[
f_D(x) = \begin{cases}
    1 & x \in \mathbb{R} \setminus \mathbb{Q} \\
    0 & x \in \mathbb{Q}
\end{cases}
\]
Notiamo che se $x_0 \in \mathbb{R} \setminus \mathbb{Q}$, per caratterizzazione sequenziale possiamo scegliere  $(a_n)_(n \in \mathbb{n})$ tale che $a_n \in \mathbb{Q}$ e $a_n \to x_0$, e questo possiamo farlo per la densità di $\mathbb{Q}$. In quei casi $\lim_{n\to +\infty} f_D(a_n) = 0$ perchè $a_n \in \mathbb{Q}$ e quindi la funzione restituisce 0, ma è diverso da $f_D(x_0) = 1$, visto che $x_0$ è irrazionale. Quindi visto che limite e valore effettivo sono diversi la funzione non è continua. Mentre per i casi $x_0 \in \mathbb{Q}$, possiamo usare sempre la caratterizzazione sequenziale con $a_n = x_0  + \frac{1}{n} \sqrt(2)$, e questa successione $a_n \in \mathbb{R} \setminus \mathbb{Q}$ dato che c'è $\sqrt{2}$, ma $a_n \to x_0$, quindi il limite $f(a_n) \to 1$ visto che $a_n\in \mathbb{R} \setminus \mathbb{Q}$ ma il valore della funzione è $f(x_0) = 0$, e quindi la funzione non è continua. In sostanza la funzione è discontinua $\forall x \in \mathbb{R}$, come era prevedibile.

\newpage

\addcontentsline{toc}{subsection}{Caratterizzazione $\varepsilon - \delta$ della Funzione Continua }
\begin{teorema}{Caratterizzazione $\varepsilon - \delta$ della Funzione Continua }{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$, se $f$ è continua in $x_0$ allora
    \[
        \forall \varepsilon > 0 \;\; \exists \delta > 0 : f(x) \in (f(x_0) - \varepsilon , f(x_0) + \varepsilon ) \;\; \forall x \in (x_0 - \delta, x_0 + \delta)
    \]
\end{teorema}
Questo viene della definizione di limite.

\addcontentsline{toc}{subsection}{Algebra delle Funzioni Continue }
\begin{teorema}{Algebra delle Funzioni Confinue}{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f,g: A \to \mathbb{R}$ e $x_0 \in A$, se $f, g$  sono continue in $x_0$ allora
    \begin{itemize}
        \item $(f\pm g)(x)$ è continua in $x_0$
        \item $(f\cdot g)(x)$ è continua in $x_0$
        \item $(\frac{f}{g})(x)$ è continua in $x_0$ se $g(x_0) \neq 0$
    \end{itemize}
\end{teorema}

Per dimostrare queste proposizioni è necessario usare l'algebra dei limiti finiti. 

\textbf{N.B.} se le funzioni sono continue perchè $x_0$ è isolato, funziona lo stesso il teorema, perchè vuol dire che anche le funzioni $(f\pm g)$, $(f\cdot g)$ e $(\frac{f}{g})$ sono isolate nel punto $x_0$, e quindi è continua per definizione.
Con questo teorema possiamo dimostrare che le funzioni $\sinh(x)$,$\cosh(x)$ e $\tan(x)$ sono continue.

\addcontentsline{toc}{subsection}{Limitatezza delle Funzioni Continue }
\begin{teorema}{Limitatezza delle funzioni continue}{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$, se $f$ è continua in $x_0$ allora è definitivamente limitata per $x\to x_0$
\end{teorema}

\begin{proof}
    Se $x_0$ è punto isolato, allora perforza la funzione è limitata visto che $\forall\varepsilon >0$ abbiamo che $|f(x)| <  |f(x_0)| + \varepsilon$, quindi non abbiamo nulla da dimostrare.
    
    Se $x_0$ è un punto di accumulazione, possiamo usare la definizione di limite e abbiamo che
    \[
    \lim_{x\to x_0} f(x) = f(x_0) \iff |f(x) - f(x_0)| < \varepsilon \;\;\; \forall x \in A \cap I 
    \]
    Ma se guardiamo il termine 
    \begin{align*}
        |f(x)| &= |f(x) - f(x_0) + f(x_0)|  \\
        &\leq |f(x) - f(x_0)| + |f(x_0)|  \\
        &\leq  \varepsilon + |f(x_0)|
    \end{align*}
    Quindi notiamo che $f(x)$ è sempre limitata.
\end{proof}

\addcontentsline{toc}{subsection}{Permanenza del Segno delle Funzioni Continue }
\begin{teorema}{Permanenza del Segno di Funzioni Continue}{}
     Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$, se $f$ è continua in $x_0$ allora 
    \begin{itemize}
        \item $f(x_0) > 0$, allora $f(x) > 0$ definitivamente per $x\to x_0$
        \item $f(x_0) < 0$, allora $f(x) < 0$ definitivamente per $x\to x_0$
    \end{itemize}
\end{teorema}
Per dimostrarlo è sufficiente fare il ragionamento analogo al teorema della permanenza del segno 
\newpage

\addcontentsline{toc}{subsection}{Continuità delle Funzioni Composte }
\begin{teorema}{Continuità di Funzioni Composte }{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f,g$ funzioni tali che $g \circ f: A \to \mathbb{R}$ sia ben definita e sono tali
    \begin{itemize}
        \item $f$ è continua in $x_0 \in \mathbb{D}(f) \cap A$
        \item $g$ è continua in $f(x_0) \in \mathbb{D}(g)$
    \end{itemize}
    Allora anche $(g \circ f)(x)$ è continua in $x_0 \in A$.
\end{teorema}
Per dimostrarlo è sufficiente usare il teorema del cambio di variabile nella definizione di continuità.

\addcontentsline{toc}{subsection}{Caratterizzazione Sequenziale di funzioni Continue}
\begin{teorema}{Caratterizzazione Sequenziale di funzioni Continue}{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 \in A$, se $f$  è continua in $x_0$ se e solo se  $\forall (a_n)_{n\in \mathbb{N}} \subseteq A$ tale che $\displaystyle \lim_{n\to +\infty} a_n = x_0$ vale $\displaystyle \lim_{n\to +\infty} f(a_n) = f(x_0)$.
\end{teorema}
Per dimostrarlo è sufficiente usare la caratterizzazione sequenziale del limite nella definizione di continuità.


\addcontentsline{toc}{subsection}{Definizione di Prolungamento per Continuità}
\begin{definizione}{Prolungamento per Continuità}{}
     Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 $ punto di accumulazione in $A$, $\displaystyle\exists \lim_{x\to x_0} f(x) = l$, se
     \[
     \begin{array}{c @{\qquad} c @{\qquad} c} 
        l \ne f(x_0) & \text{oppure} & x_0 \not \in A
     \end{array}
     \]
     Allora possiamo definire una nuova funzione $\tilde{f}: A \cup \{x_0\} \to \mathbb{R}$ che chiamiamo \textbf{prolungamento di $f$ in $x_0$} e la definiamo come
     \[
     \tilde{f}(x) = \begin{cases}
        f(x) & x \in A \setminus \{x_0\}\\
        l & x = x_0
     \end{cases}
     \]
\end{definizione}
Vediamo un esempio pratico.

\begin{esempio}{}{}
    \[
        f(x) = \frac{\log(1+x)}{x}
    \]
\end{esempio}
Notiamo che il dominio di quesa funzione è $\mathbb{D}(f) = x > -1 \land x \neq 0$, quindi possiamo vedere se possiamo prolungare in $x=0$, e notiamo che 
\[
    \lim_{x\to 0}\frac{\log(1+x)}{x} = 1
\]
Quindi visto che esiste il limite e $0 \not \in \mathbb{D}(f)$, e quindi possiamo prolungare la funzione:
\[
    \tilde{f}(x) =\begin{cases}
        \frac{\log(1+x)}{x} & x \in  (-1, 0) \cup (0, +\infty)\\
        1 & x = 0
     \end{cases}
\]
In questo modo abbiamo reso continua una funzione che prima era discontinua.

\newpage

\textbf{N.B.} con la definzione che abbiamo dato di prolungamento, potremmo estendere anche per $\pm \infty$, infatti se la funzione ha limite ad infinito e sicuramente $\pm \infty \not \in \mathbb{R}$, quindi potremmo fare anche estensioni del tipo   
\[
\lim_{x\to\pm\infty} \arctan(x) = \pm\frac{\pi}{2}
\]
Di conseguenza possiamo scrivere che 
\[
     \tilde{f}(x) = \begin{cases}
        \arctan(x) & x \in \mathbb{R} \\
        \frac{\pi}{2} & x = +\infty \\
        -\frac{\pi}{2} & x = -\infty 
     \end{cases}
\]
Pertanto possiamo calcolare la funzione nei punti $\pm \infty$, e possiamo scrivere $\tilde{f}(+\infty) = \frac{\pi}{2}$, visto che ora $+\infty$ è un punto del dominio. Questo può essere comodo per dimostrare certi problemi, però è meno intuitivo. Quindi per non rendere le definizioni ambigue, definiamo \textbf{prolungamento di $f$ nei numeri reali} il  prolungamento dove $x\neq \pm \infty$.D'ora in poi, quando scriverò prolungamento di una funzione, darò per scontato che sia un prolungamento nei reali. 


\addcontentsline{toc}{subsection}{Classificazione Punti di Discontinità}
\begin{definizione}{Punti di Discontinità}{}
    Sia $\varnothing \neq A \subseteq \mathbb{R}$, $f: A \to \mathbb{R}$ e $x_0 $ punto di accumulazione in $A$, e diciamo che \textbf{$f$ presenta una discontinità} nel punto $x_0$, se $f$ non è continua in $x_0$. Definiamo 3 tipologie di discontinità:
    \begin{itemize}
        \item definiamo discontinità \textbf{ eliminabile} un punto $x_0$ se possiamo fare un prolungamento di $f$ in $x_0$. 
        \item definiamo discontinità \textbf{I specie} se 
        \[
        \begin{array}{c @{\qquad} c}
            \displaystyle\exists \lim_{x\to x_0^-} f(x) =l_{sx} \in \mathbb{R}& \displaystyle\exists \lim_{x\to x_0^+} f(x) =l_{dx}\in \mathbb{R}
        \end{array}
        \] 
        Ma $l_{sx} \ne l_{dx}$. Questa discontinità è detta anche \textbf{Salto}, infatti possiamo definire il salto come $S = l_{dx} - l_{sx}$.
        \item definiamo discontinità \textbf{II specie} tutte le altre casistiche che non rientrano nella discontinità eliminabile o di I specie. 
    \end{itemize}
\end{definizione}

Vediamo qualche esempio
\begin{esempio}{}{}
    Determinare le discontinuità di 
    \[
        f(x) = \frac{x^2-3x+2}{x-2}
    \]
\end{esempio}

In primis notiamo che abbiamo un denominatore, quindi necessitiamo che il denominatore sia diverso da zero: $x-2 \ne 0 \implies x \neq 2$, quindi $\mathbb{D}(f) = \mathbb{R} \setminus \{2\}$. Di conseguenza nel punto $x=2$ la funzione non sarà continua. Capiamo che specie di discontinità è
\[
\lim_{x\to 2}\frac{x^2-3x+2}{x-2} = \lim_{x\to 2}\frac{(x-2)(x-1)}{x-2} =  \lim_{x\to 2} (x-1) = 1
\] 
Quindi notiamo che ricadiamo nella discontinità eliminabile, visto che esiste il limite nel punto. E quindi è anche estendibile:
\[
    \tilde{f}(x) = \begin{cases}
        \frac{x^2-3x+2}{x-2} & x \neq 2 \\
        1 & x=2
    \end{cases}
\]
Guardiamo il grafico delle 2 funzioni

\[
\begin{array}{c @{\qquad}@{\qquad} c}

% --- Primo grafico: f(x) con buco ---
% --- Primo grafico: f(x) con buco (pallino vuoto) ---
\begin{tikzpicture}
\begin{axis}[
    xmin=-1, xmax=4,
    ymin=-3, ymax=3,
    axis equal,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=7cm
]
    % Linea x-1 escluso x=2
    \addplot[blue, ultra thick, domain=-1:1.999] ({x},{x-1});
    \addplot[blue, ultra thick, domain=2.001:4] ({x},{x-1});

    % Pallino davvero vuoto in x=2
    % Cerchio bianco sotto
    \addplot[only marks, mark=*, mark size=3pt, white] coordinates {(2,1)};
    % Cerchio blu sopra
    \addplot[only marks, mark=o, mark size=3pt, blue] coordinates {(2,1)};

    \node[blue] at (axis cs:2.3,2.2) {$f(x)$};
\end{axis}
\end{tikzpicture}
&
% --- Secondo grafico: prolungamento continuo ---
\begin{tikzpicture}
\begin{axis}[
    xmin=-1, xmax=4,
    ymin=-3, ymax=3,
    axis equal,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=7cm
]
    % Linea x-1
    \addplot[red, ultra thick, domain=-1:4] ({x},{x-1});

    % Punto chiuso in x=2
    \addplot[only marks, mark=*, mark size=3pt, red] coordinates {(2,1)};

    \node[red] at (axis cs:2.3,2.2) {$\tilde f(x)$};
\end{axis}
\end{tikzpicture}

\end{array}
\]
\begin{esempio}{}{}
     Determinare le discontinuità di 
     \[
        f(x) = \arctan\left(\frac{1}{x}\right)
     \]
\end{esempio}
Anche qua come prima dobbiamo stare attenti al denominatore: $x\ne 0$, e quindi dobbiamo controllare solamente il punto $x=0$
\[
\lim_{x\to 0}  \arctan\left(\frac{1}{x}\right)
\]
Però ricordiamo che $\lim_{x\to 0}\frac{1}{x}$ perchè dobbiamo dividere nel limite destro e limite sinistro
\[
\begin{array}{c @{\qquad} c}
    \displaystyle\lim_{x\to 0^+}  \arctan\left(\frac{1}{x}\right) = \frac{\pi}{2} & \displaystyle\lim_{x\to 0^-}  \arctan\left(\frac{1}{x}\right)= -\frac{\pi}{2}
\end{array}
\]
Quindi visto che esistono il limite destro e sinitro ma sono valori finiti ma diversi, ricadiamo nella discontinità di I specie, e possiamo calcolare il salto $S = \frac{\pi}{2} - \left(-\frac{\pi}{2}\right) = \pi$. Vediamo il grafico
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-4, xmax=4,
    ymin=-2, ymax=2,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=8cm,
    samples=400
]

    % funzione corretta: atan() in deg → convertire in rad
    \addplot[blue, ultra thick, domain=-4:-0.05]
        (x,{atan(1/x)*pi/180});
    \addplot[blue, ultra thick, domain=0.05:4]
        (x,{atan(1/x)*pi/180});

   % --- Palline vuote negli asintoti ---
    % Punto bianco sotto
    \addplot[only marks, mark=*, mark size=3pt, white]
        coordinates {(0,pi/2) (0,-pi/2)};
    % Cerchio esterno blu sopra
    \addplot[only marks, mark=o, mark size=3pt, blue]
        coordinates {(0,pi/2) (0,-pi/2)};
\begin{scope}
        \draw[
            decorate,
            decoration={brace, amplitude=8pt},
            thick,
            red
        ]
            (axis cs:0,pi/2) -- (axis cs:0,-pi/2)
            node[midway, right=5pt, yshift=-6pt, red] {$S=\pi$};
    \end{scope}
    % etichette
    \node[blue] at (axis cs:2,1.3) {$f(x)$};

\end{axis}
\end{tikzpicture}
\end{center}


\addcontentsline{toc}{subsection}{Teormea di Weierstrass}
\begin{teorema}{Teorema di Weierstrass}{}
    Siano $a,b \in \mathbb{R}$ con $a < b$, $f: [a, b] \to \mathbb{R}$, e $f \in C^0([a,b])$, allora $f$ ammette un massimo e un minimo in $[a, b]$. Cioè $\exists x_{min}, x_{max} \in [a, b]$ tali che 
    \[
    \begin{array}{c @{\qquad}@{\qquad} c}
        f(x_{min}) \leq f(x) \;\;\; \forall x \in [a,b] &f(x_{max}) \geq f(x) \;\;\; \forall x \in [a,b]
    \end{array}
    \]
\end{teorema}

\begin{proof}
    Dimostramo per il caso del massimo, il minimo è analogo.
    Dato che $f \in C^0([a,b])$ allora sappiamo che almeno esiste un estremo superiore
    \[
    M = \sup\{f(x) : x \in [a,b]\}
    \]
    E supponiamo che $M \in \mathbb{R}$, con la caratterizzazione dell'estremo superiore, sappiamo che $\forall \varepsilon > 0 $, $\exists x_\varepsilon \in [a, b]$ tale che
    \[
    f(x_\varepsilon) > M -\varepsilon \;\;\;\;\; 
    \]
    Di conseguenza possiamo scegliere $\varepsilon = \frac{1}{n}$, con $n \in \mathbb{N}$,e quindi
    \[
    f(x_n) > M -\frac{1}{n} \;\;\;\;\; \forall n \geq 1
    \]
    Ma visto che sappiamo che per ogni $n$ possiamo trovare un $x_n$ che soddisfa quella relazione, possiamo definire una successione $(x_n)_{n\in \mathbb{N}}$, che sappiamo che è limitato da
    \[  
        a \leq x_n \leq b\;\;\;\;\; \forall n \geq 1
    \]  
    Ora però per il teorema di Bolzano-Weierstrass, dato che la successione $(x_n)_{n\in \mathbb{N}}$ è limitata, sappiamo che esiste almeno una sottosuccessione, $(x_{\varphi(n)})_{n\in \mathbb{N}} \to l \in [a,b]$, per un qualsiasi valore di $l$. Ma dato che la nostra funzione è continua $\forall x \in [a,b]$, e dato che $l\in [a,b]$, allora 
    \begin{equation}\label{eq:weie1}
   \mathunderline{red}{ \lim_{n\to+\infty} f(x_{\varphi(n)}) = f(l)}
    \end{equation}
    Riusiamo la caratterizzazione degli estremi, e la definizione di estremo superiore e sappiamo che 
    \[
    M-\frac{1}{n} < f(x_n) < M \;\;\;\;\; \forall n \geq 1
    \] 
    Portando tutto al limite abbiamo che
    \[
    \lim_{n\to+\infty}M-\frac{1}{n} <\lim_{n\to+\infty} f(x_n) <\lim_{n\to+\infty} M
    \]
    \[
    M <\lim_{n\to+\infty} f(x_n) < M \implies \lim_{n\to+\infty} f(x_n) = M
    \]
    Ma dato che la serie converge ad $M$ con la successione  $(x_n)_{n\in \mathbb{N}}$, allora questo varrà per ogni sottosuccessione  $(x_{\varphi(n)})_{n\in \mathbb{N}}$, e quindi
    \begin{equation}\label{eq:weie2}
        \lim_{n\to+\infty} f(x_n) = M \implies \mathunderline{blue}{\lim_{n\to+\infty} f(x_{\varphi(n)}) = M}
    \end{equation}
    Ma combinando le informazioni (\ref{eq:weie1}) e(\ref{eq:weie2}) sappiamo che esiste almeno una sottosuccessione tale che
    \[
   \mathunderline{red}{f(l)=  \lim_{n\to+\infty}} \mathunderline{blue}{f(x_{\varphi(n)}) = M } \implies f(l) = M
    \] 
    Quindi abbiamo trovato un valore $l \in [a, b]$ tale che $f(l) = M$, ma $M$ è l'estremo superiore, di conseguenza $M$ è anche un massimo.
    \newpage
    Se invece $M = +\infty$, dobbiamo ripetere tutto il ragionamento, e quindi sappiamo che 
    \[
    \forall n \in \mathbb{N} \;\;\; \exists x_n \in [a, b] \;\;: \;\; f(x_n) > n
    \]
    Ma allora $x_n$ è limitata, e di conseguenza esiste almeno una sottosuccessione  $(x_{\varphi(n)})_{n\in \mathbb{N}} \to l \in [a,b]$, ma dato che $f$ è continua abbiamo che
    \begin{equation}\label{eq:weie3}
        \lim_{n\to+\infty} f(x_{\varphi(n)}) = f(l)
    \end{equation}
    Riutilizzando la caratterizzazione degli estremi sappiamo che
    \[
    f(x_n) > n
    \]
    ma portando al limite abbiamo che 
    \begin{equation}\label{eq:weie4}
    \lim_{n\to+\infty}  f(x_n) > \lim_{n\to+\infty}  n \implies \lim_{n\to+\infty}  f(x_n) = +\infty
    \end{equation}
    Ma questo varrà anche per ogni sottosuccessione $(x_{\varphi(n)})_{n\in \mathbb{N}}$, ma quindi campinando le informazioni (\ref{eq:weie3}) e (\ref{eq:weie3}) sappiamo che 
    \[
    \begin{aligned}
    \lim_{n\to+\infty}  f(x_{\varphi(n)}) = +\infty \\ 
    \lim_{n\to+\infty} f(x_{\varphi(n)}) = f(l)
    \end{aligned}
    \;\Rightarrow\;
    f(l) = +\infty 
    \]
    Ma così abbiamo scoperto che un punto $l \in [a,b]$ vale $+\infty$, ma ciò è impossibile visto che $f$ è continua in $[a,b]$, e quindi deve valutare solo valori finiti, quindi per assurdo, scopriamo che $M \neq +\infty$. 
\end{proof}

\addcontentsline{toc}{subsection}{Teormea di Bolzano}
\begin{teorema}{Teorema di Bolzano (o degli Zeri)}{}
     Siano $a,b \in \mathbb{R}$ con $a < b$, $f: [a, b] \to \mathbb{R}$, e $f \in C^0([a,b])$ e se $f(a) \cdot f(b) < 0$ allora 
     \[
        \exists c \in (a,b) \; : f(c) = 0 
     \]
\end{teorema}
\begin{proof}
    Dato che $f(a) \cdot f(b) < 0$, allora possono accadere due casistiche: $f(a) < 0 < f(b)$ oppure $f(b) < 0 < f(a)$, per comodità supponiamo che $f(a) < 0 < f(b)$, nell'altro caso è sufficiente porre $g(x) = -f(x)$, in questo modo $g(a) \cdot g(b) < 0$ e  anche che $g(a) < 0 < g(b)$, e così con la dimostrazione che segue abbiamo dimostrato entrambe le casistiche. 

    Definiamo \[
    \begin{array}{c @{\qquad}@{\qquad} c @{\qquad}@{\qquad} c }
        \displaystyle a_0 = a & \displaystyle c_0 =\frac{a_0+b_0}{2} & \displaystyle b_0 = b
    \end{array}\]
    E seguiamo il seguente algoritmo:
    \begin{itemize}
        \item Se $f(c_0) = 0$ abbiamo dimostrato il teorema, dato che $c_0 \in (a,b)$
        \item Se $f(c_0) < 0$, allora poniamo $a_1 = c_0$ e $b_1 = b_0$, in modo tale che $f(a_1) \cdot f(b_1) < 0$
        \item Se $f(c_0) > 0$, allora poniamo $a_1 = a_0$ e $b_1 = c_0$, in modo tale che $f(a_1) \cdot f(b_1) < 0$
    \end{itemize}

    \newpage

    Reiteriamo il procedimento, quindi definiamo 
    \[
    c_1 = \frac{a_1 + b_1}{2}
    \]
    E seguiamo l' algoritmo:
    \begin{itemize}
        \item Se $f(c_1) = 0$ abbiamo dimostrato il teorema, dato che $c_1 \in (a,b)$
        \item Se $f(c_1) < 0$, allora poniamo $a_2 = c_1$ e $b_2 = b_1$, in modo tale che $f(a_2) \cdot f(b_2) < 0$
        \item Se $f(c_1) > 0$, allora poniamo $a_2 = a_1$ e $b_2 = c_1$, in modo tale che $f(a_2) \cdot f(b_2) < 0$
    \end{itemize}
    All'n-esimo passo, qualora non fosse mai stata verificata la prima condizione dell'algoritmo, avremo che
    \[
    c_n = \frac{a_n + b_n}{2}
    \]
    \begin{itemize}
        \item Se $f(c_n) = 0$ abbiamo dimostrato il teorema, dato che $c_n \in (a,b)$
        \item Se $f(c_n) < 0$, allora poniamo $a_{n+1} = c_n$ e $b_{n+1} = b_n$
        \item Se $f(c_n) > 0$, allora poniamo $a_{n+1} = a_n$ e $b_{n+1} = c_n$
    \end{itemize}
    Con questo, possiamo definire le due successioni $(a_n)_{n\in \mathbb{N}}$ e $(b_n)_{n\in \mathbb{N}}$, e sappiamo che
    \begin{enumerate}[label=(\roman*)]
        \item $f(a_n) \le 0 $ $\forall n \in \mathbb{N}$
        \item $f(b_n) \ge 0 $ $\forall n \in \mathbb{N}$
    \end{enumerate}
    \begin{itemize}
        \item $(a_n)_{n\in \mathbb{N}}$ è crescente, dato che ad ogni passaggio o resta uguale al passaggio precedente oppure diventa $\frac{a_n + b_n}{2}$, che dato che $a_n < b_n$ sappiamo che 
        \[
           a_{n+1} = \frac{a_n + b_n}{2} \geq \frac{a_n + a_n}{2} = \frac{2a_n }{2} = a_n
        \]
        \item $(b_n)_{n\in \mathbb{N}}$ è decrescente, per lo stesso ragionamento di $(a_n)_{n\in \mathbb{N}}$
    \end{itemize}
    Dato che entrambe le successioni sono monotone e limitate sappiamo che convegono a dei valori, che chiamiamo $a_n \to A \in (a, b)$ e $b_n \to B \in (a, b)$. In più, dato che ad ogni passaggio prendiamo la metà degli estremi, sappiamo che 
    \[
    b_n - a_n = \frac{b-a}{2^n} \implies b_n = a_n +\frac{b-a}{2^n}
    \]
    Se portiamo al limite questa informazione scopriamo che
    \[
    \lim_{n\to+\infty} b_n = \lim_{n\to+\infty} \left(a_n +\frac{b-a}{2^n}\right) \implies B = A + 0 \implies B = A
    \]
    Se prendo $c = A = B$,  usando la continuità di $f$ e $(i)$, $(ii)$ 
    \[
        f(c) = f(A) = \lim_{n\to +\infty} f(a_n) \leq 0
    \]
    \[
        f(c) = f(B) = \lim_{n\to +\infty} f(b_n) \geq 0
    \]
    Da cui
    \[
    0 \leq f(c) \leq 0 \implies f(c) = 0
    \]
\end{proof}

\newpage
\textbf{N.B.} Il teorema ci dice che esiste almeno uno zero, ciò vuol dire che ne possono esistere molteplici. Per esempio se prendo $f(x) = \sin(x)$ e prendo $a=\frac{-\pi}{2}$ e $b=\frac{9\pi}{2}$, sappiamo che $\sin(x) \in C^0(\mathbb{R})$ e che $f(a) = \sin(-\frac{\pi}{2}) = -1$, $f(b) = \sin(\frac{9\pi}{2}) = 1$ e quindi $f(a)\cdot f(b) = -1\cdot 1 = -1 < 0$, quindi è applicabile il teorema di Bolzano, e quindi sappiamo che ne esiste almeno uno zero. Ma se andiamo a vedere il grafico. 

\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-1.5*pi, xmax=5.5*pi,
    ymin=-2, ymax=2,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=15cm,                  % <-- SOLO larghezza aumentata
    height=6cm,                  % <-- altezza controllata, non cresce
    samples=500,
    xtick={
        -3.1416, -1.5708, 0, 3.1416, 6.2832, 9.4248, 12.5664, 14.1372
    },
    xticklabels={
        $-\pi$, $-\frac{\pi}{2}$, $0$, $\pi$, $2\pi$, $3\pi$, $4\pi$, $\frac{9\pi}{2}$
    },
    ytick={-1,0,1}
]

    % --- Grafico sin(x) ---
    \addplot[blue, ultra thick, domain=-pi:5*pi] {sin(deg(x))};

    % --- Zeri ---
    \addplot[only marks, mark=*, blue, mark size=2pt]
        coordinates { (0,0) (pi,0) (2*pi,0) (3*pi,0) (4*pi,0)};

    % --- Punto rosso: a = -π/2 ---
    \addplot[only marks, mark=*, red, mark size=3pt]
        coordinates {(-pi/2, {-1})};
    \node[red] at (axis cs:-pi/2, -1.3) {$f(a)$};

    % --- Punto rosso: b = 9π/2 ---
    \addplot[only marks, mark=*, red, mark size=3pt]
        coordinates {(9*pi/2, {1})};
    \node[red] at (axis cs:9*pi/2, 1.3) {$f(b)$};

    % Etichetta funzione
    \node[blue] at (axis cs:4,1.3) {$\sin(x)$};

\end{axis}
\end{tikzpicture}
\end{center}

Ma notiamo subito che esistono 4 zeri. Può succedere che ci venga richiesto dell'esistenza di un unico zero, in quel caso bisogna controllare se è verificato il teorema di Bolzano (per vedere se ha degli zeri), e poi controllare se la funzione è monotona. Infatti se la funzione è monotona è verifica il teorema di Bolzano allora sappiamo che $\exists! c \in (a,b)$ tale che $f(c) =0$. Infatti con l'esempio del seno, sappiamo che il seno non è monotono, e pertanto abbiamo che ci sono più zeri. 

Invece se prendiamo la funzione $f(x)=e^x + x$ e prendiamo $a=-1$ e $b=0$, notiamo subito che $f \in C^0(\mathbb{R})$ e che $f(b)=f(0) = e^0 + 0 = 1$ e per $f(a)$ dobbiamo ricordarci che $e^{-1} < 1$, infatti $f(-1) = e^{-1} - 1 < 1 - 1 = 0$, e quindi il teorema è applicabile. 

In più notiamo che $e^x$ è monotona crescente, e anche $x$ è crescente e di conseguenza $f$ è crescente e per questo sappiamo che $\exists! c \in (-1,0)$ tale che $f(c) = 0$. Infatti con il grafico notiamo che
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-2, xmax=1.5,
    ymin=-3, ymax=6,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=400,
    xtick={-3,-2,-1,0,1,2},
    ytick={-2,0,2,4}
]

    % --- Funzione f(x) = e^x + x ---
    \addplot[blue, ultra thick, domain=-2:1.5] {exp(x) + x};

    % --- Punto a = -1 ---
    \addplot[
        only marks,
        mark=*,
        red,
        mark size=3pt
    ]
    coordinates {(-1, {exp(-1) - 1})};
    \node[red] at (axis cs:-1+0.1, {exp(-1)-1 - 0.7}) {$f(a)$};

    % --- Punto b = 0 ---
    \addplot[
        only marks,
        mark=*,
        red,
        mark size=3pt
    ]
    coordinates {(0, {exp(0) + 0})};
    \node[red] at (axis cs:0.25, {0.5}) {$f(b)$};

% --- Zero della funzione: c ≈ -0.567143 ---
    \addplot[only marks, mark=*, green!60!black, mark size=3pt]
        coordinates {(-0.567143, 0)};
    \node[green!60!black] at (axis cs:-0.567143, 0.8) {$f(c)$};

    % --- Etichetta funzione ---
    \node[blue] at (axis cs:0.6, 4.6) {$f(x)=e^x + x$};

\end{axis}
\end{tikzpicture}
\end{center}

Infatti graficamente abbiamo cioò che ci aspettavamo. Per calcolare $c$ possiamo usare il metodo usato nella dimostrazione del teorema di Bolzano, che è detto \textbf{metodo di bisezione}. Facciamo qualche passo per questo esempio:
\[
    c_0 = \frac{-1 +0}{2} = -0.5 \implies f(c_1) = e^{-0.5} + 0.5 \approx 0.1065
\]
\[
    a_1 = -1 \;\;\; b_1=-0.5 \;\;\; c_1 =-0.75 \implies f(c_1) \approx -0.2776 
\]
\[
a_2 = -0.75 \;\;\; b_2=-0.5 \;\;\; c_2 =-0.625 \implies f(c_2) \approx -0.090
\]
 E così via. Tramite una decina di iterazioni con un computer scopriamo che $c \approx -0.5671$.


\newpage

\addcontentsline{toc}{subsection}{Teormea dei Valori Intermedi}
\begin{teorema}{Teorema dei Valori Intermedi}{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$, $f: I \to \mathbb{R}$ e $f \in C^0(I)$ allora 
    \[
    \left(\inf_I(f), \sup_I(f)\right) \subseteq f(I)  \subseteq \left[\inf_I(f), \sup_I(f)\right]
    \]
\end{teorema}
\begin{proof}
    In primis possiamo notare che la disequazione 
    \[
     f(I)  \subseteq \left[\inf_I(f), \sup_I(f)\right] \implies \inf_I(f) \leq f(I) \leq \sup_I(f)
    \]
    Ma questo è ovvio, per la definizione di estremo superiore e inferiore. Pertanto dobbiamo solamente dimostrare l'altra disequazione. 
    
    Per dimostrare il teorema è sufficiente che  $\forall y \in \left(\inf_I(f), \sup_I(f)\right)$ $\exists x \in I : y = f(x)$. Perchè vuol dire che ad ogni valore di $y$ riesco a trovare una $x$ in modo tale che $y=f(x)$, e se riesco a dimostrarlo per tutti i valori di $y$ abbiamo dimostrato il teorema. Ora quindi supponiamo di prendere $y \in \left(\inf_I(f), \sup_I(f)\right)$, riscrivendo scopriamo che
    \[
        \mathunderline{red}{\inf_{I}f <} y  \mathunderline{blue}{<\sup_If}
    \]
    Da cui scopriamo che
    \begin{itemize}
        \item $\mathunderline{red}{\inf_{I}f < y}$ vuol dire che $y$ NON è un minorante, dato che è più grande dell' estremo inferiore. Quindi potremmo trovare $\forall y$ $\exists a \in I$ tale che
        \begin{equation}\label{eq:int1}
            \inf_{I}f < f(a) < y \implies f(a) -  y < 0
        \end{equation}
        \item $ \mathunderline{blue}{y <\sup_If}$ vuol dire che $y$ NON è un maggiorante, dato che è più piccolo dell'estremi superiore. Quindi potremmo trovare $\forall y$ $\exists b \in I$ tale che
        \begin{equation}\label{eq:int2}
            y < f(b) <\sup_If\implies f(b) -  y > 0
        \end{equation}
    \end{itemize}
    Per comodità supponiamo che $a<b$. Da qui definiamo $g:[a,b] \to \mathbb{R}$ come
    \[
        g(x) = f(x) - y \;\;\;\;\; \forall x \in [a,b]
    \]
    Notiamo che 
    \begin{enumerate}[label=(\roman*)]
        \item dato che, per ipotesi, $f(x) \in C^0(I)$ allora anche $g(x) \in C^0(I)$
        \item  Con (\ref{eq:int1}) deduciamo che $g(a) = f(a) - y < 0$
        \item  Con (\ref{eq:int2}) deduciamo che $g(b) = f(b) - y > 0$
    \end{enumerate}
    Ma queste tre condizioni rendono possibile l'applicazione del teorema di Bolzano, dato che $g(a)\cdot g(b) <0$, e quindi sappiamo che $\exists c \in (a,b)$ tale che 
    \[
        g(c) = 0\implies f(c) - y = 0 \implies f(c) = y
    \]
    Ma quindi  $\forall y \in \left(\inf_I(f), \sup_I(f)\right)$ riesco trovare un $\exists a, b \in I$, e abbiamo scoperto che $\exists c \in (a,b)$ tale che $f(c) = y$, e quindi abbiamo verificato il teorema.
\end{proof}
\newpage

\addcontentsline{toc}{subsection}{Corollario del Teorema dei Valori Intermedi}
\begin{corollario}{del Teorema dei Valori Intermedi}{}
     Sia $a,b \in \mathbb{R}$, $f: [a,b] \to \mathbb{R}$ e $f \in C^0([a,b])$ allora
    \[
        f([a,b]) = [\min_{[a,b]}f, \max_{[a,b]}f]
    \] 
\end{corollario}
\begin{proof}
    Per il teorema dei valori intermedi sappiamo che
    \[
        \left(\inf_{[a,b]}(f), \sup_{[a,b]}(f)\right) \subseteq f([a,b])  \subseteq \left[\inf_{[a,b]}, \sup_{[a,b]}\right]
    \]
    Ma con il teorema di Weierstrass sappiamo che la funzione ammette un massimo e un minimo, per la relazione dei estrimi e massimi e minimi, allora sappiamo che se esiste un massimo o minimo allora coincide con l'estremo superiore/inferiore, quindi l'equazione sopra diventa 
     \[
        \left(\min_{[a,b]}(f), \max_{[a,b]}(f)\right) \subseteq f([a,b])  \subseteq \left[\min_{[a,b]} f, \max_{[a,b]} f\right]
    \]
    Ma dato che $\max_{[a,b]}(f) \in f([a,b])$ e $\min_{[a,b]}(f) \in f([a,b])$, allora sappiamo che
    \[
    \left[\min_{[a,b]}(f), \max_{[a,b]}(f)\right] \subseteq f([a,b])  \subseteq \left[\min_{[a,b]} f , \max_{[a,b]}f \right] \implies  f([a,b])  = \left[\min_{[a,b]}f, \max_{[a,b]}f\right] 
    \]  
\end{proof}

\addcontentsline{toc}{subsection}{Continuità delle Funzioni Inverse}
\begin{teorema}{Continuità della funzione inversa}{}
     Sia $\varnothing \ne I \subseteq \mathbb{R}$, $f: I \to \mathbb{R}$ e $f \in C^0(I)$ e invertibile in $I$ allora 
     \[
     f^{-1} \in C^0(f(I)) \iff f \text{ è strettamente monotona}
     \]  
\end{teorema}
Con questo teorema possiamo trovare altre funzioni continue, per esempio $f(x) = a^x$ con $a \in (0,1) \cup (1,+\infty)$, noi sappiamo che $f \in C^0(\mathbb{R})$ e in più sappiamo che $f$ è strettamente crescente per $x \in (1,+\infty)$ e decrescente per $x \in (0,1)$, e quindi da questo sappiamo che la funzione inversa: $f(x) = \log_a(x)$ è continua nel suo dominio: $(0, +\infty)$. 

Poi possiamo trovare anche per le funzione trigonometriche inverse, ma dobbiamo fare delle eventuali constrizioni. Prendiamo la funzione $f(x)=\sin(x)$, è vero che è continua in $\mathbb{R}$, ma non è sempre monotona, però se noi prendiamo il seno da $[-\frac{\pi}{2}, \frac{\pi}{2}]$ allora in questo intervallo è strettamente crescente, quindi possiamo dedurre che $f(x) = \arcsin(x)$ è continua in $[-1,1]$. 

Stesso ragionamento, per $f(x) =\cos(x)$ se lo prendiamo per $x\in[0, \pi]$ allora la funzione è strettamente decrescente e quindi possiamo dedurre che $f(x)= \arccos(x)$ e che $f\in C^0([-1,1])$. 

 Ragionamento analogo anche per $f(x) = \tan(x)$, infatti se prendo la funzione nell'intervallo $(-\frac{\pi}{2}, \frac{\pi}{2})$, allora so che è monotona crescente e quindi $f(x) = \arctan(x)$ è continua in $\mathbb{R}$.
 
 Tutto ciò si può dire anche per le funzioni iperboliche.

 \newpage 
 \section{Derivabilità}

\addcontentsline{toc}{subsection}{Definizione di Derivata di una Funzione}
 \begin{definizione}{Derivata di una funzione in un punto }{}
    Sia $\varnothing \ne I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, $x_0\in I$, se esiste il seguente limite finito
    \[
        \exists \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} \in \mathbb{R}
    \]
    Diciamo che \textbf{$f$ è derivabile in $x_0$} e lo indichiamo con il simbolo 
    \[
        f'(x_0) =  \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}
    \]
    Se invece non esiste il limite oppure tende a $\pm \infty$ diciamo che non è derivabile.

    Diciamo che $f$  è \textbf{derivabile } (oppure \textbf{differenziabile}) se è derivabile per ogni punto del suo dominio.
 \end{definizione}
 \textbf{N.B.} la notazione di derivata può variare, infatti ci sono varie notazioni e possono aiutare a capire il contesto, infatti potreste trovare indicata la derivata di $f(x)$ in $x_0$ come:
 \begin{itemize}
    \item $f'(x_0)$ che è la più classica ed usata
    \item $\displaystyle\frac{df}{dx}(x_0)$ oppure $\displaystyle\frac{d}{dx}(f(x_0))$ nei contesti geometrici oppure nelle equazioni differenziali
    \item $\dot{f}(x_0)$ tendenzialemente si usa in fisica
    \item $D[f(x_0)]$ quando volgiamo indicare l'operatore derivata
    \item $\displaystyle\frac{\partial f}{\partial x}(x_0)$ quando usiamo funzioni a più dimensioni
 \end{itemize}
 Quindi quando trovate uno di questi simboli indicano sempre una derivata, ma in contesti differenti. Ora proviamo a calcolare quelle fondamentali
 \addcontentsline{toc}{subsection}{Calcolo delle Derivate Elementari}
 \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=c$ in $x_0 \in \mathbb{R}$ con $c \in\mathbb{R}$ 
 \end{esercizio}
 Usiamo la definizione di derivata
 \[
 f'(x_0) =  \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} =  \lim_{x\to x_0} \frac{c-c}{x-x_0} =  \lim_{x\to x_0} \frac{0}{x-x_0} = 0
 \]
Quindi per qualsiasi funzione costante abbiamo che la sua derivata è sempre e solo 0. Questo quando vedremo gli integrali porterà ad una proprietà unica degli integrali. 
 \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=x$ in $x_0 \in \mathbb{R}$  
 \end{esercizio}
 Usiamo sempre la definizione
  \[
 f'(x_0) =  \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} =  \lim_{x\to x_0} \frac{x-x_0}{x-x_0} =  1
 \]
 \newpage
  \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=x^2$ in $x_0 \in \mathbb{R}$ 
 \end{esercizio}
 Usiamo sempre la definizione
  \[
 f'(x_0) =  \lim_{x\to x_0} \frac{x^2-x^2_0}{x-x_0} =  \lim_{x\to x_0} \frac{(x-x_0)(x+x_0)}{x-x_0} = \lim_{x\to x_0}(x+x_0) = 2x_0
 \]
 Per ora non abbiamo ancora trovato un pattern comodo per le potenze, provamo a calcolare la potenza n-esima.
 \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=x^n$ in $x_0 \in \mathbb{R}$ con $n \in \mathbb{N}$ 
 \end{esercizio}
 Usiamo sempre la definizione
  \[
 f'(x_0) =  \lim_{x\to x_0} \frac{x^n-x^n_0}{x-x_0} 
 \]
 Per calcolarlo dobbiamo riutilizzare la formula che abbiamo usato anche per il limite $x^n\to x^n_0$ e quindi
 \begin{align*}
    \lim_{x\to x_0} \frac{x^n-x^n_0}{x-x_0}  &= \lim_{x\to x_0} \frac{\displaystyle(x-x_0)\left(\sum_{k=0}^{n-1}x^{n-1-k}x^k_0\right)}{x-x_0} = \lim_{x\to x_0} \left(\sum_{k=0}^{n-1}x^{n-1-k}x^k_0\right) \\
    &= \sum_{k=0}^{n-1}x^{n-1-k}_0x^k_0 = \sum_{k=0}^{n-1}x^{n-1-k + k}_0 = \sum_{k=0}^{n-1}x^{n-1}_0 = nx_0^{n-1}
\end{align*}
Per le prossime derivate dobbiamo un attimino modificare la definizione di derivata, infatti se alla definizione applico un cambio di variabole con $h = x-x_0$, e quindi $x = h+x_0$, abbiamo che
\[
f'(x_0) = \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} =  \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)}{h}
\]
Con questo possiamo sostituire $x_0$ con $x$ per calcolare la funzione derivata su qualsiasi punto anzichè su un punto solo. Quindi la nuova definizione di derivata (che è uguale a quella di prima ma può essere più comoda per certi conti) è
\[
f'(x) = \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}
\]
 \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=x^\alpha$ in $x> 0$ con $\alpha \in \mathbb{R}$ 
 \end{esercizio}
Usiamo sempre la definizione
 \begin{align*}
    f'(x_0) &=  \lim_{h\to 0} \frac{(x+h)^\alpha-x^\alpha}{h} = \lim_{h\to 0} \frac{\left(x\left(1+\frac{h}{x}\right)\right)^\alpha-x^\alpha}{h} \\
 &= \lim_{h\to 0} \frac{x^\alpha\cdot\left(1+\frac{h}{x}\right)^\alpha-x^\alpha}{h} = \lim_{h\to 0} \frac{x^\alpha\cdot\left(\left(1+\frac{h}{x}\right)^\alpha-1\right)}{h}
 \end{align*}
Possiamo applicare un cambio di variabile con $t = \frac{h}{x}$, e dato che $x>0$ non ci reca alcun danno.
\[
\lim_{h\to 0} \frac{x^\alpha\cdot\left(\left(1+\frac{h}{x}\right)^\alpha-1\right)}{h} = \lim_{t\to 0} x^\alpha\cdot \frac{(1+t)^\alpha-1}{tx} = \lim_{t\to 0} x^{\alpha-1}\cdot \frac{(1+t)^\alpha-1}{t} 
\]
Ora dobbiamo usare un limite notevole al contrario. Infatti noi sappiamo che 
\[
 e^t \sim 1+t \;\;\; \text{per } t\to0 
\]
E quindi nella stramagioranza degli esercizi sostituavamo $1+t$ al posto di $e^t$, ma possiamo fare anche il contrario. In effetti in questo caso ci conviene sostituire $1+t$ con $e^t$. Quindi
\[
\lim_{t\to 0} x^{\alpha-1}\cdot \frac{(1+t)^\alpha-1}{t} \sim \lim_{t\to 0} x^{\alpha-1}\cdot \frac{(e^t)^\alpha-1}{t} = \lim_{t\to 0} x^{\alpha-1}\cdot \frac{e^{\alpha t}-1}{t} = \alpha \cdot x^{\alpha-1}
\]
  



\begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=\sin(x)$  
 \end{esercizio}
  Usiamo sempre la definizione
\begin{align*}
     f'(x) &=  \lim_{h\to 0} \frac{f(x+h)-f(x)}{h} = \lim_{h\to 0} \frac{\sin(x+h)-\sin(x)}{h} \\
     &= \lim_{h\to 0} \frac{\mathunderline{red}{\sin(x+h)}-\sin(x)}{h} = \lim_{h\to 0} \frac{\mathunderline{red}{\sin(x)\cos(h)+\sin(h)\cos(x)}-\sin(x)}{h} \\
&= \lim_{h\to 0} \left(\frac{\sin(x)\cos(h)-\sin(x)}{h} + \frac{\sin(h)\cos(x)}{h} \right)
    \end{align*}
    Sistemando i limiti notevoli abbiamo
\[
=\lim_{h\to 0} \left(\frac{\sin(x)\cos(h)-\sin(x)}{h} + \frac{\sin(h)\cos(x)}{h} \right) = \lim_{h\to 0} \left(\frac{\sin(x)(\cos(h)-1)}{h} + \frac{\sin(h)}{h} \cos(x)\right)  
\]
\[
= \lim_{h\to 0} \left(\sin(x)\frac{(\cos(h)-1)}{h^2} \cdot h + \frac{\sin(h)}{h} \cos(x)\right)  = \sin(x)\cdot \left(\frac{-1}{2}\right) \cdot 0 + 1\cdot \cos(x) = \cos(x)       
\]
Ragionamento analogo per il coseno
  \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=\cos(x)$  
 \end{esercizio}
\begin{align*}
     f'(x) &=  \lim_{h\to 0} \frac{f(x+h)-f(x)}{h} = \lim_{h\to 0} \frac{\cos(x+h)-\cos(x)}{h} \\
     &= \lim_{h\to 0} \frac{\mathunderline{red}{\cos(x+h)}-\cos(x)}{h} = \lim_{h\to 0} \frac{\mathunderline{red}{\cos(x)\cos(h)-\sin(h)\sin(x)}-\cos(x)}{h} \\
&= \lim_{h\to 0} \left(\frac{\cos(x)\cos(h)-\cos(x)}{h} - \frac{\sin(h)\sin(x)}{h} \right)
    \end{align*}
    Per lo stesso ragionamento di prima
    \[
=\lim_{h\to 0} \left(\cos(x)\frac{\cos(h)-1}{h} - \frac{\sin(h)\sin(x)}{h} \right) = \cos(x)\cdot \left(\frac{-1}{2}\right) \cdot 0 - 1\cdot \sin(x) = -\sin(x)  
\]

Attenzione quindi che la derivata del coseno è meno seno, cosa che può portare a confunsione anche perchè con gli integrali sarà il contrario, quindi attenzione
\[
\begin{array}{c @{\qquad}@{\qquad} c}
    \displaystyle \frac{d}{dx}(\sin(x)) = \cos(x) &\displaystyle \frac{d}{dx}(\cos(x)) = -\sin(x)
\end{array}
\]
  \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=e^x$  
 \end{esercizio}
 \[
    f'(x) =  \lim_{h\to 0} \frac{e^{x+h} - e^x}{h} = \frac{e^{x}\cdot e^h - e^x}{h} = \frac{e^{x}(e^h - 1)}{h} =e^x
 \]
 Una caratteristica che ci tornerà utile con le equazioni differenziali è che l'esponenziale è l'unica funzione la cui derivatà è uguale alla funzione di partenza, e vedremo come ci aiuterà a risolvere le equazioni differenziali.
   \begin{esercizio}{}{}
    Calcolare da derivata di $f(x)=\log(x)$  
 \end{esercizio}
 \begin{align*}
    f'(x) &=  \lim_{h\to 0} \frac{\log(x+h) - \log(x)}{h} =  \lim_{h\to 0} \frac{1}{h}\cdot \log\left(\frac{x+h}{x}\right) \\
    &= \lim_{h\to 0} \frac{1}{h}\cdot \log\left(1 + \frac{h}{x}\right) =\lim_{h\to 0} \log\left(\left(1 + \frac{h}{x}\right) ^{\frac{1}{h}}\right) 
 \end{align*}
 Ora è sufficiente fare un cambio di variabile con $t = \frac{h}{x}$, questo è possibile dato che $x>0$ per il dominio di $f(x)$.
\begin{align*}
 \lim_{h\to 0} \log\left(\left(1 + \frac{h}{x}\right) ^{\frac{1}{h}}\right)  &= \lim_{t\to 0} \log\left(\left(1 + t\right) ^{\frac{1}{tx}}\right) =  \lim_{t\to 0} \log\left(\left(\left(1 + t\right) ^{\frac{1}{t}}\right)^{\frac{1}{x}}\right)\\
    &= \lim_{t\to 0} \frac{1}{x}\log\left(\left(1 + t\right) ^{\frac{1}{t}}\right) = \frac{1}{x} \log(e) = \frac{1}{x}
\end{align*}
In seguito una tabella che racchiude le principali derivate

\begin{center}
\begin{tabular}{c @{\quad} c}

% prima tabella (solo tabular)
\begin{tabular}{c|c}
Funzione Base & Derivata              \\ \hline
$c$           & $0$                   \\
$\sin(x)$     & $\cos(x)$             \\
$e^x$         & $e^x$
\end{tabular}
&
% seconda tabella (solo tabular)
\begin{tabular}{c|c}
Funzione Base & Derivata              \\ \hline
$x^\alpha$    & $\alpha x^{\alpha-1}$ \\
$\cos(x)$     & $-\sin(x)$            \\
$\log(x)$     & $\frac{1}{x}$
\end{tabular}

\end{tabular}
\end{center}

 \newpage

 \addcontentsline{toc}{subsection}{Interpretazione Geometrica di Derivata}
Ora vediamo da dove viene fuori la derivata, infatti se prendiamo una qualsiasi funzione e scegliamo un punto $x_0$ e prendiamo anche un altro punto $x_0+h$, con $h>0$, e tracciamo la retta passante per $(x_0, f(x_0))$ e $(x_0+h, f(x_0+h))$ notiamo che
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=0, xmax=4,
    ymin=-1, ymax=2.5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=400,
    xtick=\empty,
    ytick=\empty
]

    % --- Funzione f(x) = ln(x) ---
    \addplot[blue, ultra thick, domain=0.3:4] {ln(x)};
\node[blue] at (axis cs:3.65,1.75) {$y=f(x)$};
    % --- Punto x0 = 2 ---
    \addplot[
        only marks,
        mark=*,
        red,
        mark size=2pt
    ]
    coordinates {(2, {ln(2)})};

 --- Linea tratteggiata in x0 = 2 ---
    \addplot[dashed, gray, thick] coordinates {(2,-0.5) (2,2.5)};
    \node at (axis cs:2,-0.5) [below] {$x_0$};

    % --- Linea tratteggiata in x0+h = 3 ---
    \addplot[dashed, gray, thick] coordinates {(3,-0.5) (3,2.5)};
    \node at (axis cs:3,-0.5) [below] {$x_0 + h$};

    % --- Linea tratteggiata orizzontale per f(x0) = ln(2) ---
\addplot[dashed, gray, thick] coordinates {(4, {ln(2)}) (0, {ln(2)})};
\node at (axis cs:0, {ln(2)}) [left] {$f(x_0)$};

% --- Linea tratteggiata orizzontale per f(x0+h) = ln(3) ---
\addplot[dashed, gray, thick] coordinates {(4, {ln(3)}) (0, {ln(3)})};
\node at (axis cs:0, {ln(3)}) [left] {$f(x_0+h)$};



    % --- Punto x0 + h = 3 ---
    \addplot[
        only marks,
        mark=*,
        red,
        mark size=2pt
    ]
    coordinates {(3, {ln(3)})};

    % --- Retta secante tra 2 e 3 ---
    % Pendenze: (ln(3)-ln(2)) / (3-2) = ln(3)-ln(2)
    \addplot[red, ultra thick, domain=0:4]
        {ln(2) + (ln(3)-ln(2))*(x-2)};

\end{axis}
\end{tikzpicture}
\end{center}

Notiamo che il limite che abbiamo calcolato fino ad ora non è altro che il coefficente angolare della retta disegnata in rosso. Infatti
\[
m = \frac{\Delta y}{\Delta x} = \frac{f(x_0+h) - f(x_0)}{x_0+h - x_0} = \frac{f(x_0+h) - f(x_0)}{h}
\]
Questo rapposto è definito anche come \textbf{rapporto incrementale}. Ma la derivata è il limite di questo rapporto, quindi con la derivata stiamo cercando di avvicinarci il più possibile alla retta tangente nel punto $x_0$, infatti più che $h\to 0$ più che la retta rossa tende alla retta tangente effettiva (quella in verde)
\begin{center}
\begin{tabular}{c@{\qquad}c@{\qquad}c}

% ================= PRIMO GRAFICO: h = 1.5 =================
\begin{tikzpicture}
\begin{axis}[
    xmin=0, xmax=4,
    ymin=-1, ymax=2.5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=6cm,
    height=6cm,
    samples=400,
    xtick=\empty,
    ytick=\empty
]
    \addplot[blue, ultra thick, domain=0.3:4] {ln(x)};

    % x0 = 2
    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(2, {ln(2)})};

    % h = 1.5 → x0+h = 3.5
    \addplot[dashed, gray, thick] coordinates {(2,-0.5) (2,2.5)};
    \addplot[dashed, gray, thick] coordinates {(3.5,-0.5) (3.5,2.5)};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(2)}) (0, {ln(2)})};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(3.5)}) (0, {ln(3.5)})};

    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(3.5, {ln(3.5)})};

% tangente in x=2
    \addplot[green!60!black, ultra thick, domain=0:4]
        {ln(2) + (1/2)*(x - 2)};

    % secante
    \addplot[red, ultra thick, domain=0:4]
        {ln(2) + (ln(3.5)-ln(2))/1.5*(x-2)};

    

\end{axis}
\end{tikzpicture}
&
% ================= SECONDO GRAFICO: h = 1 =================
\begin{tikzpicture}
\begin{axis}[
    xmin=0, xmax=4,
    ymin=-1, ymax=2.5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=6cm,
    height=6cm,
    samples=400,
    xtick=\empty,
    ytick=\empty
]
    \addplot[blue, ultra thick, domain=0.3:4] {ln(x)};

    % x0 = 2
    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(2, {ln(2)})};

    % h = 1 → x0+h = 3
    \addplot[dashed, gray, thick] coordinates {(2,-0.5) (2,2.5)};
    \addplot[dashed, gray, thick] coordinates {(3,-0.5) (3,2.5)};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(2)}) (0, {ln(2)})};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(3)}) (0, {ln(3)})};

    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(3, {ln(3)})};
 %tangente in x=2
    \addplot[green!60!black, ultra thick, domain=0:4]
        {ln(2) + (1/2)*(x - 2)};
    % secante
    \addplot[red, ultra thick, domain=0:4]
        {ln(2) + (ln(3)-ln(2))*(x-2)};

    

\end{axis}
\end{tikzpicture}
&
% ================= TERZO GRAFICO: h = 0.5 =================
\begin{tikzpicture}
\begin{axis}[
    xmin=0, xmax=4,
    ymin=-1, ymax=2.5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=6cm,
    height=6cm,
    samples=400,
    xtick=\empty,
    ytick=\empty
]
    \addplot[blue, ultra thick, domain=0.3:4] {ln(x)};

    % x0 = 2
    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(2, {ln(2)})};

    % h = 0.5 → x0+h = 2.5
    \addplot[dashed, gray, thick] coordinates {(2,-0.5) (2,2.5)};
    \addplot[dashed, gray, thick] coordinates {(2.5,-0.5) (2.5,2.5)};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(2)}) (0, {ln(2)})};
    \addplot[dashed, gray, thick] coordinates {(4, {ln(2.5)}) (0, {ln(2.5)})};

    \addplot[only marks, mark=*, red, mark size=2pt]
    coordinates {(2.5, {ln(2.5)})};
\addplot[green!60!black, ultra thick, domain=0:4]
        {ln(2) + (1/2)*(x - 2)};
    % secante
    \addplot[red, ultra thick, domain=0:4]
        {ln(2) + (ln(2.5)-ln(2))/0.5*(x-2)};

    % tangente
    

\end{axis}
\end{tikzpicture}

\end{tabular}
\end{center}

Di conseguenza abbiamo capito che la derivata è il limite del rapporto incrementale, e che quindi è il coefficente angolare della retta tangete in un punto ad una qualsiasi funzione $y=f(x)$. Quindi in generale la retta tangente ad una funzione in un punto è
\[
y = f(x_0) + f'(x_0) (x-x_0)
\]

Possiamo notare che se calcoliamo questo limite della differenza tra la funzione $f(x)$ e la sua retta tangente e lo dividiamo per $x-x_0$ notiamo che
\[
\lim_{x\to x_0} \frac{f(x) - (f(x_0) + f'(x_0)(x-x_0))}{x-x_0} = \lim_{x\to x_0} \left(\frac{f(x) - f(x_0) }{x-x_0} - f'(x_0)\right) 
\]
Ma dato che sappiamo che la funzione è derivabile (visto che esiste la retta tangente) in $x_0$ allora il limite tende a $f'(x_0)$
\[
\lim_{x\to x_0} \left(\frac{f(x) - f(x_0) }{x-x_0} - f'(x_0)\right)= f'(x_0) - f'(x_0) = 0
\]
Dato che è venuto fuori 0, allora se definiamo $a=f'(x_0)$ e $b=f(x_0) - f'(x_0)x_0$ possiamo dire
\[
f(x) - (ax+b) = o(x-x_0) \;\;\;\;\; \text{per } x\to x_0
\]

\addcontentsline{toc}{subsection}{Definizione di Retta Tangente}
\begin{definizione}{Retta Tangente}{}
    In maniera più formale, definiamo \textbf{retta tangente} a $f$ nel punto $x_0$ la retta di equazione
    \[
        y=ax+b \;\;\;\;\; a,b \in \mathbb{R}
    \]
    tale che
    \[
        f(x) - (ax+b) = o(x-x_0) \;\;\;\;\; \text{per } x\to x_0
    \]
\end{definizione}
Con questa formula si possono ricavare, ormai già ben noti, i prodotti notevoli, infatti se prendiamo $f(x) = e^x$, allora calcoliamo la retta tangente in $x_0 = 0$
\[
\begin{array}{c @{\qquad}@{\qquad} c}
    f(x_0) = e^0 = 1 & f'(x_0) = e^0 = 1
\end{array}
\] 
Quindi la retta tangente ha l'equazione
\[
    y=1 + 1(x-0) \implies y =x+1
\]
E quindi dato che è la retta tangente sappiamo che
\[
e^x - (x+1) = o(x-x_0) \;\;\;\;\; \text{per } x\to 0
\]
Da cui ricaviamo
\[
e^x  = x+1 + o(x-x_0) \;\;\;\;\; \text{per } x\to 0
\] 
Che conoscevamo già. Poi si possono dimostrare anche tutti gli altri limiti notevoli.

\begin{esercizio}{}{}
    Determinare l'equazione delle due rette passanti per $(1, -3)$ e tangenti al grafico di $f(x) = x^2$
\end{esercizio}
Possiamo trovare la retta generale ad $f(x)=x^2$ per un punto generale $x_0$, che poi decideremo con l'altra condizione. Quindi in primis dobbiamo alcolare la derivata di $f(x)$, che con le formule che abbiamo scoperto prima abbiamo che
\[
    \frac{d}{dx}(x^2) = 2x
\]
Quindi la retta tangente sarà
\[
\begin{array}{c @{\qquad}@{\qquad} c}
    f(x_0) = x^2_0 & f'(x_0) = 2x_0
\end{array}
\]
\[
y=  x^2_0 +2x_0 (x-x_0) \implies y= x^2_0 + 2x_0 x  +2x_0^2  \implies y=  2x_0 x -x^2_0 
\]
Ora questa è la retta generica, noi dobbiamo trovare quelle che passano per $(1,-3)$ quindi sostituiamo le coordinate
\[
-3 = 2x_0 \cdot  1 - x^2_0 \implies    -x^2_0 + 2x_0 +3 = 0 \implies x_{0} = -1  \lor x_{0} = 3
\]
Quindi le due rette sono
\[
\begin{array}{c @{\qquad} c}
    y=  2(-1) x -(-1)^2 \implies y=-2x -2 & y=  2(3) x -(3)^2 \implies y=6x -9
\end{array}
\]
\newpage
\addcontentsline{toc}{subsection}{Relazione tra Continuità e Derivabilità}
\begin{teorema}{Relazione tra Continuità e Derivabilità}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$. Se $f$ è derivabile in $x_0 \in I$ allora è anche continua.
    \[
    \text{Derivabilità } \implies \text{ Continuità}
    \]
\end{teorema}
\begin{proof}
    Dato che $f$ è derivabile in $x_0$ allora esiste la retta tangente nel punto $x_0$. Quindi se calcoliamo il limite di $f$ possiamo sostituirlo con la formula della retta tangente
    \begin{align*}
        \lim_{x\to x_0} f(x) &= \lim_{x\to x_0} \left(f(x_0) + f'(x_0) (x-x_0) + o(x-x_0)\right) \\
        &= f(x_0) + f'(x_0) \cdot\lim_{x\to x_0} (x-x_0) +\lim_{x\to x_0}  \left(\frac{o(x-x_0)}{x-x_0} \cdot (x-x_0)\right) \\
        &=  f(x_0) + f'(x_0) \cdot 0 + 0\cdot 0 = f(x_0)
    \end{align*}
\end{proof}
Questa informazione può essere molto utile negli esercizi perchè se vediamo che un punto non è continuo, allora non sarà nemmeno derivabile. 
 
\textbf{N.B.} Non è vero il contrario, infatti se prendiamo $f(x) = |x|$, per i limiti sappiamo che è continua in $\mathbb{R}$, però se proviamo a calcolare la derivata in $x=0$ notiamo che 
\[
    \lim_{x\to x_0} \frac{|x| -|x_0|}{x - x_0} =\lim_{x\to 0} \frac{|x| -|0|}{x - 0}=  \lim_{x\to 0} \frac{|x|}{x}
\]
Se proviamo a distinguere il il caso del limite destro e sinistro, usando anche la proprietà del modulo tale che se $x > 0$ allora $|x| = x$, mentre se $x < 0$ allora $|x| = -x$, quindi
\[
\begin{array}{c @{\qquad}@{\qquad} c}
    \displaystyle\lim_{x\to 0^+} \frac{|x|}{x} = \lim_{x\to 0^+} \frac{x}{x} = 1 &  \displaystyle\lim_{x\to 0^-} \frac{|x|}{x} = \lim_{x\to 0^+} \frac{-x}{x} = -1
\end{array}
\]
Ma quindi limite destro e sinistro sono diversi e quindi il limite non esiste e questo vuol dire che la funzione non  è derivabile in quel punto. Con questo ragionamento possiamo dedurre che
\[
\frac{d}{dx}(|x|)= \begin{cases}
    1 & x>0 \\
    -1 & x <0
\end{cases}
\]

\addcontentsline{toc}{subsection}{Definizione di Derivata Destra e Sinistra}
\begin{definizione}{Derivata Destra e Sinistra}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, $x_0 \in I$ allora
    \begin{itemize}
        \item Se
    \[
        \exists \lim_{x\to x_0^+} \frac{f(x)-f(x_0)}{x-x_0} = \lim_{h\to 0^+} \frac{f(x+h)-f(x)}{h} \in \mathbb{R}
    \]
     diciamo che $f$ \textbf{è derivabile da destra in $x_0$} e lo indichiamo come $f'_+(x_0)$. 
    \item Se
    \[
        \exists \lim_{x\to x_0^-} \frac{f(x)-f(x_0)}{x-x_0} = \lim_{h\to 0^-} \frac{f(x+h)-f(x)}{h} \in \mathbb{R}
    \]
     diciamo che $f$ \textbf{è derivabile da sinistra in $x_0$} e lo indichiamo come $f'_-(x_0)$. 
    \end{itemize}
\end{definizione}


\addcontentsline{toc}{subsection}{Classificazione dei punti di non Derivabilità}
\begin{definizione}{Classificazione dei punti di non Derivabilità}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, $x_0 \in I$ punto di non derivabilità, allora 
    \begin{itemize}
        \item Se $f'_-(x_0) = f'_+(x_0) = +\infty$ oppure $f'_-(x_0) = f'_+(x_0) = -\infty$ allora diciamo che $f$ ha un \textbf{Flesso a tangente verticale} in $x_0$
        \item Se $f'_-(x_0) \in \mathbb{R} \lor f'_+(x_0) \in \mathbb{R}$ (chiaramente $f'_-(x_0) \neq f'_+(x_0) $, altrimenti sarebbe derivabile), allora diciamo che $f$ ha un \textbf{punto angoloso} in $x_0$
        \item Se $f'_-(x_0) -\infty \land f'_+(x_0) = +\infty$ oppure $f'_-(x_0) +\infty \land f'_+(x_0) = -\infty$ allora diciamo che $f$ ha una \textbf{cuspide} in $x_0$
    \end{itemize}
\end{definizione}
Vediamo degli esempi, per esempio la funzione $f(x) =\sqrt[3]{x}$, possiamo calcolare la derivata con le regole di prima
\[
\frac{d}{dx}(\sqrt[3]{x}) = \frac{d}{dx}(x^\frac{1}{3}) = \frac{1}{3} x^{\frac{1}{3}-1} = \frac{1}{3} x^{-\frac{2}{3}} = \frac{1}{3\sqrt[3]{x^2}}
\]
Ora possiamo calcolare la derivata destra e sinistra in $x=0$
\[
\begin{array}{c@{\qquad}@{\qquad}c}
    \displaystyle\lim_{x\to 0^+} \frac{1}{3\sqrt[3]{x^2}} = +\infty & \displaystyle\lim_{x\to 0^-} \frac{1}{3\sqrt[3]{x^2}} = +\infty 
\end{array}
\]
Quindi notiamo che è un flesso a tangente verticale, e vediamo graficamente che nel punto $x=0$ la funzione "impenna" drasticamente per poi riassestarsi.
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-8, xmax=8,
    ymin=-4, ymax=4,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=500
]

    % --- f(x) = cube root(x) (forma corretta!) ---
    \addplot[blue, ultra thick, domain=-8:8]
        {sign(x)*abs(x)^(1/3)};

    % Etichetta funzione
    \node[blue] at (axis cs:6,2.5) {$f(x)=\sqrt[3]{x}$};

\end{axis}
\end{tikzpicture}
\end{center}

Per il punto angoloso abbiamo già visto un esempio: il valore assoluto. Infatti avevamo visto che 
\[
\begin{array}{c @{\qquad} c}
    f'_-(0) = -1 & f'_+(0) = 1
\end{array}
\]
E graficamente notiamo che si forma un angolo in $x=0$
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-5, xmax=5,
    ymin=-0.5, ymax=5,
    axis equal=false,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=500
]

    % --- f(x) = |x| ---
    \addplot[blue, ultra thick, domain=-5:5]
        {abs(x)};

    % Etichetta funzione
    \node[blue] at (axis cs:4.5,3) {$f(x)=|x|$};

\end{axis}
\end{tikzpicture}
\end{center}

\newpage

Invece se prendiamo la funzione $f(x)=\sqrt{|x|}$ dobbiamo usare la definizione di derivata destra e sinistra
\[
\lim_{x\to 0^+} \frac{\sqrt{|x|} - \sqrt{|0|}}{x-0} = \lim_{x\to 0^+} \frac{\sqrt{x}}{x} = \lim_{x\to 0^+} \frac{1}{\sqrt{x}} = +\infty
\]
\[
\lim_{x\to 0^-} \frac{\sqrt{|x|} - \sqrt{|0|}}{x-0} = \lim_{x\to 0^-} \frac{\sqrt{-x}}{-(\sqrt{-x})^2} = \lim_{x\to 0^-} \frac{1}{-\sqrt{-x}} = -\infty
\]
Quindi ricadiamo nella casistica della cuspide, che graficamente assomiglia ad una V (o una V rovesciata se il segno segni infiniti è invertito)
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    xmin=-5, xmax=5,
    ymin=-0.5, ymax=3,
    axis lines=middle,
    enlargelimits=false,
    clip=false,
    axis line style={-stealth, thick},
    width=10cm,
    height=6cm,
    samples=500
]

    % --- f(x) = sqrt(abs(x)) ---
    \addplot[blue, ultra thick, domain=-5:5]
        {sqrt(abs(x))};

    % Etichetta funzione
    \node[blue] at (axis cs:3.2,2.4) {$f(x)=\sqrt{|x|}$};

\end{axis}
\end{tikzpicture}
\end{center}

\addcontentsline{toc}{subsection}{Algebra delle Derivate}
\begin{teorema}{Algebra delle Derivate}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f,g:I \to \mathbb{R}$. Se $f$ e $g$ sono derivabili in $I$ allora
    \begin{enumerate}[label=(\roman*)]
        \item $(\alpha f\pm \beta g)(x)$ è continua in $I$ (con $\alpha, \beta \in \mathbb{R}$) e vale 
        \[
            \frac{d}{dx}((\alpha f\pm \beta g)(x)) = \alpha f'(x) \pm \beta g'(x)
        \]
        \item $(f \cdot g)(x)$ è continua in $I$ e vale 
        \[
            \frac{d}{dx}((f \cdot g)(x)) = f'(x)g(x) + f(x)g'(x)
        \]
        \item $\left(\dfrac{f}{g}\right)(x)$ è continua in $I$ se $g(x)\ne 0$ e vale 
        \[
            \frac{d}{dx}\left(\left(\dfrac{f}{g}\right)(x)\right) = \dfrac{f'(x)g(x)-f(x)g'(x)}{g^2 (x)}
        \]
    \end{enumerate}
\end{teorema}
\begin{proof}
    $(i)$ quindi data la funzione $(\alpha f\pm \beta g)(x)$ usiamo la definizione di derivata
    \begin{align*}
        \frac{d}{dx}((\alpha f\pm \beta g)(x)) &= \lim_{x\to x_0}\dfrac{(\alpha f(x) \pm \beta g(x)) - (\alpha f(x_0) \pm \beta g(x_0))}{x-x_0} \\
        &= \lim_{x\to x_0}\dfrac{(\alpha f(x) - \alpha f(x_0)) \pm (\beta g(x) - \beta g(x_0))}{x-x_0} \\
    &= \lim_{x\to x_0}\alpha\cdot\dfrac{f(x) - f(x_0) }{x-x_0} \pm \beta \cdot\frac{ g(x) - g(x_0)}{x-x_0} 
    \end{align*} 
    \newpage 
    Per ipotesi sappiamo che $f$ e $g$ sono derivabili e quindi vale 
    \[
    \begin{array}{c @{\qquad} @{\qquad} c}
        \displaystyle f'(x_0) = \lim_{x\to x_0}\dfrac{ f(x) - f(x_0) }{x-x_0} & \displaystyle g'(x_0) = \lim_{x\to x_0}\dfrac{ g(x) - g(x_0) }{x-x_0}
    \end{array}\]
    Quindi usando l'algebra dei limiti scopriamo che
    \[
    \lim_{x\to x_0}\alpha\cdot\dfrac{ f(x) - f(x_0) }{x-x_0} \pm \beta \cdot\frac{ g(x) - g(x_0)}{x-x_0}  = \alpha f'(x_0) \pm \beta g'(x_0)
    \]
    Ma dato che $f$ e $g$ sono continue $\forall x \in I$, possiamo più semplicemente scrivere
    \[
\frac{d}{dx}((\alpha f\pm \beta g)(x))=    \alpha f'(x) \pm \beta g'(x)
    \] 

    $(ii)$ Usando la definizione di derivata abbiamo che
    \[
    \frac{d}{dx}((f \cdot g)(x)) = \lim_{x\to x_0} \frac{f(x)g(x) - f(x_0)g(x_0)}{x-x_0}
    \]
    Per dimostrare questo è sufficiente aggiungere e togliere il termine $f(x_0)g(x)$ (oppure anche $f(x)g(x_0)$) e notiamo che
    \begin{align*}
        \lim_{x\to x_0} \frac{f(x)g(x) - f(x_0)g(x_0)}{x-x_0} &= \lim_{x\to x_0} \frac{f(x)g(x) \color{red}{-f(x_0)g(x) +f(x_0)g(x)} \color{black}{- f(x_0)g(x_0)}}{x-x_0} \\
        &= \lim_{x\to x_0} \frac{(f(x) -f(x_0))g(x) +f(x_0)(g(x) - g(x_0)}{x-x_0} \\
        &=   \lim_{x\to x_0} g(x)\cdot\frac{f(x) -f(x_0)}{x-x_0} + \lim_{x\to x_0} f(x_0)\cdot\frac{g(x) -g(x_0)}{x-x_0}
    \end{align*}
    Per le frazioni usiamo lo stesso ragionamento di prima, per il termine $f(x_0)$ lo possiamo portare fuori dato che è una costante, invece il termine $g(x)$ dobbiamo ragionare. Infatti noi sappiamo solamente che $g$ è derivabile, però non sappiamo nulla su $\displaystyle\lim_{x\to x_0} g(x)$. Però possiamo ricordarci che se una funzione è derivabile allora è anche continua. Pertanto scopriamo che $g$ è anche continua, e che quindi vale 
    \[
     \lim_{x\to x_0} g(x) = g(x_0)
    \] 
    Quindi il limite diventa
    \[
     \lim_{x\to x_0} g(x)\cdot\frac{f(x) -f(x_0)}{x-x_0} + \lim_{x\to x_0} f(x_0)\cdot\frac{g(x) -g(x_0)}{x-x_0} = g(x_0)f'(x_0) + f(x_0)g'(x_0)
    \]
    Come prima, dato che $f$ e $g$ sono derivabili (e continue) $\forall x \in I$, possiamo riscriverlo come
    \[
g(x_0)f'(x_0) + f(x_0)g'(x_0) = f'(x)g(x) + f(x)g'(x)
    \]

    $(iii)$ Usiamo la definizione di derivata
    \[
    \frac{d}{dx}\left(\left(\dfrac{f}{g}\right)(x)\right) = \lim_{x\to x_0} \dfrac{\dfrac{f(x)}{g(x)} - \dfrac{f(x_0)}{g(x_0)}}{x-x_0}
    \]
    \newpage
    Facciamo il denominatore comune
    \[
    \lim_{x\to x_0} \dfrac{\dfrac{f(x)}{g(x)} - \dfrac{f(x_0)}{g(x_0)}}{x-x_0} = \lim_{x\to x_0} \dfrac{\dfrac{f(x)g(x_0) - f(x_0)g(x)}{g(x)g(x_0)}}{x-x_0} = \lim_{x\to x_0} \dfrac{f(x)g(x_0) - f(x_0)g(x)}{g(x)g(x_0)(x-x_0)}  
    \]
    Ora possiamo fare lo stesso trucchetto di prima aggiungendo e sottraendo $f(x)g(x)$
    \begin{align*}
        \lim_{x\to x_0} \dfrac{f(x)g(x_0) - f(x_0)g(x)}{g(x)g(x_0)(x-x_0)} &= \lim_{x\to x_0} \dfrac{f(x)g(x_0) \color{red}{-f(x)g(x) +f(x)g(x)}\color{black}- f(x_0)g(x)}{g(x)g(x_0)(x-x_0)} \\
        &= \lim_{x\to x_0} \dfrac{-f(x)(-g(x_0) +g(x)) +(f(x)- f(x_0))g(x)}{g(x)g(x_0)(x-x_0)} \\
        &= \lim_{x\to x_0} -f(x)\cdot \dfrac{-g(x_0) +g(x)}{g(x)g(x_0)(x-x_0)} +\dfrac{f(x)- f(x_0)}{g(x)g(x_0)(x-x_0)}\cdot g(x)
    \end{align*}
    Ora possiamo usare come prima la definizione di defivata per $f$ e $g$, e anche per la continuità di $f$ e $g$ sappiamo che $f(x)\to f(x_0)$ e $g(x)\to g(x_0)$
    \begin{align*}
        \lim_{x\to x_0} -f(x)\cdot \dfrac{-g(x_0) +g(x)}{g(x)g(x_0)(x-x_0)} +\dfrac{f(x)- f(x_0)}{g(x)g(x_0)(x-x_0)}\cdot g(x)  &= \dfrac{-f(x_0)g'(x_0)}{g(x_0)g(x_0)} + \dfrac{f'(x_0)g(x_0)}{g(x_0)g(x_0)} \\
&= \dfrac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}
    \end{align*}
\end{proof}


\addcontentsline{toc}{subsection}{Derivata di Funzioni Composte}
\begin{teorema}{Derivata di Funzioni Composte}{}
    Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f$, $g$ due funzioni tali che $g \circ f : I \to \mathbb{R}$, è ben definito. Se
    \begin{itemize}
        \item $f$ è derivabile in $x_0 \in I$
        \item $g$ è derivabile in $f(x_0)$ 
    \end{itemize}
    Allora 
    \[
    \dfrac{d}{dx}((g\circ f)(x)) = g'(f(x))\cdot f'(x)
    \]
\end{teorema}
\begin{proof}
    Usando la definione di retta tangente, e il cambio di variabile sappiamo che 
    \begin{enumerate}[label=(\arabic*)]
        \item $f(x) = f(x_0) + f'(x_0)(x-x_0) + o(x-x_0)$ per $x\to x_0$
        \item $g(y) = g(y_0) + g'(y_0)(y-y_0) + o(y-y_0)$ per $y\to y_0$
    \end{enumerate}
    Quindi se scegliamo $y_0 = f(x_0)$, avremo che
    \[
    g(\mathunderline{red}{y}) = g(f(x_0)) + g'(f(x_0))(\mathunderline{red}{y}-f(x_0)) + o(\mathunderline{red}{y}-f(x_0)) \text{ per } \mathunderline{red}{y}\to f(x_0)
    \]
    Quindi ora calcolando $g(f(x))$ avremo che
    \[
    g(\mathunderline{red}{f(x)}) = g(f(x_0)) + g'(f(x_0))(\mathunderline{red}{f(x)}-f(x_0)) + \mathunderline{blue}{o(\mathunderline{red}{f(x)}-f(x_0))} \text{ per } \mathunderline{red}{f(x)}\to f(x_0)
    \]
    \newpage
    Ora dato usando $(1)$ possiamo calcolare meglio
    \begin{align*}
        \mathunderline{green}{f(x)}-f(x_0) &= \mathunderline{green}{f(x_0) + f'(x_0)(x-x_0) + o(x-x_0)} - f(x_0) \\
        &= f'(x_0)(x-x_0) + o(x-x_0) 
    \end{align*}
    Ora riscriviamo meglio il termine con l'o-piccolo, usando la proprietà $o(f+o(f)) =o(f)$ e quella che perde tutte le costanti moltiplicative
    \begin{align*}
        \mathunderline{blue}{o(f(x)-f(x_0))} &= o(f'(x_0)(x-x_0) + o(x-x_0)) \\
        &= o\left(f'(x_0)\left((x-x_0) + \frac{o(x-x_0)}{f'(x_0)}\right)\right) \\
        &= o(x-x_0 + o(x-x_0)) = o(x-x_0)
    \end{align*}
    Quindi riprendendo l'espressione con $g(f(x))$ possiamo dire che
    \begin{align*}
    g(f(x)) &= g(f(x_0)) + g'(f(x_0))(\mathunderline{red}{f(x)-f(x_0)}) + \mathunderline{blue}{o(f(x)-f(x_0))} \\
     &= g(f(x_0)) + g'(f(x_0))(\mathunderline{red}{f'(x_0)(x-x_0) + o(x-x_0)}) + \mathunderline{blue}{o(x-x_0)} \\
    &= g(f(x_0)) + g'(f(x_0))(f'(x_0)(x-x_0)) + g'(f(x_0))o(x-x_0) + o(x-x_0) \\
    &= g(f(x_0)) + g'(f(x_0))(f'(x_0)(x-x_0)) + o(x-x_0)
    \end{align*}
    Portanto il termine $g(f(x_0))$ alla sinistra scopriamo che
    \[
    g(f(x)) - g(f(x_0)) = g'(f(x_0))(f'(x_0)(x-x_0)) + o(x-x_0) \text{ per } x\to x_0
    \]
    Con questa informazione possiamo usare la definizione di derivata 
    \begin{align*}
       \lim_{x\to x_0} \dfrac{g(f(x)) - g(f(x_0))}{x-x_0} &= \lim_{x\to x_0} \dfrac{g'(f(x_0))(f'(x_0)(x-x_0)) + o(x-x_0)}{x-x_0} \\
     &= \lim_{x\to x_0} \left(\dfrac{g'(f(x_0))(f'(x_0)(x-x_0))}{x-x_0} + \dfrac{o(x-x_0)}{x-x_0} \right) \\
    &= g'(f(x_0))f'(x_0) + 0\\
    &= g'(f(x))f'(x)
    \end{align*}
\end{proof}


\addcontentsline{toc}{subsection}{Esercizi sulla Derivata di Funzioni Composte}
Vediamo qualche esercizio per impraticarci con i teoremi visti fino ad ora.
\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
        f(x) = \frac{1+x^2}{1+x}
    \]
\end{esercizio}
Notiamo subito che è una frazione, quindi usiamo la regola del quoziente:
\begin{align*}
    \frac{d}{dx} \left(\frac{1+x^2}{1+x}\right) &= \dfrac{(1+x^2)'(1+x) - (1+x^2)(1+x)'}{(1+x)^2} \\
&= \dfrac{(2x)(1+x) - (1+x^2)(1)}{(x+1)^2} \\
&= \dfrac{2x + 2x^2 -1-x^2}{(x+1)^2} \\
&= \dfrac{x^2+2x  -1}{(x+1)^2}
\end{align*}

\newpage
\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
        f(x) = \left(\frac{1+x^2}{1+x}\right)^5
    \]
\end{esercizio}
Notiamo che questo esercizio è molto simile a quello precedente, però c'è la potenza 5 che rovina i piani. Quindi possiamo fare il teorema delle funzioni composte, infatti se scelgo $y= \left(\frac{1+x^2}{1+x}\right)$, in questo modo abbiamo 
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle g(y) = y^5 &\displaystyle h(x) = \frac{1+x^2}{1+x}
\end{array}
\]
Dato che la funzione originale non è altro che $f(x)=g(h(x))$, la derivata di $h(x)$ la possiamo utilizzare quella dell'esercizio scorso. Quindi calcoliamo le singole derivate
\[\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle g'(y) = 5y^4 &\displaystyle h'(x) = \dfrac{x^2+2x  -1}{(x+1)^2}
\end{array}
\]
Di conseguenza:
\[
\frac{d}{dx}(f(x)) = \frac{d}{dx}(g(h(x))) = g'(h(x))\cdot h'(x) = 5\left(\frac{1+x^2}{1+x}\right)^4\cdot \dfrac{x^2+2x  -1}{(x+1)^2}
\]
Scrivendo meglio 
\[
\frac{d}{dx}(f(x)) = \dfrac{5(1+x^2)^4(x^2+2x  -1)}{(x+1)^6}
\]
\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
        f(x) = \cot(x) = \dfrac{\cos(x)}{\sin(x)}
    \]
\end{esercizio}
Questo esercizio è semplicemente una frazione
\begin{align*}
    \frac{d}{dx} \left(\dfrac{\cos(x)}{\sin(x)}\right) &= \dfrac{(\cos(x))'(\sin(x)) - (\cos(x))(\sin(x))'}{(\sin(x))^2} \\
    &= \dfrac{(-\sin(x))(\sin(x)) - (\cos(x))(\cos(x))}{\sin^2(x)} \\
    &= \dfrac{-\sin^2(x) - \cos^2(x)}{\sin^2(x)} = \dfrac{-1}{\sin^2(x)}
\end{align*}

\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
        f(x) = (x\cot(x))^2 
    \]
\end{esercizio}
Notiamo che sembra di essere nella situazione simile a prima, quindi posso scegliere
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle g(y) = y^2 &\displaystyle h(x) = x\cot(x)
\end{array}
\]
Però notiamo che la derivata di $g(y)$ è molto semplice, infatti $g'(y) = 2y$, però la derivata di $h(x)$ dobbiamo fare qualche conto in più, infatti dobbiamo usare la regola del prodotto 
\[
h'(x) = (x)'\cot(x) + x(\cot(x))' = 1\cdot \frac{\cos(x)}{\sin(x)} + x \cdot \left(\frac{-1}{\sin^2(x)}\right) = \frac{\cos(x)\sin(x) - x}{\sin^2(x)}
\]
Ora possiamo usare la regola della composizione
\[
\frac{d}{dx}(f(x)) = \frac{d}{dx}(g(h(x))) = g'(h(x))h'(x) = 2(x\cot(x)) \cdot \frac{\cos(x)\sin(x) - x}{\sin^2(x)}
\]
Che scrivendo meglio
\[
\frac{d}{dx}(f(x))=  \dfrac{2x\cos(x)(\cos(x)\sin(x) - x)}{\sin^3(x)}  
\]

\begin{esercizio}{}{}
    Calcolare la dericata di 
    \[
    f(x) = \sqrt{x+\sqrt{x+\sqrt{x}}}
    \]
\end{esercizio}
Rimandendo con la stessa logica potremmo scegliere
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle g(y) = \sqrt{y} &\displaystyle h(x) = x+\sqrt{x+\sqrt{x}}
\end{array}
\]
La derivata di $g(y)$ sappiamo che è $g'(y) = \frac{1}{2\sqrt{x}}$, ma la dobbiamo invece calcolare la derivata di $h(x)$.
\[
h'(x) = \frac{d}{dx}\left(x+\sqrt{x+\sqrt{x}}\right) = 1 + \left(\sqrt{x+\sqrt{x}}\right)'
\]
Però noi non sappiamo quale è la derivata di quella radice, quindi dobbiamo riapplicare il criterio della composta, e scegliamo
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle j(y) = \sqrt{y} &\displaystyle k(x) = x+\sqrt{x}
\end{array}
\] 
Di questo possiamo calcolare la derivata 
\[
\begin{array}{c@{\qquad}@{\qquad} c}
   \displaystyle j'(y) = \frac{1}{2\sqrt{y}} &\displaystyle k'(x) = 1+\frac{1}{2\sqrt{x}}
\end{array}
\] 
Di conseguenza
\[
\frac{d}{dx}(j(k(x))) = j'(k(x))k'(x) =   \frac{1}{2\sqrt{x+\sqrt{x}}}\cdot \left(1+\frac{1}{2\sqrt{x}}\right) =  \frac{2\sqrt{x}+1}{4\sqrt{x}\sqrt{x+\sqrt{x}}} 
\]
Quindi ora possiamo calcolare $h'(x)$
\[
h'(x) = 1 + \left(\sqrt{x+\sqrt{x}}\right)' = 1 +\frac{2\sqrt{x}+1}{4\sqrt{x}\sqrt{x+\sqrt{x}}} 
\]
Ma quindi ora possiamo calcolare $f'(x)$
\[
\frac{d}{dx}(f(x)) = \frac{d}{dx}(g(h(x))) = g'(h(x)) h'(x) = \frac{1}{2\sqrt{x+\sqrt{x+\sqrt{x}}}} \cdot \left(1 +\frac{2\sqrt{x}+1}{4\sqrt{x}\sqrt{x+\sqrt{x}}} \right) 
\]

\newpage

\addcontentsline{toc}{subsection}{Derivabilità della funzione Inversa}
\begin{teorema}{Derivabilità della funzione Inversa}{}
     Sia $\varnothing \neq I \subseteq \mathbb{R}$ un intervallo, $f:I \to \mathbb{R}$, se $f$ è derivabile e invertibile, sia $x_0 \in I$, se $f(x_0) \neq 0$ allora definiamo $y_0 = f(x_0)$ e vale
     \[
     \frac{d}{dx}(f^{-1}(y_0)) = \frac{1}{f'(x_0)}
     \]
\end{teorema}
\begin{proof}
    Usiamo la definizione di derivata
    \[
    \frac{d}{dy}(f^{-1}(y_0)) = \lim_{y\to y_0} \frac{f^{-1}(y) - f^{-1}(y_0) }{y-y_0}
    \]
    Ora possiamo applicare un cambio di variabile con $y=f(x)$, allora sappiamo che $y_0=f(x_0)$, e di conseguenza $x=f^{-1}(y)$ e $x_0=f^{-1}(y_0)$ allora 
    \[
    \lim_{y\to y_0} \frac{f^{-1}(y) - f^{-1}(y_0) }{y-y_0} = \lim_{x\to x_0} \frac{x - x_0}{f(x) - f(x_0)} = \lim_{x\to x_0} \frac{1}{\frac{f(x) - f(x_0)}{x - x_0} } = \frac{1}{f'(x_0)} = \frac{1}{f'(f^{-1}(y_0))}
    \] 
\end{proof}

Proviamo a derivare la derivata della funzione inversa in maniera geometrica. Infatti noi sappiamo che la derivata è il limite del coefficiente angolare della retta tangente, e quindi se il coefficiente della retta lo calcoliamo con $\dfrac{\Delta y}{\Delta x}$ se lo portiamo il limite lo scriviamo come $\dfrac{dy}{dx}$, è per questo motivo che si scrive anche in questo modo la derivata. Quindi se prendiamo la funzione $f(x) = x^2$ e la sua inversa $f^{-1}(y)=\sqrt{y}$ (Consideriamo solo valori positivi di $x$).

\begin{figure}[h!]
\centering

% --- Primo grafico ---
\begin{minipage}{0.48\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=6.2cm,
    height=5.3cm,
    xmin=0, xmax=2.5,
    ymin=0, ymax=4.5,
    axis lines=middle,
    axis line style={-stealth, thick},
    samples=300,
    xlabel={$x$}, ylabel={$y$},
    xtick={1,2}, ytick={1,2,3,4},
    clip=false
]

\addplot[blue, ultra thick, domain=0:2.2] {x^2};
\node[blue] at (axis cs:1.7,4.0) {$f(x)$};

\addplot[only marks, mark=*] coordinates {(1,1) (2,4)};

\addplot[red, thick, domain=0.67:2.2] {3*x - 2};

\draw[green!60!black, thick]
    (1,1) -- (2,1) -- (2,4);

\node[green!60!black] at (1.5,0.7) {$dx$};
\node[green!60!black] at (2.3,2.5) {$dy$};

\end{axis}
\end{tikzpicture}
\end{minipage}
%
\begin{minipage}{0.48\textwidth}
\centering
% --- Secondo grafico: f^{-1}(y)=sqrt(y) ---
\begin{tikzpicture}
\begin{axis}[
    width=6.2cm,  
    height=5.3cm,      % <-- più stretto
    xmin=0, xmax=4.5,
    ymin=0, ymax=2.5,     % <-- più basso = radice meno allungata   
    axis lines=middle,
    axis line style={-stealth, thick},
    samples=300,
    xlabel={$y$}, ylabel={$x$},
    xtick={1,2,3,4}, ytick={1,2},
    clip=false
]

\addplot[blue, ultra thick, domain=0:4.5] ({x}, {sqrt(x)});
\node[blue] at (axis cs:3.0,2.1) {$f^{-1}(y)$};

\addplot[only marks, mark=*] coordinates {(1,1) (4,2)};

% retta secante dell'inverso estesa
\addplot[red, thick, domain=0:4.5] ({x}, {1/3*x + 2/3});

\draw[green!60!black, thick]
    (1,1) -- (4,1) -- (4,2);

\node[green!60!black] at (2.5,0.7) {$dy$};
\node[green!60!black] at (4.3,1.5) {$dx$};

\end{axis}
\end{tikzpicture}

\end{minipage}
\end{figure}

Possiamo notare che il coefficiente angolare della retta di $f^{-1}(y)$ non è altro che $\displaystyle\frac{\Delta x}{\Delta y}$, dato che gli assi sono invertiri rispetto alla funzione originale. Quindi se potiamo al limite questa informazione sappiamo che la derivata di $f^{-1}(y)$ la calcoliamo come $\displaystyle\frac{dx}{dy}$. Noi però conosciamo solamente la derivata di $f(x)$ che è $\displaystyle\frac{dy}{dx}$, quindi dato che è una frazione possiamo calcolare la derivata di $f^{-1}(y)$ come
\[
 (f^{-1}(y_0))' = \dfrac{dx}{dy} = \dfrac{1}{\dfrac{dy}{dx}} = \frac{1}{f'(x_0)} 
\]
\newpage
Esercitiamoci con qualche funzione inversa molto importante, partiamo con
\begin{esercizio}{}{}
    Calcolare la derivata della funzione inversa di 
    \[
    f(x) = \sin(x)
    \]
\end{esercizio}
In sostanza l'esercizio ci sta chiedendo di calcolare la derivata di $f^{-1}(y)=\arcsin(y)$, quindi in primis dobbiamo calcolare la derivata di $f(x)$
\[
f'(x) = \cos(x)
\] 
Quindi 
\[
\frac{d}{dy}(\arcsin(y)) = \frac{1}{\cos(x)}
\]
Però ora dobbiamo tornare alla variabile y, e quindi dovremmo sostituire al posto di $x=\arcsin(y)$, ma dopo verrebbe un denominatore troppo complesso. Quindi per semplificarci la vita, cerchiamo di riscrivere $\cos(x)$ in funzione di $\sin(x)$, in modo tale che dopo possiamo sostituire $\sin(x)=y$ (dato che prima dovevamo sostituire $x=\arcsin(y))$. Quindi usando le formule fondamentali della trigonometria abbiamo che
\[
\frac{d}{dy}(\arcsin(y)) = \dfrac{1}{\cos(x)} = \dfrac{1}{\sqrt{1-\sin^2(x)}} = \dfrac{1}{\sqrt{1-y^2}} 
\]
\begin{esercizio}{}{}
    Calcolare la derivata della funzione inversa di 
    \[
    f(x) = \tan(x)
    \]
\end{esercizio}
Quindi ci stanno chiedendo di calcolare la derivata di $f^{-1}(y)=\arctan(y)$, quindi in primi sobbiamo calcolare la derivata di $f(x)$
\begin{align*}
f'(x) = \left(\dfrac{\sin(x)}{\cos(x)}\right)' &= \dfrac{\cos(x)\cos(x) - \sin(x)(-\sin(x))}{\cos^2(x)}\\
&= \dfrac{\cos^2(x)}{\cos^2(x)} + \dfrac{\sin^2(x)}{\cos^2(x)} \\
&= 1+\tan^2(x)
\end{align*}
Ho scritto la derivata così, e non $\frac{1}{\cos^2(x)}$, per evitare di avere lo stesso problema dell'esercizio di prima. Infatti prima abbiamo dovuto riscrivere il $\cos(x)$ in funzione del $\sin(x)$ in modo dale da sostituire $y=\sin(x)$. Quindi ho già scritto la derivata di $f(x)$ in funzione di $\tan(x)$, così da evitare il passaggio che abbiamo fatto sopra (e anche perchè non era così semplice per questo esercizio) e poter sostituire subito $y=\tan(x)$. Quindi la derivata di $\arctan(y)$ è
\[
\dfrac{d}{dy}(\arctan(y)) = \dfrac{1}{f'(x)} = \dfrac{1}{1+\tan^2(x)} = \dfrac{1}{1+y^2}
\]

\end{document}
